{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import datetime\n",
    "# import pandas_ta as ta\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import numpy as np\n",
    "from data_loader_c1 import TwoStreamDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from data_loader_c1 import load_dataloader\n",
    "# from model_real_1 import RealCNNLSTM\n",
    "from model_real_2 import RealCNNLSTM\n",
    "# from model_complex_1 import ComplexCNNLSTM\n",
    "from model_complex_2 import ComplexCNNLSTM\n",
    "# from model_fusion_1 import FusionGated\n",
    "# from model_FusionMagPhase import FusionMagPhase\n",
    "from model_FusionMagPhase_log import FusionMagPhase\n",
    "# from fusion_no_gate import FusionConcatMag\n",
    "# from model_fusion_real_mag import FusionComplexLinear\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results_acc(results_acc, result_tab, config):\n",
    "    \"\"\"Запись результатов\"\"\"\n",
    "    results_acc.loc[len(results_acc)] = [result_tab['date'][0], config['data']['time_shift'], config['data']['window_size'], \\\n",
    "    config['data']['lr_lambda'], config['train']['epochs'], config['model']['dropout'], config['model']['n_res_blocks'], \\\n",
    "    round(result_tab['Pred_last_10_ep_mean'].mean(), 4), round(result_tab['Pred_last_10_ep_mean'].std(), 4), \\\n",
    "    round(result_tab['Pred_last_10_ep_std'].mean(), 4), round(result_tab['Pred_last_10_ep_std'].std(), 4)] + \\\n",
    "    result_tab['Pred_last_10_ep_mean'].tolist() + result_tab['Pred_last_10_ep_std'].tolist()\n",
    "    return results_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_time_series_split_4(df_len, time_shift=1, interval_days=1, main_splits_number=2, initial_train_frac=0.90):\n",
    "    splits = []\n",
    "    df_len_sh = df_len - interval_days - time_shift + 1\n",
    "    initial_train_size = int(df_len_sh * initial_train_frac)\n",
    "\n",
    "    initial_train_size += (df_len_sh - initial_train_size) % main_splits_number\n",
    "    val_size = int((df_len_sh - initial_train_size) / main_splits_number)\n",
    "    assert val_size not in range(1)\n",
    "    \n",
    "    train_end = initial_train_size - time_shift + 1\n",
    "    val_start = initial_train_size\n",
    "    val_end = val_start + val_size\n",
    "\n",
    "    while val_end <= df_len_sh:\n",
    "        \n",
    "        train_idx = list(range(0, train_end))\n",
    "        val_idx = list(range(val_start, val_end))\n",
    "        splits.append((train_idx, val_idx))\n",
    "        train_end = val_end\n",
    "        val_start = train_end\n",
    "        val_end = val_start + val_size\n",
    "\n",
    "    # print(splits)\n",
    "    one_day_start = splits[-1][1][-1]\n",
    "\n",
    "    for i in range(interval_days):\n",
    "        train_idx = list(range(0, one_day_start + i + 1))\n",
    "        val_idx = list(range(one_day_start + i + time_shift, one_day_start + i + time_shift + 1))\n",
    "        splits.append((train_idx, val_idx))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = custom_time_series_split_4(55, time_shift=1, interval_days=2, main_splits_number=1, initial_train_frac=0.90)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Вариант с двумя фолдами!!!\n",
    "# !!! Вариант с логированием вкладов реальной и комплексной частей!!!\n",
    "\n",
    "# ---------- EarlyStopping ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=1e-2, mode=\"min\", restore_best=True, rel_delta=True):\n",
    "        assert mode in (\"min\", \"max\")\n",
    "        self.patience, self.min_delta, self.mode = patience, float(min_delta), mode\n",
    "        self.restore_best, self.rel_delta = restore_best, rel_delta\n",
    "        self.best, self.wait, self.best_states = None, 0, {}\n",
    "\n",
    "    def _improved(self, cur, best):\n",
    "        if self.mode == \"min\":\n",
    "            return cur < (best*(1-self.min_delta) if self.rel_delta else best - self.min_delta)\n",
    "        else:\n",
    "            return cur > (best*(1+self.min_delta) if self.rel_delta else best + self.min_delta)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, metric: float, models: Dict[str, torch.nn.Module]) -> bool:\n",
    "        if self.best is None or self._improved(metric, self.best):\n",
    "            self.best, self.wait = metric, 0\n",
    "            self.best_states = {k:{n:v.detach().cpu().clone() for n,v in m.state_dict().items()}\n",
    "                                for k,m in models.items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, models: Dict[str, torch.nn.Module]) -> None:\n",
    "        if not self.best_states: return\n",
    "        for k,m in models.items():\n",
    "            if k in self.best_states:\n",
    "                m.load_state_dict(self.best_states[k])\n",
    "\n",
    "\n",
    "# ---------- вспомогательные утилиты ----------\n",
    "def _set_seed(seed=42):\n",
    "    import os, random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _reset_weights(m: nn.Module):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "        m.reset_parameters()\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, p in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(p.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(p.data)\n",
    "                h = p.shape[0]//4\n",
    "                p.data[h:2*h] = 1.0  # forget-bias\n",
    "\n",
    "\n",
    "# ---------- основная функция ----------\n",
    "def train_tscv_model_8(config: dict, df: pd.DataFrame, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Обучение с кастомным TSCV, EMA-сглаживанием, EarlyStopping, LR-Plateau, grad-accum, clip-grad.\n",
    "    Возвращает:\n",
    "        results: dict с метриками по фолдам и сериями предсказаний\n",
    "    \"\"\"\n",
    "    _set_seed(config.get(\"seed\", 42))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- даты и срез df ---\n",
    "    start_preds_date = pd.to_datetime(config['data']['start_preds_date'], errors='coerce')\n",
    "    stop_preds_date  = pd.to_datetime(config['data']['stop_preds_date'],  errors='coerce')\n",
    "    time_shift       = int(config['data']['time_shift'])\n",
    "    window_size      = int(config['data']['window_size'])\n",
    "    lr_lambda_type = config['data']['lr_lambda']\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df[pd.to_datetime(df['date']) <= stop_preds_date].reset_index(drop=True)\n",
    "\n",
    "    interval_days = int((stop_preds_date - start_preds_date).days)\n",
    "    if verbose:\n",
    "        print(\"Dates:\", start_preds_date, \"→\", stop_preds_date,\n",
    "              \"| interval_days:\", interval_days, \"| time_shift:\", time_shift)\n",
    "\n",
    "    # --- сплиты ---\n",
    "    # splits = custom_time_series_split_3(\n",
    "    #     len(df),\n",
    "    #     time_shift=time_shift,\n",
    "    #     interval_days=interval_days,\n",
    "    #     main_splits_number=0,\n",
    "    #     initial_train_frac=0.99\n",
    "    # )\n",
    "    # splits = custom_time_series_split_2(len(df), time_shift=time_shift, interval_days=interval_days, main_splits_number=2, initial_train_frac=0.90)\n",
    "    splits = custom_time_series_split_4(len(df), time_shift=time_shift, interval_days=interval_days, main_splits_number=1, initial_train_frac=0.90)\n",
    "    if verbose: print('num of splits =', len(splits))\n",
    "\n",
    "    # --- bootstrap-loader (определяем F и C до инициализации моделей) ---\n",
    "    first_train_idx, _ = splits[0]\n",
    "    bootstrap_loader = load_dataloader(\n",
    "        df.iloc[first_train_idx], window_size=window_size,\n",
    "        batch_size=config['train']['batch_size'],\n",
    "        shuffle=False, drop_last=False\n",
    "    )\n",
    "    first_batch = next(iter(bootstrap_loader))\n",
    "    F = first_batch['real_feats'].shape[-1]\n",
    "    C = (first_batch['complex_time_real'].shape[-1]\n",
    "         if ('complex_time_real' in first_batch and 'complex_time_imag' in first_batch) else 1)\n",
    "\n",
    "    # --- модели ---\n",
    "    real_net = RealCNNLSTM(\n",
    "        num_real_features=F,\n",
    "        hidden_dim=config['model']['real_hidden_dim'],\n",
    "        lstm_layers=config['model'].get('real_lstm_layers', 2),\n",
    "        dropout=config['model'].get('dropout', 0.3),\n",
    "        kernel_size=3,\n",
    "        bidirectional=config['model'].get('bidirectional', True),\n",
    "        take=config['model'].get('take', \"last_timestep\"),\n",
    "        proj_out=True\n",
    "    ).to(device)\n",
    "\n",
    "    complex_net = ComplexCNNLSTM(\n",
    "        in_channels=C,\n",
    "        hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        num_layers=config['model']['n_res_blocks'],\n",
    "        dropout=config['model']['dropout'],\n",
    "        kernel_size=3,\n",
    "        proj_out=False\n",
    "    ).to(device)\n",
    "\n",
    "    # fusion_net = FusionGated(\n",
    "    #     real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "    #     complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "    #     output_dim=1, hidden=128, dropout=0.2, use_softmax=True\n",
    "    # ).to(device)\n",
    "\n",
    "    fusion_net = FusionMagPhase(\n",
    "        real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "        complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        output_dim=1, hidden=128, dropout=0.2, use_softmax=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # чуть смещаем гейт в сторону complex-ветки (второй логит)\n",
    "        fusion_net.gate[-1].bias[:] = torch.tensor(\n",
    "            [-0.4, 0.4],\n",
    "            device=fusion_net.gate[-1].bias.device\n",
    "        )\n",
    "\n",
    "    # fusion_net = FusionConcatMag(\n",
    "    #     real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "    #     complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "    #     output_dim=1,\n",
    "    #     hidden=128,\n",
    "    #     dropout=0.2,\n",
    "    # ).to(device)\n",
    "\n",
    "    # вариант 1: только real-часть complex-linear (real)\n",
    "    # fusion_net = FusionComplexLinear(\n",
    "    #     real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "    #     complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "    #     output_dim=1,\n",
    "    #     hidden=128,\n",
    "    #     dropout=0.2,\n",
    "    #     complex_mode=\"real\"\n",
    "    # ).to(device)\n",
    "    \n",
    "    # # вариант 2: модуль комплексного линейного выхода (mag)\n",
    "    # fusion_net = FusionComplexLinear(\n",
    "    #     real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "    #     complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "    #     output_dim=1,\n",
    "    #     hidden=128,\n",
    "    #     dropout=0.2,\n",
    "    #     complex_mode=\"mag\"\n",
    "    # ).to(device)\n",
    "\n",
    "    # --- ✅ Инициализация bias последнего линейного слоя ---\n",
    "    # with torch.no_grad():\n",
    "    #     for mod in fusion_net.modules():\n",
    "    #         if isinstance(mod, torch.nn.Linear) and mod.out_features == 1:\n",
    "    #             mod.bias.fill_(0.0)   # так как таргет уже нормирован (среднее ~ 0)\n",
    "    reset_each_fold= bool(config['train'].get('reset_each_fold', True))   \n",
    "    if reset_each_fold and (config['train']['lr'] < 0.003):\n",
    "        real_net.apply(_reset_weights)\n",
    "        complex_net.apply(_reset_weights)\n",
    "        fusion_net.apply(_reset_weights)\n",
    "\n",
    "    # --- оптимизатор/лоссы/планировщик/ES ---\n",
    "    optimizer = optim.Adam(\n",
    "        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "        lr=config['train']['lr'], weight_decay=1e-4\n",
    "    )\n",
    "    criterion = nn.MSELoss() if config['train'].get('loss', 'mse') == 'mse' else nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "    epochs = int(config['train']['epochs'])\n",
    "    # warmup: 20% эпох, но не менее 1 и не более epochs-1\n",
    "    warmup_epochs = max(1, min(epochs - 1, int(0.2 * epochs)))\n",
    "    min_factor = 1e-3\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t))\n",
    "    else: \n",
    "        def lr_lambda(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 1.\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5\n",
    "            else:\n",
    "                return 0.1\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda_fold2(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs * 0.2\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t)) * 0.2\n",
    "    else: \n",
    "        def lr_lambda_fold2(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 0.4\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5 * 0.4\n",
    "            else:\n",
    "                return 0.1 * 0.4\n",
    "            \n",
    "    # else: \n",
    "    #     def lr_lambda(epoch: int):\n",
    "    #         if epoch < epochs // 4:\n",
    "    #             return 1.\n",
    "    #         elif epochs // 4 <= epoch < (epochs // 4) * 2:\n",
    "    #             return 0.5\n",
    "    #         elif (epochs // 4) * 2 <= epoch < (epochs // 4) * 3:\n",
    "    #             return 0.1\n",
    "    #         else:\n",
    "    #             return 0.05\n",
    "            \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    early = EarlyStopping(\n",
    "        patience=config['train'].get('early_stop_patience', 10),\n",
    "        min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "        mode=\"min\", restore_best=True, rel_delta=True\n",
    "    )\n",
    "    ema_beta = float(config['train'].get('ema_beta', 0.8))\n",
    "\n",
    "    # --- тренинг-опции ---\n",
    "    epochs         = int(config['train']['epochs'])\n",
    "    batch_size     = int(config['train']['batch_size'])\n",
    "    accum_steps    = int(config['train'].get('accum_steps', 1))\n",
    "    max_grad_norm  = config['train'].get('max_grad_norm', 1.0)\n",
    "    min_bs_for_bn  = int(config['train'].get('min_bs_for_bn', 2))\n",
    "    reset_each_fold= bool(config['train'].get('reset_each_fold', False))\n",
    "\n",
    "    if verbose: print('size of df:', df.shape)\n",
    "\n",
    "    # --- нормализация (фиксируем μ,σ по первому train-сплиту) ---\n",
    "    cols_to_scale = config['data'].get('cols_to_scale', [\n",
    "        'DayAvgPrice','IntradayStd','Volume','Log_Profit','DayAvgPrice_diff',\n",
    "        'DAP_1','DAP_2','DAP_3','DAP_4','DAP_5','DAP_6',\n",
    "        'POLY_1','POLY_2','POLY_3','lambda_C3','lambda_C2','lambda_C1'\n",
    "    ])\n",
    "\n",
    "    mu = sigma = None\n",
    "    mu_target = sigma_target = None\n",
    "\n",
    "    # --- сбор результатов по фолдам ---\n",
    "    fold_metrics: List[Dict] = []\n",
    "    fold_pred_series: List[pd.Series] = []\n",
    "    preds_last_10 = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        # опциональный ресет весов (для «честной» CV)\n",
    "        if reset_each_fold and (fold + 1) > 1:\n",
    "            # real_net.apply(_reset_weights)\n",
    "            # complex_net.apply(_reset_weights)\n",
    "            # fusion_net.apply(_reset_weights)\n",
    "            # optimizer = optim.Adam(\n",
    "            #     list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "            #     lr=config['train']['lr']\n",
    "            # )\n",
    "            # сбросить состояние LR-схем, ES, EMA\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=100, threshold=1e-3, threshold_mode='rel',\n",
    "            #     cooldown=2, verbose=True\n",
    "            # )\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=5, verbose=verbose\n",
    "            # )\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fold2)\n",
    "            early = EarlyStopping(\n",
    "                patience=config['train'].get('early_stop_patience', 10),\n",
    "                min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "                mode=\"min\", restore_best=True, rel_delta=True\n",
    "            )\n",
    "            ema_val = None\n",
    "        else:\n",
    "            ema_val = None  # сбрасываем EMA в любом случае на новый фолд (иначе метрики не сопоставимы)\n",
    "\n",
    "        train_df = df.iloc[train_idx].copy()\n",
    "\n",
    "        # расширим val назад до window_size, безопасно по границам\n",
    "        if len(val_idx) <= window_size:\n",
    "            end = val_idx[-1]\n",
    "            start = max(0, end - window_size)\n",
    "            val_idx = list(range(start, end + 1))\n",
    "\n",
    "        val_df = df.iloc[val_idx].copy()\n",
    "\n",
    "        # μ,σ считаем по ПЕРВОМУ train-сплиту\n",
    "        if mu is None or sigma is None:\n",
    "            mu = train_df[cols_to_scale].mean()\n",
    "            sigma = train_df[cols_to_scale].std().replace(0, 1.0)\n",
    "\n",
    "        # применяем нормировку\n",
    "        for col in cols_to_scale:\n",
    "            train_df.loc[:, col] = (train_df[col] - mu[col]) / sigma[col]\n",
    "            val_df.loc[:,   col] = (val_df[col]   - mu[col]) / sigma[col]\n",
    "\n",
    "        # μ,σ для z-нормировки Target\n",
    "        if mu_target is None or sigma_target is None:\n",
    "            mu_target = train_df['Target'].mean()\n",
    "            sigma_target = train_df['Target'].std()\n",
    "\n",
    "        train_df['Target'] = (train_df['Target'] - mu_target) / sigma_target\n",
    "        val_df['Target'] = (val_df['Target']   - mu_target) / sigma_target \n",
    "        train_df['Target_smooth'] = (train_df['Target_smooth'] - mu_target) / sigma_target\n",
    "        val_df['Target_smooth'] = (val_df['Target_smooth']   - mu_target) / sigma_target  \n",
    "\n",
    "        #print(f\"\\ntrain_end {train_df.iloc[-1]}, val_end {val_df.iloc[-1]}\")  \n",
    "\n",
    "        # лоадеры\n",
    "        train_loader = load_dataloader(train_df, window_size, batch_size, shuffle=True,  drop_last=False)\n",
    "        val_loader   = load_dataloader(val_df,   window_size, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFold {fold+1}: train {len(train_df)}, val {len(val_df)}\",\n",
    "                  \"\\ntrain first...last :\", train_df['date'].min(), \"...\" , train_df['date'].max(),\n",
    "                  \"\\nval first...last   :\", val_df['date'].min(), \"...\" ,   val_df['date'].max())\n",
    "\n",
    "        # контрольный принт последнего окна\n",
    "            \n",
    "        dst   = TwoStreamDataset(train_df, window_size=window_size)\n",
    "        sampt = dst[len(dst) - 1]\n",
    "        print(f\"mu_DAP={mu['c_month_real']:.4f}\")\n",
    "        last_DAP_in_t_window = float(sampt['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        # last_DAP_in_t_window = float(sampt['real_feats'][-1, 0]) * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        target_pricet           = float(sampt['target']) * sigma_target + mu_target\n",
    "        target_smooth_pricet = train_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"train last row | DAP={last_DAP_in_t_window:.4f} | Target={target_pricet:.4f} | Target_smooth={target_smooth_pricet:.4f}\")\n",
    "\n",
    "        ds   = TwoStreamDataset(val_df, window_size=window_size)\n",
    "        samp = ds[len(ds) - 1]\n",
    "        # last_DAP_in_val_window = float(samp['real_feats'][-1, 0]) * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        last_DAP_in_val_window = float(samp['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        # last_DAP_in_val_window = val_df['imag_time'].iloc[-1] * sigma['imag_time'] + mu['imag_time']\n",
    "        # last_DAP_in_val_window = val_df['DayAvgPrice'].iloc[-1] * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        target_price           = float(samp['target']) * sigma_target + mu_target\n",
    "        target_smooth_price = val_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"val last row   | DAP={last_DAP_in_val_window:.4f} | Target={target_price:.4f} | Target_smooth={target_smooth_price:.4f}\")\n",
    "\n",
    "        # Графики\n",
    "        # from utils_live_plot import EMAMeter, TrainingPlotter  # если вынес в файл, иначе опусти импорт\n",
    "        # ema_meter    = EMAMeter(beta=config['train'].get('ema_beta', 0.8))\n",
    "        # live_plotter = TrainingPlotter(max_points=500)\n",
    "\n",
    "        # ----- ЭПОХИ -----\n",
    "        for epoch in range(epochs):\n",
    "            real_net.train(); complex_net.train(); fusion_net.train()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # TRAIN\n",
    "            step = 0\n",
    "            for step, batch in enumerate(train_loader, start=1):\n",
    "                x_real = batch['real_feats'].to(device)\n",
    "                if x_real.size(0) < min_bs_for_bn:\n",
    "                    continue\n",
    "                x_complex = torch.complex(\n",
    "                    batch['complex_time_real'].to(device),\n",
    "                    batch['complex_time_imag'].to(device)\n",
    "                )\n",
    "                y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                h_real = real_net(x_real)\n",
    "                h_r, h_i = complex_net(x_complex)\n",
    "                out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                loss = criterion(out, y) / accum_steps\n",
    "                if step == len(train_loader) - 1: train_loss_temp = loss\n",
    "                loss.backward()\n",
    "\n",
    "                if step % accum_steps == 0:\n",
    "                    if max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                            max_grad_norm\n",
    "                        )\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # доводим хвост аккумулирования\n",
    "            if step % accum_steps != 0:\n",
    "                if max_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                        max_grad_norm\n",
    "                    )\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # VALID\n",
    "            real_net.eval(); complex_net.eval(); fusion_net.eval()\n",
    "            val_loss_sum, n_val = 0.0, 0\n",
    "            pred_vals, pred_dates = [], []\n",
    "\n",
    "            # --- накопление гейтов ---\n",
    "            gate_sum = torch.zeros(2, device=device)  # [sum_w_real, sum_w_abs]\n",
    "            gate_count = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_real = batch['real_feats'].to(device)\n",
    "                    x_complex = torch.complex(\n",
    "                        batch['complex_time_real'].to(device),\n",
    "                        batch['complex_time_imag'].to(device)\n",
    "                    )\n",
    "                    y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                    h_real = real_net(x_real)\n",
    "                    h_r, h_i = complex_net(x_complex)\n",
    "                    out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                    val_loss_sum += criterion(out, y).item()\n",
    "                    n_val += 1\n",
    "\n",
    "                    # даты предиктов\n",
    "                    pred_vals.extend(out.squeeze(-1).cpu().numpy())\n",
    "\n",
    "                    # --- гейты для этого батча ---\n",
    "                    # получаем веса (B,2) на GPU\n",
    "                    w_batch, _, _ = fusion_net._compute_gates(h_real, h_r, h_i)\n",
    "                    gate_sum += w_batch.sum(dim=0)          # суммируем по батчу\n",
    "                    gate_count += w_batch.size(0)\n",
    "\n",
    "\n",
    "                    if 'target_date' in batch:\n",
    "                        pred_dates.extend(list(batch['target_date']))\n",
    "                    else:\n",
    "                        # fallback: считаем по глобальному индексу\n",
    "                        batch_idx_local  = batch['row_idx'].cpu().numpy()   # индекс окна в val_df\n",
    "                        global_start     = val_idx[0]                        # смещение в исходном df\n",
    "                        batch_idx_global = global_start + batch_idx_local\n",
    "                        # y = target[idx + window_size] → цель на \"завтра\" относительно начала окна\n",
    "                        tgt_idx = batch_idx_global + window_size\n",
    "                        pred_dates.extend(df['date'].iloc[tgt_idx].tolist())\n",
    "\n",
    "            avg_val_loss = val_loss_sum / max(1, n_val)\n",
    "\n",
    "            # средние веса гейта за всю валидацию\n",
    "            if gate_count > 0:\n",
    "                gate_mean = (gate_sum / gate_count).detach().cpu().numpy()  # [w_real, w_abs]\n",
    "            else:\n",
    "                gate_mean = None\n",
    "\n",
    "\n",
    "            # --- EMA для scheduler/early ---\n",
    "            ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            use_metric = ema_val\n",
    "\n",
    "            scheduler.step()\n",
    "            # if epoch+1 >= config['train'].get('early_stop_warmup', 0):\n",
    "            #     scheduler.step()\n",
    "            #     #scheduler.step(use_metric)\n",
    "            #     stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     # диагностика (разово/по условию)\n",
    "            #     # print(f\"[ES] e{epoch+1} metric={use_metric:.5f}, best={early.best}, wait={early.wait}\")\n",
    "            #     if stop:\n",
    "            #         if verbose:\n",
    "            #             print(f\"[EarlyStop] best(ema)={early.best:.4f} → restore best weights\")\n",
    "            #         early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #         break\n",
    "            # else:\n",
    "            #     pass\n",
    "\n",
    "            # EMA для scheduler/early\n",
    "            # ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            # use_metric = ema_val  # <— ключевая строка: ES и Plateau смотрят на одно и то же\n",
    "\n",
    "            # Графики\n",
    "            # если ты уже считаешь ema_val сам — можно синхронизировать:\n",
    "            # ema_val = ema_meter.update(avg_val_loss)\n",
    "            # # обновляем график каждые, скажем, 1-5 эпох\n",
    "            # live_plotter.update(epoch+1, avg_val_loss, ema_val)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Validation Loss vs EMA\")\n",
    "            # Графики\n",
    "            # pred_val_denorm = [x * sigma_target + mu_target for x in pred_vals][0]    # откат нормировки целевой переменной\n",
    "            # live_plotter.update(epoch+1, pred_val_denorm, last_DAP_in_val_window)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Predict vs DAP\")\n",
    "            # Графики\n",
    "\n",
    "            # scheduler.step(use_metric)\n",
    "            # stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "\n",
    "            if verbose and ((epoch+1) % 4 == 0 or (epoch+1) == 1):\n",
    "                if gate_mean is not None:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch+1:03d} | train_loss={train_loss_temp:.4f} | val_loss={avg_val_loss:.4f} | ema={ema_val:.4f} \"\n",
    "                        f\"| lr={optimizer.param_groups[0]['lr']:.5f} | pred = {pred_vals_denorm[-1]:.4f} \"\n",
    "                        f\"| gate_real={gate_mean[0]:.3f} | gate_abs={gate_mean[1]:.3f}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch+1:03d} | val={avg_val_loss:.5f} | ema={ema_val:.5f} \"\n",
    "                        f\"| lr={optimizer.param_groups[0]['lr']:.3e}\"\n",
    "                    )\n",
    "\n",
    "            # if verbose and ((epoch+1) % 4 == 0 or (epoch+1) == 1):\n",
    "                # print(f\"Epoch {epoch+1:03d} | train_loss={train_loss_temp:.6f} | val_loss={avg_val_loss:.4f} | ema={ema_val:.4f} | lr={optimizer.param_groups[0]['lr']:.5f} | pred = {pred_vals_denorm[-1]:.4f}\")\n",
    "\n",
    "            if (epoch >= (epochs - 10)) and (fold + 1 == len(splits)):\n",
    "                preds_last_10.append(round(pred_vals_denorm[0], 4))\n",
    "            # if stop:\n",
    "            #     if verbose:\n",
    "            #         print(f\"[EarlyStop] best(ema)={early.best:.6f} → restore best weights\")\n",
    "            #     early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     break\n",
    "\n",
    "        # пост-валид метрики по датам target\n",
    "        pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "        print(f\"pred = {pred_vals_denorm[-1]:.4f}, {len(pred_vals_denorm)}\")\n",
    "        pred_series = pd.Series(pred_vals_denorm, index=pd.to_datetime(pred_dates))\n",
    "        gt_series   = pd.to_datetime(df['date']).map(pd.Timestamp).map(\n",
    "            lambda d: d\n",
    "        )  # просто для явности типов\n",
    "        # берём фактические target по тем же датам:\n",
    "        gt_series = df.set_index(pd.to_datetime(df['date']))['Target'].reindex(pred_series.index)\n",
    "\n",
    "        mae_target_pred = mean_absolute_error(gt_series, pred_series)\n",
    "        # dap_series = df.set_index(pd.to_datetime(df['date']))['DayAvgPrice'].reindex(pred_series.index)\n",
    "        dap_series = df.set_index(pd.to_datetime(df['date']))['c_month_real'].reindex(pred_series.index)\n",
    "        mae_target_dap  = mean_absolute_error(gt_series, dap_series)\n",
    "\n",
    "        if fold + 1 == len(splits):\n",
    "            fold_pred_series.append(pred_series)\n",
    "            fold_metrics.append({\n",
    "                \"fold\": fold + 1,\n",
    "                \"n_pred\": len(pred_series),\n",
    "                \"mae_target_pred\": float(mae_target_pred),\n",
    "                \"mae_target_dap\":  float(mae_target_dap),\n",
    "                \"val_loss_last_epoch\": float(avg_val_loss),\n",
    "            })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Fold {fold+1}] Target = {gt_series[-1]:.4f} | Predict = {pred_series[-1]:.4f}\")\n",
    "            print(f\"MAE(Target~DAP) = {mae_target_dap:.4f} | MAE(Target~Predict) = {mae_target_pred:.4f}\\n\")\n",
    "\n",
    "    torch.save(real_net.state_dict(), f\"../checkpoints/trained_real_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(complex_net.state_dict(), f\"../checkpoints/trained_complex_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(fusion_net.state_dict(), f\"../checkpoints/trained_fusion_net_{config['train']['lr']}.pth\")\n",
    "\n",
    "    # агрегированные результаты\n",
    "    results = {\n",
    "        \"fold_metrics\": fold_metrics,\n",
    "        \"pred_series_per_fold\": fold_pred_series,\n",
    "        \"preds_last_10\": preds_last_10,\n",
    "        \"mu\": mu, \"sigma\": sigma,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Вариант с двумя фолдами!!!\n",
    "\n",
    "# ---------- EarlyStopping ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=1e-2, mode=\"min\", restore_best=True, rel_delta=True):\n",
    "        assert mode in (\"min\", \"max\")\n",
    "        self.patience, self.min_delta, self.mode = patience, float(min_delta), mode\n",
    "        self.restore_best, self.rel_delta = restore_best, rel_delta\n",
    "        self.best, self.wait, self.best_states = None, 0, {}\n",
    "\n",
    "    def _improved(self, cur, best):\n",
    "        if self.mode == \"min\":\n",
    "            return cur < (best*(1-self.min_delta) if self.rel_delta else best - self.min_delta)\n",
    "        else:\n",
    "            return cur > (best*(1+self.min_delta) if self.rel_delta else best + self.min_delta)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, metric: float, models: Dict[str, torch.nn.Module]) -> bool:\n",
    "        if self.best is None or self._improved(metric, self.best):\n",
    "            self.best, self.wait = metric, 0\n",
    "            self.best_states = {k:{n:v.detach().cpu().clone() for n,v in m.state_dict().items()}\n",
    "                                for k,m in models.items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, models: Dict[str, torch.nn.Module]) -> None:\n",
    "        if not self.best_states: return\n",
    "        for k,m in models.items():\n",
    "            if k in self.best_states:\n",
    "                m.load_state_dict(self.best_states[k])\n",
    "\n",
    "\n",
    "# ---------- вспомогательные утилиты ----------\n",
    "def _set_seed(seed=42):\n",
    "    import os, random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _reset_weights(m: nn.Module):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "        m.reset_parameters()\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, p in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(p.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(p.data)\n",
    "                h = p.shape[0]//4\n",
    "                p.data[h:2*h] = 1.0  # forget-bias\n",
    "\n",
    "\n",
    "# ---------- основная функция ----------\n",
    "def train_tscv_model_7(config: dict, df: pd.DataFrame, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Обучение с кастомным TSCV, EMA-сглаживанием, EarlyStopping, LR-Plateau, grad-accum, clip-grad.\n",
    "    Возвращает:\n",
    "        results: dict с метриками по фолдам и сериями предсказаний\n",
    "    \"\"\"\n",
    "    _set_seed(config.get(\"seed\", 42))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- даты и срез df ---\n",
    "    start_preds_date = pd.to_datetime(config['data']['start_preds_date'], errors='coerce')\n",
    "    stop_preds_date  = pd.to_datetime(config['data']['stop_preds_date'],  errors='coerce')\n",
    "    time_shift       = int(config['data']['time_shift'])\n",
    "    window_size      = int(config['data']['window_size'])\n",
    "    lr_lambda_type = config['data']['lr_lambda']\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df[pd.to_datetime(df['date']) <= stop_preds_date].reset_index(drop=True)\n",
    "\n",
    "    interval_days = int((stop_preds_date - start_preds_date).days)\n",
    "    if verbose:\n",
    "        print(\"Dates:\", start_preds_date, \"→\", stop_preds_date,\n",
    "              \"| interval_days:\", interval_days, \"| time_shift:\", time_shift)\n",
    "\n",
    "    # --- сплиты ---\n",
    "    # splits = custom_time_series_split_3(\n",
    "    #     len(df),\n",
    "    #     time_shift=time_shift,\n",
    "    #     interval_days=interval_days,\n",
    "    #     main_splits_number=0,\n",
    "    #     initial_train_frac=0.99\n",
    "    # )\n",
    "    # splits = custom_time_series_split_2(len(df), time_shift=time_shift, interval_days=interval_days, main_splits_number=2, initial_train_frac=0.90)\n",
    "    splits = custom_time_series_split_4(len(df), time_shift=time_shift, interval_days=1, main_splits_number=1, initial_train_frac=0.90)\n",
    "    if verbose: print('num of splits =', len(splits))\n",
    "\n",
    "    # --- bootstrap-loader (определяем F и C до инициализации моделей) ---\n",
    "    first_train_idx, _ = splits[0]\n",
    "    bootstrap_loader = load_dataloader(\n",
    "        df.iloc[first_train_idx], window_size=window_size,\n",
    "        batch_size=config['train']['batch_size'],\n",
    "        shuffle=False, drop_last=False\n",
    "    )\n",
    "    first_batch = next(iter(bootstrap_loader))\n",
    "    F = first_batch['real_feats'].shape[-1]\n",
    "    C = (first_batch['complex_time_real'].shape[-1]\n",
    "         if ('complex_time_real' in first_batch and 'complex_time_imag' in first_batch) else 1)\n",
    "\n",
    "    # --- модели ---\n",
    "    real_net = RealCNNLSTM(\n",
    "        num_real_features=F,\n",
    "        hidden_dim=config['model']['real_hidden_dim'],\n",
    "        lstm_layers=config['model'].get('real_lstm_layers', 2),\n",
    "        dropout=config['model'].get('dropout', 0.3),\n",
    "        kernel_size=3,\n",
    "        bidirectional=config['model'].get('bidirectional', True),\n",
    "        take=config['model'].get('take', \"last_timestep\"),\n",
    "        proj_out=True\n",
    "    ).to(device)\n",
    "\n",
    "    complex_net = ComplexCNNLSTM(\n",
    "        in_channels=C,\n",
    "        hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        num_layers=config['model']['n_res_blocks'],\n",
    "        dropout=config['model']['dropout'],\n",
    "        kernel_size=3,\n",
    "        proj_out=False\n",
    "    ).to(device)\n",
    "\n",
    "    # fusion_net = FusionGated(\n",
    "    #     real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "    #     complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "    #     output_dim=1, hidden=128, dropout=0.2, use_softmax=True\n",
    "    # ).to(device)\n",
    "\n",
    "    fusion_net = FusionMagPhase(\n",
    "        real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "        complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        output_dim=1, hidden=128, dropout=0.2, use_softmax=True\n",
    "    ).to(device)\n",
    "\n",
    "    # вариант 1: только real-часть complex-linear (real)\n",
    "    # fusion_net = FusionComplexLinear(\n",
    "    #     real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "    #     complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "    #     output_dim=1,\n",
    "    #     hidden=128,\n",
    "    #     dropout=0.2,\n",
    "    #     complex_mode=\"real\"\n",
    "    # ).to(device)\n",
    "    \n",
    "    # # вариант 2: модуль комплексного линейного выхода (mag)\n",
    "    # fusion_net = FusionComplexLinear(\n",
    "    #     real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "    #     complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "    #     output_dim=1,\n",
    "    #     hidden=128,\n",
    "    #     dropout=0.2,\n",
    "    #     complex_mode=\"mag\"\n",
    "    # ).to(device)\n",
    "\n",
    "    # --- ✅ Инициализация bias последнего линейного слоя ---\n",
    "    # with torch.no_grad():\n",
    "    #     for mod in fusion_net.modules():\n",
    "    #         if isinstance(mod, torch.nn.Linear) and mod.out_features == 1:\n",
    "    #             mod.bias.fill_(0.0)   # так как таргет уже нормирован (среднее ~ 0)\n",
    "\n",
    "    # --- оптимизатор/лоссы/планировщик/ES ---\n",
    "    optimizer = optim.Adam(\n",
    "        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "        lr=config['train']['lr'], weight_decay=1e-4\n",
    "    )\n",
    "    criterion = nn.MSELoss() if config['train'].get('loss', 'mse') == 'mse' else nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "    epochs = int(config['train']['epochs'])\n",
    "    # warmup: 20% эпох, но не менее 1 и не более epochs-1\n",
    "    warmup_epochs = max(1, min(epochs - 1, int(0.2 * epochs)))\n",
    "    min_factor = 1e-3\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t))\n",
    "    else: \n",
    "        def lr_lambda(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 1.\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5\n",
    "            else:\n",
    "                return 0.1\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda_fold2(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs * 0.2\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t)) * 0.2\n",
    "    else: \n",
    "        def lr_lambda_fold2(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 0.4\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5 * 0.4\n",
    "            else:\n",
    "                return 0.1 * 0.4\n",
    "            \n",
    "    # else: \n",
    "    #     def lr_lambda(epoch: int):\n",
    "    #         if epoch < epochs // 4:\n",
    "    #             return 1.\n",
    "    #         elif epochs // 4 <= epoch < (epochs // 4) * 2:\n",
    "    #             return 0.5\n",
    "    #         elif (epochs // 4) * 2 <= epoch < (epochs // 4) * 3:\n",
    "    #             return 0.1\n",
    "    #         else:\n",
    "    #             return 0.05\n",
    "            \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    early = EarlyStopping(\n",
    "        patience=config['train'].get('early_stop_patience', 10),\n",
    "        min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "        mode=\"min\", restore_best=True, rel_delta=True\n",
    "    )\n",
    "    ema_beta = float(config['train'].get('ema_beta', 0.8))\n",
    "\n",
    "    # --- тренинг-опции ---\n",
    "    epochs         = int(config['train']['epochs'])\n",
    "    batch_size     = int(config['train']['batch_size'])\n",
    "    accum_steps    = int(config['train'].get('accum_steps', 1))\n",
    "    max_grad_norm  = config['train'].get('max_grad_norm', 1.0)\n",
    "    min_bs_for_bn  = int(config['train'].get('min_bs_for_bn', 2))\n",
    "    reset_each_fold= bool(config['train'].get('reset_each_fold', False))\n",
    "\n",
    "    if verbose: print('size of df:', df.shape)\n",
    "\n",
    "    # --- нормализация (фиксируем μ,σ по первому train-сплиту) ---\n",
    "    cols_to_scale = config['data'].get('cols_to_scale', [\n",
    "        'DayAvgPrice','IntradayStd','Volume','Log_Profit','DayAvgPrice_diff',\n",
    "        'DAP_1','DAP_2','DAP_3','DAP_4','DAP_5','DAP_6',\n",
    "        'POLY_1','POLY_2','POLY_3','lambda_C3','lambda_C2','lambda_C1'\n",
    "    ])\n",
    "\n",
    "    mu = sigma = None\n",
    "    mu_target = sigma_target = None\n",
    "\n",
    "    # --- сбор результатов по фолдам ---\n",
    "    fold_metrics: List[Dict] = []\n",
    "    fold_pred_series: List[pd.Series] = []\n",
    "    preds_last_10 = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        # опциональный ресет весов (для «честной» CV)\n",
    "        if reset_each_fold and (fold + 1) > 1:\n",
    "            # real_net.apply(_reset_weights)\n",
    "            # complex_net.apply(_reset_weights)\n",
    "            # fusion_net.apply(_reset_weights)\n",
    "            # optimizer = optim.Adam(\n",
    "            #     list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "            #     lr=config['train']['lr']\n",
    "            # )\n",
    "            # сбросить состояние LR-схем, ES, EMA\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=100, threshold=1e-3, threshold_mode='rel',\n",
    "            #     cooldown=2, verbose=True\n",
    "            # )\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=5, verbose=verbose\n",
    "            # )\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fold2)\n",
    "            early = EarlyStopping(\n",
    "                patience=config['train'].get('early_stop_patience', 10),\n",
    "                min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "                mode=\"min\", restore_best=True, rel_delta=True\n",
    "            )\n",
    "            ema_val = None\n",
    "        else:\n",
    "            ema_val = None  # сбрасываем EMA в любом случае на новый фолд (иначе метрики не сопоставимы)\n",
    "\n",
    "        train_df = df.iloc[train_idx].copy()\n",
    "\n",
    "        # расширим val назад до window_size, безопасно по границам\n",
    "        if len(val_idx) <= window_size:\n",
    "            end = val_idx[-1]\n",
    "            start = max(0, end - window_size)\n",
    "            val_idx = list(range(start, end + 1))\n",
    "\n",
    "        val_df = df.iloc[val_idx].copy()\n",
    "\n",
    "        # μ,σ считаем по ПЕРВОМУ train-сплиту\n",
    "        if mu is None or sigma is None:\n",
    "            mu = train_df[cols_to_scale].mean()\n",
    "            sigma = train_df[cols_to_scale].std().replace(0, 1.0)\n",
    "\n",
    "        # применяем нормировку\n",
    "        for col in cols_to_scale:\n",
    "            train_df.loc[:, col] = (train_df[col] - mu[col]) / sigma[col]\n",
    "            val_df.loc[:,   col] = (val_df[col]   - mu[col]) / sigma[col]\n",
    "\n",
    "        # μ,σ для z-нормировки Target\n",
    "        if mu_target is None or sigma_target is None:\n",
    "            mu_target = train_df['Target'].mean()\n",
    "            sigma_target = train_df['Target'].std()\n",
    "\n",
    "        train_df['Target'] = (train_df['Target'] - mu_target) / sigma_target\n",
    "        val_df['Target'] = (val_df['Target']   - mu_target) / sigma_target \n",
    "        train_df['Target_smooth'] = (train_df['Target_smooth'] - mu_target) / sigma_target\n",
    "        val_df['Target_smooth'] = (val_df['Target_smooth']   - mu_target) / sigma_target  \n",
    "\n",
    "        #print(f\"\\ntrain_end {train_df.iloc[-1]}, val_end {val_df.iloc[-1]}\")  \n",
    "\n",
    "        # лоадеры\n",
    "        train_loader = load_dataloader(train_df, window_size, batch_size, shuffle=True,  drop_last=False)\n",
    "        val_loader   = load_dataloader(val_df,   window_size, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFold {fold+1}: train {len(train_df)}, val {len(val_df)}\",\n",
    "                  \"\\ntrain first...last :\", train_df['date'].min(), \"...\" , train_df['date'].max(),\n",
    "                  \"\\nval first...last   :\", val_df['date'].min(), \"...\" ,   val_df['date'].max())\n",
    "\n",
    "        # контрольный принт последнего окна\n",
    "            \n",
    "        dst   = TwoStreamDataset(train_df, window_size=window_size)\n",
    "        sampt = dst[len(dst) - 1]\n",
    "        # last_DAP_in_t_window = float(sampt['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        last_DAP_in_t_window = float(sampt['real_feats'][-1, 0]) * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        target_pricet           = float(sampt['target']) * sigma_target + mu_target\n",
    "        target_smooth_pricet = train_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"train last row | DAP={last_DAP_in_t_window:.4f} | Target={target_pricet:.4f} | Target_smooth={target_smooth_pricet:.4f}\")\n",
    "\n",
    "        ds   = TwoStreamDataset(val_df, window_size=window_size)\n",
    "        samp = ds[len(ds) - 1]\n",
    "        last_DAP_in_val_window = float(samp['real_feats'][-1, 0]) * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        # last_DAP_in_val_window = float(samp['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        target_price           = float(samp['target']) * sigma_target + mu_target\n",
    "        target_smooth_price = val_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"val last row   | DAP={last_DAP_in_val_window:.4f} | Target={target_price:.4f} | Target_smooth={target_smooth_price:.4f}\")\n",
    "\n",
    "        # Графики\n",
    "        # from utils_live_plot import EMAMeter, TrainingPlotter  # если вынес в файл, иначе опусти импорт\n",
    "        # ema_meter    = EMAMeter(beta=config['train'].get('ema_beta', 0.8))\n",
    "        # live_plotter = TrainingPlotter(max_points=500)\n",
    "\n",
    "        # ----- ЭПОХИ -----\n",
    "        for epoch in range(epochs):\n",
    "            real_net.train(); complex_net.train(); fusion_net.train()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # TRAIN\n",
    "            step = 0\n",
    "            for step, batch in enumerate(train_loader, start=1):\n",
    "                x_real = batch['real_feats'].to(device)\n",
    "                if x_real.size(0) < min_bs_for_bn:\n",
    "                    continue\n",
    "                x_complex = torch.complex(\n",
    "                    batch['complex_time_real'].to(device),\n",
    "                    batch['complex_time_imag'].to(device)\n",
    "                )\n",
    "                y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                h_real = real_net(x_real)\n",
    "                h_r, h_i = complex_net(x_complex)\n",
    "                out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                loss = criterion(out, y) / accum_steps\n",
    "                if step == len(train_loader) - 1: train_loss_temp = loss\n",
    "                loss.backward()\n",
    "\n",
    "                if step % accum_steps == 0:\n",
    "                    if max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                            max_grad_norm\n",
    "                        )\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # доводим хвост аккумулирования\n",
    "            if step % accum_steps != 0:\n",
    "                if max_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                        max_grad_norm\n",
    "                    )\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # VALID\n",
    "            real_net.eval(); complex_net.eval(); fusion_net.eval()\n",
    "            val_loss_sum, n_val = 0.0, 0\n",
    "            pred_vals, pred_dates = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_real = batch['real_feats'].to(device)\n",
    "                    x_complex = torch.complex(\n",
    "                        batch['complex_time_real'].to(device),\n",
    "                        batch['complex_time_imag'].to(device)\n",
    "                    )\n",
    "                    y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                    h_real = real_net(x_real)\n",
    "                    h_r, h_i = complex_net(x_complex)\n",
    "                    out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                    val_loss_sum += criterion(out, y).item()\n",
    "                    n_val += 1\n",
    "\n",
    "                    # даты предиктов\n",
    "                    pred_vals.extend(out.squeeze(-1).cpu().numpy())\n",
    "\n",
    "                    if 'target_date' in batch:\n",
    "                        pred_dates.extend(list(batch['target_date']))\n",
    "                    else:\n",
    "                        # fallback: считаем по глобальному индексу\n",
    "                        batch_idx_local  = batch['row_idx'].cpu().numpy()   # индекс окна в val_df\n",
    "                        global_start     = val_idx[0]                        # смещение в исходном df\n",
    "                        batch_idx_global = global_start + batch_idx_local\n",
    "                        # y = target[idx + window_size] → цель на \"завтра\" относительно начала окна\n",
    "                        tgt_idx = batch_idx_global + window_size\n",
    "                        pred_dates.extend(df['date'].iloc[tgt_idx].tolist())\n",
    "\n",
    "            avg_val_loss = val_loss_sum / max(1, n_val)\n",
    "\n",
    "            # --- EMA для scheduler/early ---\n",
    "            ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            use_metric = ema_val\n",
    "\n",
    "            scheduler.step()\n",
    "            # if epoch+1 >= config['train'].get('early_stop_warmup', 0):\n",
    "            #     scheduler.step()\n",
    "            #     #scheduler.step(use_metric)\n",
    "            #     stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     # диагностика (разово/по условию)\n",
    "            #     # print(f\"[ES] e{epoch+1} metric={use_metric:.5f}, best={early.best}, wait={early.wait}\")\n",
    "            #     if stop:\n",
    "            #         if verbose:\n",
    "            #             print(f\"[EarlyStop] best(ema)={early.best:.4f} → restore best weights\")\n",
    "            #         early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #         break\n",
    "            # else:\n",
    "            #     pass\n",
    "\n",
    "            # EMA для scheduler/early\n",
    "            # ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            # use_metric = ema_val  # <— ключевая строка: ES и Plateau смотрят на одно и то же\n",
    "\n",
    "            # Графики\n",
    "            # если ты уже считаешь ema_val сам — можно синхронизировать:\n",
    "            # ema_val = ema_meter.update(avg_val_loss)\n",
    "            # # обновляем график каждые, скажем, 1-5 эпох\n",
    "            # live_plotter.update(epoch+1, avg_val_loss, ema_val)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Validation Loss vs EMA\")\n",
    "            # Графики\n",
    "            # pred_val_denorm = [x * sigma_target + mu_target for x in pred_vals][0]    # откат нормировки целевой переменной\n",
    "            # live_plotter.update(epoch+1, pred_val_denorm, last_DAP_in_val_window)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Predict vs DAP\")\n",
    "            # Графики\n",
    "\n",
    "            # scheduler.step(use_metric)\n",
    "            # stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "            if verbose and ((epoch+1) % 4 == 0 or (epoch+1) == 1):\n",
    "                print(f\"Epoch {epoch+1:03d} | train_loss={train_loss_temp:.6f} | val_loss={avg_val_loss:.4f} | ema={ema_val:.4f} | lr={optimizer.param_groups[0]['lr']:.5f} | pred = {pred_vals_denorm[-1]:.4f}\")\n",
    "\n",
    "            if (epoch >= (epochs - 10)) and (fold + 1 == len(splits)):\n",
    "                preds_last_10.append(round(pred_vals_denorm[0], 4))\n",
    "            # if stop:\n",
    "            #     if verbose:\n",
    "            #         print(f\"[EarlyStop] best(ema)={early.best:.6f} → restore best weights\")\n",
    "            #     early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     break\n",
    "\n",
    "        # пост-валид метрики по датам target\n",
    "        pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "        print(f\"pred = {pred_vals_denorm[-1]:.4f}, {len(pred_vals_denorm)}\")\n",
    "        pred_series = pd.Series(pred_vals_denorm, index=pd.to_datetime(pred_dates))\n",
    "        gt_series   = pd.to_datetime(df['date']).map(pd.Timestamp).map(\n",
    "            lambda d: d\n",
    "        )  # просто для явности типов\n",
    "        # берём фактические target по тем же датам:\n",
    "        gt_series = df.set_index(pd.to_datetime(df['date']))['Target'].reindex(pred_series.index)\n",
    "\n",
    "        mae_target_pred = mean_absolute_error(gt_series, pred_series)\n",
    "        dap_series = df.set_index(pd.to_datetime(df['date']))['DayAvgPrice'].reindex(pred_series.index)\n",
    "        # dap_series = df.set_index(pd.to_datetime(df['date']))['c_month_real'].reindex(pred_series.index)\n",
    "        mae_target_dap  = mean_absolute_error(gt_series, dap_series)\n",
    "\n",
    "        if fold + 1 == len(splits):\n",
    "            fold_pred_series.append(pred_series)\n",
    "            fold_metrics.append({\n",
    "                \"fold\": fold + 1,\n",
    "                \"n_pred\": len(pred_series),\n",
    "                \"mae_target_pred\": float(mae_target_pred),\n",
    "                \"mae_target_dap\":  float(mae_target_dap),\n",
    "                \"val_loss_last_epoch\": float(avg_val_loss),\n",
    "            })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Fold {fold+1}] Target = {gt_series[-1]:.4f} | Predict = {pred_series[-1]:.4f}\")\n",
    "            print(f\"MAE(Target~DAP) = {mae_target_dap:.4f} | MAE(Target~Predict) = {mae_target_pred:.4f}\\n\")\n",
    "\n",
    "    torch.save(real_net.state_dict(), f\"../checkpoints/trained_real_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(complex_net.state_dict(), f\"../checkpoints/trained_complex_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(fusion_net.state_dict(), f\"../checkpoints/trained_fusion_net_{config['train']['lr']}.pth\")\n",
    "\n",
    "    # агрегированные результаты\n",
    "    results = {\n",
    "        \"fold_metrics\": fold_metrics,\n",
    "        \"pred_series_per_fold\": fold_pred_series,\n",
    "        \"preds_last_10\": preds_last_10,\n",
    "        \"mu\": mu, \"sigma\": sigma,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['open', 'high', 'low', 'close', 'DayAvgPrice', 'IntradayStd',\n",
    "       'Volume', 'day_of_week', 'day_of_year', 'Log_Profit',\n",
    "       'DayAvgPrice_diff', 'DayAvgPrice_2diff', 'POLY_1', 'POLY_2', 'POLY_3',\n",
    "       'parkinson_vol', 'parkinson_vol_ma5',\n",
    "       'parkinson_vol_ma20', 'parkinson_vol_diff1', 'parkinson_vol_lag1',\n",
    "       'DAP_1', 'DAP_2', 'DAP_3', 'DAP_4', 'DAP_5', 'DAP_6', 'DAP_7', 'DAP_10', \n",
    "       'DayAvgPrice_roll5', 'DayAvgPrice_roll10', 'DayAvgPrice_roll20',\n",
    "       'DayAvgPrice_ema5', 'DayAvgPrice_ema20', 'IntradayStd_roll5',\n",
    "       'IntradayStd_roll10', 'IntradayStd_roll20', 'IntradayStd_ema5',\n",
    "       'IntradayStd_ema20', 'mean_w', 'std_w',\n",
    "       'q10_w', 'q90_w', 'slope_w', 'vol_w', 'macd', 'macd_signal', 'macd_hist',\n",
    "       'bb_low', 'bb_mid', 'bb_up', 'atr14', 'dphi_hilbert',\n",
    "       'stft_energy_low', 'stft_energy_mid', 'stft_energy_high',\n",
    "       'gk_sigma', 'rs_sigma', 'yz_sigma', 'adx', 'chop', 'kalm_slope', 'rv20', 'vol_of_vol', \n",
    "       'bb_pct_b', 'bb_bandwidth', 'ret_overnight', 'ret_intraday', 'corr_ret_dlogvol', \n",
    "       'c_month_real',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"seed\": 42,\n",
    "  \"data\": {\n",
    "    \"start_preds_date\": \"2025-11-10\",\n",
    "    \"stop_preds_date\": \"2025-11-12\",\n",
    "    \"time_shift\": 1,\n",
    "    \"window_size\": 21,\n",
    "    \"cols_to_scale\": cols_to_scale,\n",
    "    \"lr_lambda\": \"cos\"\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"real_hidden_dim\": 128,\n",
    "    \"complex_hidden_dim\": 128,\n",
    "    \"n_res_blocks\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"bidirectional\": True,\n",
    "    \"take\": \"last_timestep\",\n",
    "    \"real_lstm_layers\": 2\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"lr\": 0.001,\n",
    "    \"loss\": \"SmoothL1Loss\",\n",
    "    \"epochs\": 64,\n",
    "    \"batch_size\": 96,\n",
    "    \"accum_steps\": 1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"min_bs_for_bn\": 2,\n",
    "    \"early_stop_patience\": 200,\n",
    "    \"early_stop_min_delta\": 1e-2,\n",
    "    \"ema_beta\": 0.8,\n",
    "    \"reset_each_fold\": True\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем конфигурацию\n",
    "df = pd.read_csv(f\"../data_archiv/DTG/DTG_new_fea_to_14_11_2025_1d_w{config['data']['window_size']}_noweekend.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DayAvgPrice</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>35.094444</td>\n",
       "      <td>34.881389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>34.881389</td>\n",
       "      <td>35.007222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>35.007222</td>\n",
       "      <td>34.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>34.865000</td>\n",
       "      <td>34.751945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>34.751945</td>\n",
       "      <td>34.606944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>34.606944</td>\n",
       "      <td>33.886111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>2025-11-04</td>\n",
       "      <td>33.886111</td>\n",
       "      <td>34.891389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>34.891389</td>\n",
       "      <td>35.163611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>2025-11-06</td>\n",
       "      <td>35.163611</td>\n",
       "      <td>34.328333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>34.328333</td>\n",
       "      <td>35.753889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>35.753889</td>\n",
       "      <td>35.886111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>35.886111</td>\n",
       "      <td>36.299444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>36.299444</td>\n",
       "      <td>35.998056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>35.998056</td>\n",
       "      <td>35.215278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>35.215278</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  DayAvgPrice     Target\n",
       "918  2025-10-27    35.094444  34.881389\n",
       "919  2025-10-28    34.881389  35.007222\n",
       "920  2025-10-29    35.007222  34.865000\n",
       "921  2025-10-30    34.865000  34.751945\n",
       "922  2025-10-31    34.751945  34.606944\n",
       "923  2025-11-03    34.606944  33.886111\n",
       "924  2025-11-04    33.886111  34.891389\n",
       "925  2025-11-05    34.891389  35.163611\n",
       "926  2025-11-06    35.163611  34.328333\n",
       "927  2025-11-07    34.328333  35.753889\n",
       "928  2025-11-10    35.753889  35.886111\n",
       "929  2025-11-11    35.886111  36.299444\n",
       "930  2025-11-12    36.299444  35.998056\n",
       "931  2025-11-13    35.998056  35.215278\n",
       "932  2025-11-14    35.215278   0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаём Target, пустые последние значения заменяются 0.\n",
    "H = config['data']['time_shift']     # time shift\n",
    "# df['Target'] = df['DayAvgPrice'].shift( - H).fillna(method='ffill')\n",
    "# df['Target'].iloc[-H:] = 0\n",
    "df['Target'] = df['DayAvgPrice'].shift( - H).fillna(0)\n",
    "df[['date', 'DayAvgPrice', 'Target']].tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c_month_real'] = df['DayAvgPrice'].copy()\n",
    "df['c_month_imag'] = df['IntradayStd'].copy()\n",
    "df['DayAvgPrice'] = df['c_week_real'].copy()\n",
    "df['IntradayStd'] = df['c_week_imag'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: 2025-11-10 00:00:00 → 2025-11-12 00:00:00 | interval_days: 2 | time_shift: 1\n",
      "num of splits = 3\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 836, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-02 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=40.2486 | Target=40.7472 | Target_smooth=39.9818\n",
      "val last row   | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.4015 | val_loss=0.2843 | ema=0.2843 | lr=0.00050 | pred = 33.3141 | gate_real=0.779 | gate_abs=0.221\n",
      "Epoch 004 | train_loss=0.0988 | val_loss=0.0551 | ema=0.1924 | lr=0.00125 | pred = 36.0061 | gate_real=0.986 | gate_abs=0.014\n",
      "Epoch 008 | train_loss=0.0312 | val_loss=0.0651 | ema=0.1014 | lr=0.00225 | pred = 32.6867 | gate_real=0.963 | gate_abs=0.037\n",
      "Epoch 012 | train_loss=0.0227 | val_loss=0.0078 | ema=0.0546 | lr=0.00300 | pred = 36.6776 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 016 | train_loss=0.0103 | val_loss=0.0060 | ema=0.0296 | lr=0.00296 | pred = 35.6351 | gate_real=0.985 | gate_abs=0.015\n",
      "Epoch 020 | train_loss=0.0134 | val_loss=0.0075 | ema=0.0194 | lr=0.00283 | pred = 35.4179 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 024 | train_loss=0.0143 | val_loss=0.0112 | ema=0.0186 | lr=0.00262 | pred = 35.2094 | gate_real=0.987 | gate_abs=0.013\n",
      "Epoch 028 | train_loss=0.0114 | val_loss=0.0095 | ema=0.0169 | lr=0.00235 | pred = 35.0259 | gate_real=0.967 | gate_abs=0.033\n",
      "Epoch 032 | train_loss=0.0076 | val_loss=0.0105 | ema=0.0165 | lr=0.00203 | pred = 34.8034 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 036 | train_loss=0.0109 | val_loss=0.0270 | ema=0.0223 | lr=0.00168 | pred = 34.5206 | gate_real=0.972 | gate_abs=0.028\n",
      "Epoch 040 | train_loss=0.0097 | val_loss=0.0096 | ema=0.0150 | lr=0.00132 | pred = 34.9039 | gate_real=0.968 | gate_abs=0.032\n",
      "Epoch 044 | train_loss=0.0077 | val_loss=0.0194 | ema=0.0161 | lr=0.00097 | pred = 34.6011 | gate_real=0.973 | gate_abs=0.027\n",
      "Epoch 048 | train_loss=0.0067 | val_loss=0.0105 | ema=0.0138 | lr=0.00065 | pred = 35.2543 | gate_real=0.975 | gate_abs=0.025\n",
      "Epoch 052 | train_loss=0.0079 | val_loss=0.0269 | ema=0.0176 | lr=0.00038 | pred = 34.4913 | gate_real=0.971 | gate_abs=0.029\n",
      "Epoch 056 | train_loss=0.0073 | val_loss=0.0157 | ema=0.0160 | lr=0.00017 | pred = 34.6596 | gate_real=0.971 | gate_abs=0.029\n",
      "Epoch 060 | train_loss=0.0059 | val_loss=0.0138 | ema=0.0150 | lr=0.00004 | pred = 34.9093 | gate_real=0.971 | gate_abs=0.029\n",
      "Epoch 064 | train_loss=0.0095 | val_loss=0.0146 | ema=0.0147 | lr=0.00000 | pred = 34.8280 | gate_real=0.971 | gate_abs=0.029\n",
      "pred = 34.8280, 72\n",
      "[Fold 1] Target = 35.8861 | Predict = 34.8280\n",
      "MAE(Target~DAP) = 0.4384 | MAE(Target~Predict) = 0.7403\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-13 ... 2025-11-11\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "val last row   | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "Epoch 001 | train_loss=0.0102 | val_loss=0.0078 | ema=0.0078 | lr=0.00010 | pred = 35.6263 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 004 | train_loss=0.0118 | val_loss=0.0062 | ema=0.0080 | lr=0.00025 | pred = 35.6978 | gate_real=0.975 | gate_abs=0.025\n",
      "Epoch 008 | train_loss=0.0084 | val_loss=0.0043 | ema=0.0065 | lr=0.00045 | pred = 35.7971 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 012 | train_loss=0.0070 | val_loss=0.0087 | ema=0.0055 | lr=0.00060 | pred = 35.5885 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 016 | train_loss=0.0081 | val_loss=0.0002 | ema=0.0038 | lr=0.00059 | pred = 36.1930 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 020 | train_loss=0.0070 | val_loss=0.0060 | ema=0.0029 | lr=0.00057 | pred = 35.7073 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 024 | train_loss=0.0054 | val_loss=0.0007 | ema=0.0022 | lr=0.00052 | pred = 36.1046 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 028 | train_loss=0.0066 | val_loss=0.0000 | ema=0.0018 | lr=0.00047 | pred = 36.2897 | gate_real=0.974 | gate_abs=0.026\n",
      "Epoch 032 | train_loss=0.0059 | val_loss=0.0022 | ema=0.0023 | lr=0.00041 | pred = 35.9450 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 036 | train_loss=0.0082 | val_loss=0.0008 | ema=0.0013 | lr=0.00034 | pred = 36.0875 | gate_real=0.973 | gate_abs=0.027\n",
      "Epoch 040 | train_loss=0.0067 | val_loss=0.0021 | ema=0.0011 | lr=0.00026 | pred = 35.9532 | gate_real=0.972 | gate_abs=0.028\n",
      "Epoch 044 | train_loss=0.0057 | val_loss=0.0017 | ema=0.0023 | lr=0.00019 | pred = 35.9806 | gate_real=0.968 | gate_abs=0.032\n",
      "Epoch 048 | train_loss=0.0083 | val_loss=0.0000 | ema=0.0013 | lr=0.00013 | pred = 36.2515 | gate_real=0.968 | gate_abs=0.032\n",
      "Epoch 052 | train_loss=0.0072 | val_loss=0.0006 | ema=0.0010 | lr=0.00008 | pred = 36.1103 | gate_real=0.969 | gate_abs=0.031\n",
      "Epoch 056 | train_loss=0.0071 | val_loss=0.0007 | ema=0.0007 | lr=0.00003 | pred = 36.0972 | gate_real=0.969 | gate_abs=0.031\n",
      "Epoch 060 | train_loss=0.0107 | val_loss=0.0006 | ema=0.0006 | lr=0.00001 | pred = 36.1182 | gate_real=0.969 | gate_abs=0.031\n",
      "Epoch 064 | train_loss=0.0070 | val_loss=0.0007 | ema=0.0006 | lr=0.00000 | pred = 36.1047 | gate_real=0.969 | gate_abs=0.031\n",
      "pred = 36.1047, 1\n",
      "[Fold 2] Target = 36.2994 | Predict = 36.1047\n",
      "MAE(Target~DAP) = 0.4133 | MAE(Target~Predict) = 0.1948\n",
      "\n",
      "\n",
      "Fold 3: train 930, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-11 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "val last row   | DAP=36.2994 | Target=35.9981 | Target_smooth=35.7569\n",
      "Epoch 001 | train_loss=0.0063 | val_loss=0.0000 | ema=0.0000 | lr=0.00010 | pred = 35.9602 | gate_real=0.970 | gate_abs=0.030\n",
      "Epoch 004 | train_loss=0.0056 | val_loss=0.0001 | ema=0.0000 | lr=0.00025 | pred = 36.0896 | gate_real=0.970 | gate_abs=0.030\n",
      "Epoch 008 | train_loss=0.0071 | val_loss=0.0042 | ema=0.0010 | lr=0.00045 | pred = 36.4934 | gate_real=0.970 | gate_abs=0.030\n",
      "Epoch 012 | train_loss=0.0097 | val_loss=0.0027 | ema=0.0012 | lr=0.00060 | pred = 35.5989 | gate_real=0.975 | gate_abs=0.025\n",
      "Epoch 016 | train_loss=0.0146 | val_loss=0.0005 | ema=0.0016 | lr=0.00059 | pred = 36.1698 | gate_real=0.969 | gate_abs=0.031\n",
      "Epoch 020 | train_loss=0.0101 | val_loss=0.0020 | ema=0.0014 | lr=0.00057 | pred = 36.3359 | gate_real=0.972 | gate_abs=0.028\n",
      "Epoch 024 | train_loss=0.0096 | val_loss=0.0008 | ema=0.0010 | lr=0.00052 | pred = 35.7795 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 028 | train_loss=0.0062 | val_loss=0.0004 | ema=0.0013 | lr=0.00047 | pred = 36.1552 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 032 | train_loss=0.0072 | val_loss=0.0047 | ema=0.0016 | lr=0.00041 | pred = 36.5208 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 036 | train_loss=0.0141 | val_loss=0.0002 | ema=0.0014 | lr=0.00034 | pred = 35.8798 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 040 | train_loss=0.0082 | val_loss=0.0013 | ema=0.0011 | lr=0.00026 | pred = 36.2715 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 044 | train_loss=0.0084 | val_loss=0.0003 | ema=0.0008 | lr=0.00019 | pred = 36.1230 | gate_real=0.974 | gate_abs=0.026\n",
      "Epoch 048 | train_loss=0.0094 | val_loss=0.0009 | ema=0.0006 | lr=0.00013 | pred = 36.2226 | gate_real=0.975 | gate_abs=0.025\n",
      "Epoch 052 | train_loss=0.0076 | val_loss=0.0015 | ema=0.0006 | lr=0.00008 | pred = 36.2965 | gate_real=0.975 | gate_abs=0.025\n",
      "Epoch 056 | train_loss=0.0138 | val_loss=0.0006 | ema=0.0006 | lr=0.00003 | pred = 36.1852 | gate_real=0.975 | gate_abs=0.025\n",
      "Epoch 060 | train_loss=0.0066 | val_loss=0.0007 | ema=0.0006 | lr=0.00001 | pred = 36.1961 | gate_real=0.975 | gate_abs=0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  10%|█         | 1/10 [12:37<1:53:33, 757.03s/it, last_mae_pred=0.1733, last_mae_dap=0.4133, avg_mae_pred=0.1733, avg_mae_dap=0.4133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.0074 | val_loss=0.0005 | ema=0.0006 | lr=0.00000 | pred = 36.1714 | gate_real=0.975 | gate_abs=0.025\n",
      "pred = 36.1714, 1\n",
      "[Fold 3] Target = 35.9981 | Predict = 36.1714\n",
      "MAE(Target~DAP) = 0.3014 | MAE(Target~Predict) = 0.1733\n",
      "\n",
      "[36.0946, 36.1852, 36.1512, 36.1745, 36.1898, 36.1961, 36.182, 36.1744, 36.1724, 36.1714]  | mean=36.1692 | std=0.0289\n",
      "\n",
      "Dates: 2025-11-10 00:00:00 → 2025-11-12 00:00:00 | interval_days: 2 | time_shift: 1\n",
      "num of splits = 3\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 836, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-02 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=40.2486 | Target=40.7472 | Target_smooth=39.9818\n",
      "val last row   | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.4459 | val_loss=0.3702 | ema=0.3702 | lr=0.00033 | pred = 33.0257 | gate_real=0.542 | gate_abs=0.458\n",
      "Epoch 004 | train_loss=0.1389 | val_loss=0.1080 | ema=0.2287 | lr=0.00083 | pred = 33.8977 | gate_real=0.711 | gate_abs=0.289\n",
      "Epoch 008 | train_loss=0.0710 | val_loss=0.0144 | ema=0.1494 | lr=0.00150 | pred = 35.2819 | gate_real=0.791 | gate_abs=0.209\n",
      "Epoch 012 | train_loss=0.0237 | val_loss=0.0056 | ema=0.0688 | lr=0.00200 | pred = 35.9496 | gate_real=0.840 | gate_abs=0.160\n",
      "Epoch 016 | train_loss=0.0123 | val_loss=0.0170 | ema=0.0377 | lr=0.00197 | pred = 35.2229 | gate_real=0.899 | gate_abs=0.101\n",
      "Epoch 020 | train_loss=0.0250 | val_loss=0.0345 | ema=0.0299 | lr=0.00189 | pred = 33.8415 | gate_real=0.885 | gate_abs=0.115\n",
      "Epoch 024 | train_loss=0.0058 | val_loss=0.0156 | ema=0.0208 | lr=0.00175 | pred = 35.3406 | gate_real=0.911 | gate_abs=0.089\n",
      "Epoch 028 | train_loss=0.0160 | val_loss=0.0257 | ema=0.0178 | lr=0.00157 | pred = 34.3847 | gate_real=0.902 | gate_abs=0.098\n",
      "Epoch 032 | train_loss=0.0136 | val_loss=0.0112 | ema=0.0158 | lr=0.00135 | pred = 35.2377 | gate_real=0.918 | gate_abs=0.082\n",
      "Epoch 036 | train_loss=0.0128 | val_loss=0.0145 | ema=0.0164 | lr=0.00112 | pred = 35.1832 | gate_real=0.909 | gate_abs=0.091\n",
      "Epoch 040 | train_loss=0.0088 | val_loss=0.0239 | ema=0.0184 | lr=0.00088 | pred = 34.8413 | gate_real=0.923 | gate_abs=0.077\n",
      "Epoch 044 | train_loss=0.0078 | val_loss=0.0129 | ema=0.0145 | lr=0.00065 | pred = 35.2073 | gate_real=0.918 | gate_abs=0.082\n",
      "Epoch 048 | train_loss=0.0078 | val_loss=0.0135 | ema=0.0132 | lr=0.00043 | pred = 35.2057 | gate_real=0.923 | gate_abs=0.077\n",
      "Epoch 052 | train_loss=0.0066 | val_loss=0.0160 | ema=0.0140 | lr=0.00025 | pred = 34.9764 | gate_real=0.925 | gate_abs=0.075\n",
      "Epoch 056 | train_loss=0.0066 | val_loss=0.0136 | ema=0.0135 | lr=0.00011 | pred = 35.1846 | gate_real=0.926 | gate_abs=0.074\n",
      "Epoch 060 | train_loss=0.0055 | val_loss=0.0142 | ema=0.0136 | lr=0.00003 | pred = 35.1469 | gate_real=0.926 | gate_abs=0.074\n",
      "Epoch 064 | train_loss=0.0080 | val_loss=0.0139 | ema=0.0138 | lr=0.00000 | pred = 35.1732 | gate_real=0.927 | gate_abs=0.073\n",
      "pred = 35.1732, 72\n",
      "[Fold 1] Target = 35.8861 | Predict = 35.1732\n",
      "MAE(Target~DAP) = 0.4384 | MAE(Target~Predict) = 0.7188\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-13 ... 2025-11-11\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "val last row   | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "Epoch 001 | train_loss=0.0060 | val_loss=0.0059 | ema=0.0059 | lr=0.00007 | pred = 35.7135 | gate_real=0.914 | gate_abs=0.086\n",
      "Epoch 004 | train_loss=0.0129 | val_loss=0.0072 | ema=0.0060 | lr=0.00017 | pred = 35.6518 | gate_real=0.915 | gate_abs=0.085\n",
      "Epoch 008 | train_loss=0.0090 | val_loss=0.0036 | ema=0.0049 | lr=0.00030 | pred = 35.8427 | gate_real=0.922 | gate_abs=0.078\n",
      "Epoch 012 | train_loss=0.0064 | val_loss=0.0032 | ema=0.0047 | lr=0.00040 | pred = 35.8669 | gate_real=0.923 | gate_abs=0.077\n",
      "Epoch 016 | train_loss=0.0122 | val_loss=0.0060 | ema=0.0047 | lr=0.00039 | pred = 35.7068 | gate_real=0.918 | gate_abs=0.082\n",
      "Epoch 020 | train_loss=0.0065 | val_loss=0.0011 | ema=0.0045 | lr=0.00038 | pred = 36.0418 | gate_real=0.930 | gate_abs=0.070\n",
      "Epoch 024 | train_loss=0.0066 | val_loss=0.0057 | ema=0.0043 | lr=0.00035 | pred = 35.7259 | gate_real=0.923 | gate_abs=0.077\n",
      "Epoch 028 | train_loss=0.0114 | val_loss=0.0040 | ema=0.0057 | lr=0.00031 | pred = 35.8199 | gate_real=0.924 | gate_abs=0.076\n",
      "Epoch 032 | train_loss=0.0069 | val_loss=0.0004 | ema=0.0032 | lr=0.00027 | pred = 36.1565 | gate_real=0.931 | gate_abs=0.069\n",
      "Epoch 036 | train_loss=0.0084 | val_loss=0.0025 | ema=0.0030 | lr=0.00022 | pred = 35.9210 | gate_real=0.928 | gate_abs=0.072\n",
      "Epoch 040 | train_loss=0.0155 | val_loss=0.0050 | ema=0.0042 | lr=0.00018 | pred = 35.7610 | gate_real=0.929 | gate_abs=0.071\n",
      "Epoch 044 | train_loss=0.0063 | val_loss=0.0042 | ema=0.0035 | lr=0.00013 | pred = 35.8043 | gate_real=0.927 | gate_abs=0.073\n",
      "Epoch 048 | train_loss=0.0088 | val_loss=0.0029 | ema=0.0032 | lr=0.00009 | pred = 35.8872 | gate_real=0.929 | gate_abs=0.071\n",
      "Epoch 052 | train_loss=0.0063 | val_loss=0.0026 | ema=0.0030 | lr=0.00005 | pred = 35.9080 | gate_real=0.929 | gate_abs=0.071\n",
      "Epoch 056 | train_loss=0.0088 | val_loss=0.0026 | ema=0.0029 | lr=0.00002 | pred = 35.9130 | gate_real=0.929 | gate_abs=0.071\n",
      "Epoch 060 | train_loss=0.0064 | val_loss=0.0029 | ema=0.0030 | lr=0.00001 | pred = 35.8888 | gate_real=0.929 | gate_abs=0.071\n",
      "Epoch 064 | train_loss=0.0098 | val_loss=0.0025 | ema=0.0027 | lr=0.00000 | pred = 35.9149 | gate_real=0.929 | gate_abs=0.071\n",
      "pred = 35.9149, 1\n",
      "[Fold 2] Target = 36.2994 | Predict = 35.9149\n",
      "MAE(Target~DAP) = 0.4133 | MAE(Target~Predict) = 0.3845\n",
      "\n",
      "\n",
      "Fold 3: train 930, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-11 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "val last row   | DAP=36.2994 | Target=35.9981 | Target_smooth=35.7569\n",
      "Epoch 001 | train_loss=0.0062 | val_loss=0.0012 | ema=0.0012 | lr=0.00007 | pred = 35.7309 | gate_real=0.930 | gate_abs=0.070\n",
      "Epoch 004 | train_loss=0.0050 | val_loss=0.0001 | ema=0.0010 | lr=0.00017 | pred = 35.9088 | gate_real=0.932 | gate_abs=0.068\n",
      "Epoch 008 | train_loss=0.0096 | val_loss=0.0007 | ema=0.0010 | lr=0.00030 | pred = 35.7934 | gate_real=0.934 | gate_abs=0.066\n",
      "Epoch 012 | train_loss=0.0097 | val_loss=0.0003 | ema=0.0014 | lr=0.00040 | pred = 36.1205 | gate_real=0.936 | gate_abs=0.064\n",
      "Epoch 016 | train_loss=0.0060 | val_loss=0.0002 | ema=0.0012 | lr=0.00039 | pred = 36.1066 | gate_real=0.935 | gate_abs=0.065\n",
      "Epoch 020 | train_loss=0.0077 | val_loss=0.0037 | ema=0.0022 | lr=0.00038 | pred = 35.5366 | gate_real=0.932 | gate_abs=0.068\n",
      "Epoch 024 | train_loss=0.0072 | val_loss=0.0020 | ema=0.0015 | lr=0.00035 | pred = 35.6607 | gate_real=0.936 | gate_abs=0.064\n",
      "Epoch 028 | train_loss=0.0078 | val_loss=0.0002 | ema=0.0013 | lr=0.00031 | pred = 35.8856 | gate_real=0.934 | gate_abs=0.066\n",
      "Epoch 032 | train_loss=0.0132 | val_loss=0.0070 | ema=0.0026 | lr=0.00027 | pred = 35.3585 | gate_real=0.931 | gate_abs=0.069\n",
      "Epoch 036 | train_loss=0.0083 | val_loss=0.0040 | ema=0.0025 | lr=0.00022 | pred = 35.5151 | gate_real=0.934 | gate_abs=0.066\n",
      "Epoch 040 | train_loss=0.0082 | val_loss=0.0015 | ema=0.0021 | lr=0.00018 | pred = 35.6990 | gate_real=0.938 | gate_abs=0.062\n",
      "Epoch 044 | train_loss=0.0071 | val_loss=0.0001 | ema=0.0011 | lr=0.00013 | pred = 35.9243 | gate_real=0.940 | gate_abs=0.060\n",
      "Epoch 048 | train_loss=0.0075 | val_loss=0.0002 | ema=0.0008 | lr=0.00009 | pred = 35.9023 | gate_real=0.940 | gate_abs=0.060\n",
      "Epoch 052 | train_loss=0.0091 | val_loss=0.0004 | ema=0.0007 | lr=0.00005 | pred = 35.8458 | gate_real=0.940 | gate_abs=0.060\n",
      "Epoch 056 | train_loss=0.0074 | val_loss=0.0007 | ema=0.0008 | lr=0.00002 | pred = 35.7905 | gate_real=0.939 | gate_abs=0.061\n",
      "Epoch 060 | train_loss=0.0055 | val_loss=0.0007 | ema=0.0006 | lr=0.00001 | pred = 35.7951 | gate_real=0.939 | gate_abs=0.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  20%|██        | 2/10 [24:55<1:39:30, 746.32s/it, last_mae_pred=0.1877, last_mae_dap=0.4133, avg_mae_pred=0.1805, avg_mae_dap=0.4133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.0070 | val_loss=0.0006 | ema=0.0006 | lr=0.00000 | pred = 35.8104 | gate_real=0.939 | gate_abs=0.061\n",
      "pred = 35.8104, 1\n",
      "[Fold 3] Target = 35.9981 | Predict = 35.8104\n",
      "MAE(Target~DAP) = 0.3014 | MAE(Target~Predict) = 0.1877\n",
      "\n",
      "[35.7915, 35.7905, 35.8275, 35.875, 35.8444, 35.7951, 35.7904, 35.8008, 35.8092, 35.8104]  | mean=35.8135 | std=0.0279\n",
      "\n",
      "Dates: 2025-11-10 00:00:00 → 2025-11-12 00:00:00 | interval_days: 2 | time_shift: 1\n",
      "num of splits = 3\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 836, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-02 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=40.2486 | Target=40.7472 | Target_smooth=39.9818\n",
      "val last row   | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.4826 | val_loss=0.3094 | ema=0.3094 | lr=0.00017 | pred = 33.8328 | gate_real=0.539 | gate_abs=0.461\n",
      "Epoch 004 | train_loss=0.0539 | val_loss=0.0326 | ema=0.1894 | lr=0.00042 | pred = 35.6546 | gate_real=0.870 | gate_abs=0.130\n",
      "Epoch 008 | train_loss=0.0292 | val_loss=0.0121 | ema=0.1049 | lr=0.00075 | pred = 35.6910 | gate_real=0.892 | gate_abs=0.108\n",
      "Epoch 012 | train_loss=0.0177 | val_loss=0.0131 | ema=0.0502 | lr=0.00100 | pred = 34.7383 | gate_real=0.919 | gate_abs=0.081\n",
      "Epoch 016 | train_loss=0.0146 | val_loss=0.0081 | ema=0.0278 | lr=0.00099 | pred = 35.8220 | gate_real=0.947 | gate_abs=0.053\n",
      "Epoch 020 | train_loss=0.0233 | val_loss=0.0077 | ema=0.0203 | lr=0.00094 | pred = 36.2663 | gate_real=0.956 | gate_abs=0.044\n",
      "Epoch 024 | train_loss=0.0070 | val_loss=0.0088 | ema=0.0133 | lr=0.00087 | pred = 35.2022 | gate_real=0.948 | gate_abs=0.052\n",
      "Epoch 028 | train_loss=0.0148 | val_loss=0.0134 | ema=0.0126 | lr=0.00078 | pred = 34.6612 | gate_real=0.956 | gate_abs=0.044\n",
      "Epoch 032 | train_loss=0.0112 | val_loss=0.0086 | ema=0.0104 | lr=0.00068 | pred = 35.1726 | gate_real=0.962 | gate_abs=0.038\n",
      "Epoch 036 | train_loss=0.0107 | val_loss=0.0086 | ema=0.0128 | lr=0.00056 | pred = 35.3403 | gate_real=0.965 | gate_abs=0.035\n",
      "Epoch 040 | train_loss=0.0083 | val_loss=0.0153 | ema=0.0119 | lr=0.00044 | pred = 35.1146 | gate_real=0.959 | gate_abs=0.041\n",
      "Epoch 044 | train_loss=0.0064 | val_loss=0.0081 | ema=0.0099 | lr=0.00032 | pred = 35.2556 | gate_real=0.961 | gate_abs=0.039\n",
      "Epoch 048 | train_loss=0.0068 | val_loss=0.0082 | ema=0.0086 | lr=0.00022 | pred = 35.1444 | gate_real=0.964 | gate_abs=0.036\n",
      "Epoch 052 | train_loss=0.0068 | val_loss=0.0093 | ema=0.0086 | lr=0.00013 | pred = 35.0995 | gate_real=0.965 | gate_abs=0.035\n",
      "Epoch 056 | train_loss=0.0068 | val_loss=0.0074 | ema=0.0078 | lr=0.00006 | pred = 35.2242 | gate_real=0.965 | gate_abs=0.035\n",
      "Epoch 060 | train_loss=0.0057 | val_loss=0.0072 | ema=0.0074 | lr=0.00001 | pred = 35.2275 | gate_real=0.966 | gate_abs=0.034\n",
      "Epoch 064 | train_loss=0.0079 | val_loss=0.0077 | ema=0.0076 | lr=0.00000 | pred = 35.2060 | gate_real=0.966 | gate_abs=0.034\n",
      "pred = 35.2060, 72\n",
      "[Fold 1] Target = 35.8861 | Predict = 35.2060\n",
      "MAE(Target~DAP) = 0.4384 | MAE(Target~Predict) = 0.5261\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-13 ... 2025-11-11\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "val last row   | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "Epoch 001 | train_loss=0.0073 | val_loss=0.0049 | ema=0.0049 | lr=0.00003 | pred = 35.7672 | gate_real=0.973 | gate_abs=0.027\n",
      "Epoch 004 | train_loss=0.0100 | val_loss=0.0055 | ema=0.0049 | lr=0.00008 | pred = 35.7319 | gate_real=0.974 | gate_abs=0.026\n",
      "Epoch 008 | train_loss=0.0076 | val_loss=0.0054 | ema=0.0049 | lr=0.00015 | pred = 35.7390 | gate_real=0.975 | gate_abs=0.025\n",
      "Epoch 012 | train_loss=0.0065 | val_loss=0.0020 | ema=0.0043 | lr=0.00020 | pred = 35.9589 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 016 | train_loss=0.0121 | val_loss=0.0007 | ema=0.0033 | lr=0.00020 | pred = 36.1011 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 020 | train_loss=0.0065 | val_loss=0.0023 | ema=0.0035 | lr=0.00019 | pred = 35.9349 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 024 | train_loss=0.0078 | val_loss=0.0043 | ema=0.0032 | lr=0.00017 | pred = 35.7993 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 028 | train_loss=0.0122 | val_loss=0.0026 | ema=0.0036 | lr=0.00016 | pred = 35.9104 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 032 | train_loss=0.0068 | val_loss=0.0009 | ema=0.0021 | lr=0.00014 | pred = 36.0762 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 036 | train_loss=0.0091 | val_loss=0.0018 | ema=0.0019 | lr=0.00011 | pred = 35.9741 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 040 | train_loss=0.0150 | val_loss=0.0036 | ema=0.0027 | lr=0.00009 | pred = 35.8426 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 044 | train_loss=0.0051 | val_loss=0.0015 | ema=0.0023 | lr=0.00006 | pred = 36.0038 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 048 | train_loss=0.0079 | val_loss=0.0020 | ema=0.0019 | lr=0.00004 | pred = 35.9613 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 052 | train_loss=0.0060 | val_loss=0.0010 | ema=0.0016 | lr=0.00003 | pred = 36.0627 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 056 | train_loss=0.0067 | val_loss=0.0013 | ema=0.0015 | lr=0.00001 | pred = 36.0272 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 060 | train_loss=0.0069 | val_loss=0.0014 | ema=0.0015 | lr=0.00000 | pred = 36.0102 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 064 | train_loss=0.0095 | val_loss=0.0012 | ema=0.0013 | lr=0.00000 | pred = 36.0355 | gate_real=0.978 | gate_abs=0.022\n",
      "pred = 36.0355, 1\n",
      "[Fold 2] Target = 36.2994 | Predict = 36.0355\n",
      "MAE(Target~DAP) = 0.4133 | MAE(Target~Predict) = 0.2639\n",
      "\n",
      "\n",
      "Fold 3: train 930, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-11 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "val last row   | DAP=36.2994 | Target=35.9981 | Target_smooth=35.7569\n",
      "Epoch 001 | train_loss=0.0063 | val_loss=0.0001 | ema=0.0001 | lr=0.00003 | pred = 35.9269 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 004 | train_loss=0.0059 | val_loss=0.0000 | ema=0.0001 | lr=0.00008 | pred = 36.0419 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 008 | train_loss=0.0125 | val_loss=0.0000 | ema=0.0001 | lr=0.00015 | pred = 35.9721 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 012 | train_loss=0.0064 | val_loss=0.0008 | ema=0.0005 | lr=0.00020 | pred = 36.2115 | gate_real=0.974 | gate_abs=0.026\n",
      "Epoch 016 | train_loss=0.0062 | val_loss=0.0006 | ema=0.0003 | lr=0.00020 | pred = 36.1837 | gate_real=0.973 | gate_abs=0.027\n",
      "Epoch 020 | train_loss=0.0072 | val_loss=0.0004 | ema=0.0007 | lr=0.00019 | pred = 35.8467 | gate_real=0.973 | gate_abs=0.027\n",
      "Epoch 024 | train_loss=0.0077 | val_loss=0.0002 | ema=0.0005 | lr=0.00017 | pred = 35.9014 | gate_real=0.973 | gate_abs=0.027\n",
      "Epoch 028 | train_loss=0.0073 | val_loss=0.0012 | ema=0.0008 | lr=0.00016 | pred = 36.2612 | gate_real=0.974 | gate_abs=0.026\n",
      "Epoch 032 | train_loss=0.0132 | val_loss=0.0010 | ema=0.0006 | lr=0.00014 | pred = 35.7612 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 036 | train_loss=0.0075 | val_loss=0.0011 | ema=0.0005 | lr=0.00011 | pred = 35.7504 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 040 | train_loss=0.0075 | val_loss=0.0000 | ema=0.0004 | lr=0.00009 | pred = 35.9919 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 044 | train_loss=0.0074 | val_loss=0.0004 | ema=0.0005 | lr=0.00006 | pred = 36.1486 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 048 | train_loss=0.0076 | val_loss=0.0006 | ema=0.0004 | lr=0.00004 | pred = 36.1815 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 052 | train_loss=0.0099 | val_loss=0.0003 | ema=0.0004 | lr=0.00003 | pred = 36.1225 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 056 | train_loss=0.0061 | val_loss=0.0001 | ema=0.0002 | lr=0.00001 | pred = 36.0768 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 060 | train_loss=0.0061 | val_loss=0.0001 | ema=0.0002 | lr=0.00000 | pred = 36.0619 | gate_real=0.979 | gate_abs=0.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  30%|███       | 3/10 [37:15<1:26:43, 743.43s/it, last_mae_pred=0.0727, last_mae_dap=0.4133, avg_mae_pred=0.1446, avg_mae_dap=0.4133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.0070 | val_loss=0.0001 | ema=0.0001 | lr=0.00000 | pred = 36.0708 | gate_real=0.979 | gate_abs=0.021\n",
      "pred = 36.0708, 1\n",
      "[Fold 3] Target = 35.9981 | Predict = 36.0708\n",
      "MAE(Target~DAP) = 0.3014 | MAE(Target~Predict) = 0.0727\n",
      "\n",
      "[36.127, 36.0768, 36.0901, 36.1342, 36.1073, 36.0619, 36.0575, 36.0608, 36.0692, 36.0708]  | mean=36.0856 | std=0.0281\n",
      "\n",
      "Dates: 2025-11-10 00:00:00 → 2025-11-12 00:00:00 | interval_days: 2 | time_shift: 1\n",
      "num of splits = 3\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 836, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-02 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=40.2486 | Target=40.7472 | Target_smooth=39.9818\n",
      "val last row   | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.4852 | val_loss=0.3019 | ema=0.3019 | lr=0.00015 | pred = 33.9331 | gate_real=0.545 | gate_abs=0.455\n",
      "Epoch 004 | train_loss=0.0583 | val_loss=0.0439 | ema=0.1904 | lr=0.00038 | pred = 34.2001 | gate_real=0.859 | gate_abs=0.141\n",
      "Epoch 008 | train_loss=0.0271 | val_loss=0.0150 | ema=0.0968 | lr=0.00068 | pred = 34.7126 | gate_real=0.913 | gate_abs=0.087\n",
      "Epoch 012 | train_loss=0.0231 | val_loss=0.0095 | ema=0.0481 | lr=0.00090 | pred = 34.8843 | gate_real=0.942 | gate_abs=0.058\n",
      "Epoch 016 | train_loss=0.0124 | val_loss=0.0128 | ema=0.0261 | lr=0.00089 | pred = 34.6437 | gate_real=0.955 | gate_abs=0.045\n",
      "Epoch 020 | train_loss=0.0216 | val_loss=0.0077 | ema=0.0153 | lr=0.00085 | pred = 34.8272 | gate_real=0.961 | gate_abs=0.039\n",
      "Epoch 024 | train_loss=0.0072 | val_loss=0.0161 | ema=0.0137 | lr=0.00079 | pred = 34.6794 | gate_real=0.964 | gate_abs=0.036\n",
      "Epoch 028 | train_loss=0.0170 | val_loss=0.0090 | ema=0.0114 | lr=0.00071 | pred = 34.8894 | gate_real=0.966 | gate_abs=0.034\n",
      "Epoch 032 | train_loss=0.0101 | val_loss=0.0065 | ema=0.0089 | lr=0.00061 | pred = 35.4556 | gate_real=0.968 | gate_abs=0.032\n",
      "Epoch 036 | train_loss=0.0086 | val_loss=0.0067 | ema=0.0097 | lr=0.00050 | pred = 35.6410 | gate_real=0.970 | gate_abs=0.030\n",
      "Epoch 040 | train_loss=0.0083 | val_loss=0.0166 | ema=0.0106 | lr=0.00040 | pred = 35.0458 | gate_real=0.968 | gate_abs=0.032\n",
      "Epoch 044 | train_loss=0.0073 | val_loss=0.0094 | ema=0.0098 | lr=0.00029 | pred = 35.2230 | gate_real=0.966 | gate_abs=0.034\n",
      "Epoch 048 | train_loss=0.0074 | val_loss=0.0090 | ema=0.0089 | lr=0.00019 | pred = 35.2280 | gate_real=0.969 | gate_abs=0.031\n",
      "Epoch 052 | train_loss=0.0061 | val_loss=0.0094 | ema=0.0089 | lr=0.00011 | pred = 35.2046 | gate_real=0.969 | gate_abs=0.031\n",
      "Epoch 056 | train_loss=0.0073 | val_loss=0.0084 | ema=0.0084 | lr=0.00005 | pred = 35.3096 | gate_real=0.970 | gate_abs=0.030\n",
      "Epoch 060 | train_loss=0.0053 | val_loss=0.0079 | ema=0.0082 | lr=0.00001 | pred = 35.3045 | gate_real=0.971 | gate_abs=0.029\n",
      "Epoch 064 | train_loss=0.0076 | val_loss=0.0083 | ema=0.0082 | lr=0.00000 | pred = 35.2881 | gate_real=0.971 | gate_abs=0.029\n",
      "pred = 35.2881, 72\n",
      "[Fold 1] Target = 35.8861 | Predict = 35.2881\n",
      "MAE(Target~DAP) = 0.4384 | MAE(Target~Predict) = 0.5174\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-13 ... 2025-11-11\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "val last row   | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "Epoch 001 | train_loss=0.0071 | val_loss=0.0032 | ema=0.0032 | lr=0.00003 | pred = 35.8654 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 004 | train_loss=0.0098 | val_loss=0.0050 | ema=0.0034 | lr=0.00008 | pred = 35.7616 | gate_real=0.976 | gate_abs=0.024\n",
      "Epoch 008 | train_loss=0.0083 | val_loss=0.0033 | ema=0.0030 | lr=0.00014 | pred = 35.8633 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 012 | train_loss=0.0070 | val_loss=0.0008 | ema=0.0025 | lr=0.00018 | pred = 36.0859 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 016 | train_loss=0.0129 | val_loss=0.0000 | ema=0.0019 | lr=0.00018 | pred = 36.2872 | gate_real=0.980 | gate_abs=0.020\n",
      "Epoch 020 | train_loss=0.0063 | val_loss=0.0003 | ema=0.0022 | lr=0.00017 | pred = 36.1603 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 024 | train_loss=0.0070 | val_loss=0.0015 | ema=0.0017 | lr=0.00016 | pred = 36.0075 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 028 | train_loss=0.0119 | val_loss=0.0015 | ema=0.0022 | lr=0.00014 | pred = 36.0066 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 032 | train_loss=0.0078 | val_loss=0.0000 | ema=0.0009 | lr=0.00012 | pred = 36.2514 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 036 | train_loss=0.0086 | val_loss=0.0004 | ema=0.0007 | lr=0.00010 | pred = 36.1399 | gate_real=0.982 | gate_abs=0.018\n",
      "Epoch 040 | train_loss=0.0139 | val_loss=0.0008 | ema=0.0009 | lr=0.00008 | pred = 36.0839 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 044 | train_loss=0.0047 | val_loss=0.0002 | ema=0.0006 | lr=0.00006 | pred = 36.1920 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 048 | train_loss=0.0090 | val_loss=0.0007 | ema=0.0005 | lr=0.00004 | pred = 36.0966 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 052 | train_loss=0.0056 | val_loss=0.0000 | ema=0.0003 | lr=0.00002 | pred = 36.2605 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 056 | train_loss=0.0063 | val_loss=0.0001 | ema=0.0002 | lr=0.00001 | pred = 36.2217 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 060 | train_loss=0.0065 | val_loss=0.0002 | ema=0.0002 | lr=0.00000 | pred = 36.1942 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 064 | train_loss=0.0085 | val_loss=0.0001 | ema=0.0001 | lr=0.00000 | pred = 36.2314 | gate_real=0.981 | gate_abs=0.019\n",
      "pred = 36.2314, 1\n",
      "[Fold 2] Target = 36.2994 | Predict = 36.2314\n",
      "MAE(Target~DAP) = 0.4133 | MAE(Target~Predict) = 0.0681\n",
      "\n",
      "\n",
      "Fold 3: train 930, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-11 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=35.8861 | Target=36.2994 | Target_smooth=35.6363\n",
      "val last row   | DAP=36.2994 | Target=35.9981 | Target_smooth=35.7569\n",
      "Epoch 001 | train_loss=0.0058 | val_loss=0.0001 | ema=0.0001 | lr=0.00003 | pred = 36.0789 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 004 | train_loss=0.0057 | val_loss=0.0000 | ema=0.0002 | lr=0.00008 | pred = 36.0379 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 008 | train_loss=0.0116 | val_loss=0.0005 | ema=0.0002 | lr=0.00014 | pred = 36.1693 | gate_real=0.980 | gate_abs=0.020\n",
      "Epoch 012 | train_loss=0.0066 | val_loss=0.0024 | ema=0.0007 | lr=0.00018 | pred = 36.3720 | gate_real=0.977 | gate_abs=0.023\n",
      "Epoch 016 | train_loss=0.0076 | val_loss=0.0015 | ema=0.0007 | lr=0.00018 | pred = 36.2919 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 020 | train_loss=0.0077 | val_loss=0.0014 | ema=0.0012 | lr=0.00017 | pred = 35.7080 | gate_real=0.978 | gate_abs=0.022\n",
      "Epoch 024 | train_loss=0.0071 | val_loss=0.0001 | ema=0.0006 | lr=0.00016 | pred = 36.0728 | gate_real=0.979 | gate_abs=0.021\n",
      "Epoch 028 | train_loss=0.0069 | val_loss=0.0038 | ema=0.0013 | lr=0.00014 | pred = 36.4686 | gate_real=0.980 | gate_abs=0.020\n",
      "Epoch 032 | train_loss=0.0144 | val_loss=0.0001 | ema=0.0010 | lr=0.00012 | pred = 35.9441 | gate_real=0.981 | gate_abs=0.019\n",
      "Epoch 036 | train_loss=0.0076 | val_loss=0.0002 | ema=0.0005 | lr=0.00010 | pred = 35.9029 | gate_real=0.982 | gate_abs=0.018\n",
      "Epoch 040 | train_loss=0.0068 | val_loss=0.0001 | ema=0.0004 | lr=0.00008 | pred = 36.0893 | gate_real=0.982 | gate_abs=0.018\n",
      "Epoch 044 | train_loss=0.0072 | val_loss=0.0005 | ema=0.0006 | lr=0.00006 | pred = 36.1625 | gate_real=0.982 | gate_abs=0.018\n",
      "Epoch 048 | train_loss=0.0074 | val_loss=0.0013 | ema=0.0008 | lr=0.00004 | pred = 36.2698 | gate_real=0.982 | gate_abs=0.018\n",
      "Epoch 052 | train_loss=0.0099 | val_loss=0.0005 | ema=0.0007 | lr=0.00002 | pred = 36.1650 | gate_real=0.982 | gate_abs=0.018\n",
      "Epoch 056 | train_loss=0.0056 | val_loss=0.0005 | ema=0.0006 | lr=0.00001 | pred = 36.1639 | gate_real=0.982 | gate_abs=0.018\n",
      "Epoch 060 | train_loss=0.0061 | val_loss=0.0005 | ema=0.0006 | lr=0.00000 | pred = 36.1743 | gate_real=0.982 | gate_abs=0.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  40%|████      | 4/10 [49:32<1:14:03, 740.59s/it, last_mae_pred=0.1737, last_mae_dap=0.4133, avg_mae_pred=0.1519, avg_mae_dap=0.4133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.0071 | val_loss=0.0005 | ema=0.0005 | lr=0.00000 | pred = 36.1718 | gate_real=0.982 | gate_abs=0.018\n",
      "pred = 36.1718, 1\n",
      "[Fold 3] Target = 35.9981 | Predict = 36.1718\n",
      "MAE(Target~DAP) = 0.3014 | MAE(Target~Predict) = 0.1737\n",
      "\n",
      "[36.2007, 36.1639, 36.1603, 36.2112, 36.2062, 36.1743, 36.1606, 36.1602, 36.1696, 36.1718]  | mean=36.1779 | std=0.0202\n",
      "\n",
      "Dates: 2025-11-10 00:00:00 → 2025-11-12 00:00:00 | interval_days: 2 | time_shift: 1\n",
      "num of splits = 3\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 836, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-02 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "mu_DAP=33.1319\n",
      "train last row | DAP=40.2486 | Target=40.7472 | Target_smooth=39.9818\n",
      "val last row   | DAP=35.7539 | Target=35.8861 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.4877 | val_loss=0.3007 | ema=0.3007 | lr=0.00013 | pred = 33.9573 | gate_real=0.548 | gate_abs=0.452\n",
      "Epoch 004 | train_loss=0.0833 | val_loss=0.0645 | ema=0.2011 | lr=0.00033 | pred = 33.3821 | gate_real=0.829 | gate_abs=0.171\n",
      "Epoch 008 | train_loss=0.0597 | val_loss=0.0301 | ema=0.1056 | lr=0.00060 | pred = 34.1182 | gate_real=0.886 | gate_abs=0.114\n",
      "Epoch 012 | train_loss=0.0197 | val_loss=0.0250 | ema=0.0537 | lr=0.00080 | pred = 34.7570 | gate_real=0.900 | gate_abs=0.100\n",
      "Epoch 016 | train_loss=0.0084 | val_loss=0.0084 | ema=0.0286 | lr=0.00079 | pred = 35.1890 | gate_real=0.944 | gate_abs=0.056\n",
      "Epoch 020 | train_loss=0.0219 | val_loss=0.0129 | ema=0.0194 | lr=0.00075 | pred = 34.3782 | gate_real=0.953 | gate_abs=0.047\n",
      "Epoch 024 | train_loss=0.0084 | val_loss=0.0055 | ema=0.0129 | lr=0.00070 | pred = 35.6380 | gate_real=0.951 | gate_abs=0.049\n",
      "Epoch 028 | train_loss=0.0157 | val_loss=0.0112 | ema=0.0120 | lr=0.00063 | pred = 34.8053 | gate_real=0.963 | gate_abs=0.037\n",
      "Epoch 032 | train_loss=0.0112 | val_loss=0.0074 | ema=0.0092 | lr=0.00054 | pred = 35.5796 | gate_real=0.961 | gate_abs=0.039\n",
      "Epoch 036 | train_loss=0.0098 | val_loss=0.0065 | ema=0.0088 | lr=0.00045 | pred = 35.6890 | gate_real=0.963 | gate_abs=0.037\n",
      "Epoch 040 | train_loss=0.0082 | val_loss=0.0139 | ema=0.0098 | lr=0.00035 | pred = 35.1856 | gate_real=0.961 | gate_abs=0.039\n",
      "Epoch 044 | train_loss=0.0073 | val_loss=0.0088 | ema=0.0087 | lr=0.00026 | pred = 35.3368 | gate_real=0.965 | gate_abs=0.035\n",
      "Epoch 048 | train_loss=0.0072 | val_loss=0.0071 | ema=0.0080 | lr=0.00017 | pred = 35.3993 | gate_real=0.966 | gate_abs=0.034\n",
      "Epoch 052 | train_loss=0.0073 | val_loss=0.0100 | ema=0.0088 | lr=0.00010 | pred = 35.2409 | gate_real=0.967 | gate_abs=0.033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  40%|████      | 4/10 [52:47<1:19:11, 791.93s/it, last_mae_pred=0.1737, last_mae_dap=0.4133, avg_mae_pred=0.1519, avg_mae_dap=0.4133]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m df_temp \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     21\u001b[0m config[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m start_lr\n\u001b[0;32m---> 23\u001b[0m results \u001b[39m=\u001b[39m train_tscv_model_8(config, df_temp, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[39m# DAP_ = round(float(df['DayAvgPrice'].iloc[start_index + 1]), 4)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# mu_DAP = round(float(results[\"mu\"]['DayAvgPrice']), 4)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# sigma_DAP = round(float(results[\"sigma\"]['DayAvgPrice']), 4)       \u001b[39;00m\n\u001b[1;32m     28\u001b[0m DAP_ \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(\u001b[39mfloat\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mc_month_real\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[start_index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 391\u001b[0m, in \u001b[0;36mtrain_tscv_model_8\u001b[0;34m(config, df, verbose)\u001b[0m\n\u001b[1;32m    385\u001b[0m x_complex \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcomplex(\n\u001b[1;32m    386\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39mcomplex_time_real\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m    387\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39mcomplex_time_imag\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    388\u001b[0m )\n\u001b[1;32m    389\u001b[0m y \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 391\u001b[0m h_real \u001b[39m=\u001b[39m real_net(x_real)\n\u001b[1;32m    392\u001b[0m h_r, h_i \u001b[39m=\u001b[39m complex_net(x_complex)\n\u001b[1;32m    393\u001b[0m out \u001b[39m=\u001b[39m fusion_net(h_real, h_r, h_i)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/Complex_T_Project/Complex_T/src/model_real_2.py:154\u001b[0m, in \u001b[0;36mRealCNNLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m x_last_raw \u001b[39m=\u001b[39m x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]           \u001b[39m# [B, F]\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m# --- CNN: [B, T, F] -> [B, H, T] ---\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m x_cnn \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()  \u001b[39m# [B, F, T]\u001b[39;00m\n\u001b[1;32m    155\u001b[0m x_cnn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_in(x_cnn)\n\u001b[1;32m    156\u001b[0m x_cnn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgn_in(x_cnn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lr_list predict: START! \n",
    "\n",
    "start_preds_date = pd.to_datetime(config['data']['start_preds_date'], errors='coerce')\n",
    "stop_preds_date = pd.to_datetime(config['data']['stop_preds_date'], errors='coerce')\n",
    "interval_days = int((stop_preds_date - start_preds_date).days)\n",
    "start_index = df.index[df['date']==config['data']['start_preds_date']].tolist()[0]\n",
    "result_list = []\n",
    "\n",
    "cum_mae_pred = 0.0\n",
    "cum_mae_dap  = 0.0\n",
    "steps = 0\n",
    "\n",
    "start_lr_list = [0.003, 0.002, 0.001, 0.0009, 0.0008, 0.0007, 0.0006, 0.0005, 0.0004, 0.0003]\n",
    "results_acc = pd.read_csv('../data_archiv/DTG/res/DTG_res_fresh.csv')\n",
    "total_iters = len(start_lr_list)\n",
    "\n",
    "with tqdm(total=total_iters, desc=\"Rolling forecast\") as pbar:\n",
    "    for start_lr in start_lr_list:\n",
    "        df_temp = df.copy()\n",
    "\n",
    "        config['train']['lr'] = start_lr\n",
    "\n",
    "        results = train_tscv_model_8(config, df_temp, verbose=True)\n",
    "\n",
    "        # DAP_ = round(float(df['DayAvgPrice'].iloc[start_index + 1]), 4)\n",
    "        # mu_DAP = round(float(results[\"mu\"]['DayAvgPrice']), 4)\n",
    "        # sigma_DAP = round(float(results[\"sigma\"]['DayAvgPrice']), 4)       \n",
    "        DAP_ = round(float(df['c_month_real'].iloc[start_index + 1]), 4)\n",
    "        mu_DAP = round(float(results[\"mu\"]['c_month_real']), 4)\n",
    "        sigma_DAP = round(float(results[\"sigma\"]['c_month_real']), 4)\n",
    "        # DAP_ = DAP * sigma_DAP + mu_DAP\n",
    "\n",
    "        Target_ = round(float(df_temp['Target'].iloc[start_index + 1]), 4)\n",
    "        Pred = round(float(results[\"pred_series_per_fold\"][0]), 4)\n",
    "        preds_last_10_mean = round(pd.Series(results[\"preds_last_10\"]).mean(), 4)\n",
    "        preds_last_10_std = round(pd.Series(results[\"preds_last_10\"]).std(), 4)\n",
    "        print(results[\"preds_last_10\"], f\" | mean={preds_last_10_mean:.4f} | std={preds_last_10_std:.4f}\\n\")\n",
    "\n",
    "        mae_target_pred = results[\"fold_metrics\"][-1][\"mae_target_pred\"]\n",
    "        mae_target_dap = abs(DAP_ - Target_)\n",
    "        mae_target_pred_avg = abs(preds_last_10_mean - Target_)\n",
    "        # mae_target_dap = results[\"fold_metrics\"][-1][\"mae_target_dap\"]\n",
    "\n",
    "        result_list.append({\n",
    "            'date': config['data']['stop_preds_date'],\n",
    "            'lr': config['train']['lr'],\n",
    "            'last_DAP': DAP_,\n",
    "            'mu_DAP': mu_DAP,\n",
    "            'Target': Target_,\n",
    "            'Predict': Pred,\n",
    "            'Pred_last_10_ep_mean': preds_last_10_mean,\n",
    "            'Pred_last_10_ep_std': preds_last_10_std,\n",
    "            'mae_target_pred': round(mae_target_pred, 4),\n",
    "            'mae_target_pred_avg': round(mae_target_pred_avg, 4),\n",
    "            'mae_target_dap': round(mae_target_dap, 4)\n",
    "        })\n",
    "\n",
    "        # обновляем кумулятивные средние\n",
    "        steps += 1\n",
    "        cum_mae_pred = ((cum_mae_pred * (steps - 1)) + mae_target_pred) / steps\n",
    "        cum_mae_dap  = ((cum_mae_dap  * (steps - 1)) + mae_target_dap)  / steps\n",
    "\n",
    "        # обновляем прогресс-бар: последние и средние MAE\n",
    "        pbar.set_postfix({\n",
    "            \"last_mae_pred\": f\"{mae_target_pred:.4f}\",\n",
    "            \"last_mae_dap\":  f\"{mae_target_dap:.4f}\",\n",
    "            \"avg_mae_pred\":  f\"{cum_mae_pred:.4f}\",\n",
    "            \"avg_mae_dap\":   f\"{cum_mae_dap:.4f}\"\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "# собираем итоговый датафрейм один раз\n",
    "result_tab = pd.DataFrame(result_list)\n",
    "make_results_acc(results_acc, result_tab, config)\n",
    "results_acc.to_csv('../data_archiv/DTG/res/DTG_res_fresh.csv', index=False)\n",
    "display(results_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lr</th>\n",
       "      <th>last_DAP</th>\n",
       "      <th>mu_DAP</th>\n",
       "      <th>Target</th>\n",
       "      <th>Predict</th>\n",
       "      <th>Pred_last_10_ep_mean</th>\n",
       "      <th>Pred_last_10_ep_std</th>\n",
       "      <th>mae_target_pred</th>\n",
       "      <th>mae_target_pred_avg</th>\n",
       "      <th>mae_target_dap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.1423</td>\n",
       "      <td>35.1600</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>35.1423</td>\n",
       "      <td>35.1600</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.2339</td>\n",
       "      <td>35.2510</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>35.2339</td>\n",
       "      <td>35.2510</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.2597</td>\n",
       "      <td>35.2743</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>35.2597</td>\n",
       "      <td>35.2743</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.2504</td>\n",
       "      <td>35.2611</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>35.2504</td>\n",
       "      <td>35.2611</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.1115</td>\n",
       "      <td>35.1194</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>35.1115</td>\n",
       "      <td>35.1194</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.3307</td>\n",
       "      <td>35.3374</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>35.3307</td>\n",
       "      <td>35.3374</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.2445</td>\n",
       "      <td>35.2510</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>35.2445</td>\n",
       "      <td>35.2510</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.3002</td>\n",
       "      <td>35.3081</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>35.3002</td>\n",
       "      <td>35.3081</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.2785</td>\n",
       "      <td>35.2864</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>35.2785</td>\n",
       "      <td>35.2864</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>35.2153</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.2957</td>\n",
       "      <td>35.2967</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>35.2957</td>\n",
       "      <td>35.2967</td>\n",
       "      <td>35.2153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      lr  last_DAP   mu_DAP  Target  Predict  \\\n",
       "0  2025-11-14  0.0030   35.2153  33.1495     0.0  35.1423   \n",
       "1  2025-11-14  0.0020   35.2153  33.1495     0.0  35.2339   \n",
       "2  2025-11-14  0.0010   35.2153  33.1495     0.0  35.2597   \n",
       "3  2025-11-14  0.0009   35.2153  33.1495     0.0  35.2504   \n",
       "4  2025-11-14  0.0008   35.2153  33.1495     0.0  35.1115   \n",
       "5  2025-11-14  0.0007   35.2153  33.1495     0.0  35.3307   \n",
       "6  2025-11-14  0.0006   35.2153  33.1495     0.0  35.2445   \n",
       "7  2025-11-14  0.0005   35.2153  33.1495     0.0  35.3002   \n",
       "8  2025-11-14  0.0004   35.2153  33.1495     0.0  35.2785   \n",
       "9  2025-11-14  0.0003   35.2153  33.1495     0.0  35.2957   \n",
       "\n",
       "   Pred_last_10_ep_mean  Pred_last_10_ep_std  mae_target_pred  \\\n",
       "0               35.1600               0.0296          35.1423   \n",
       "1               35.2510               0.0296          35.2339   \n",
       "2               35.2743               0.0241          35.2597   \n",
       "3               35.2611               0.0268          35.2504   \n",
       "4               35.1194               0.0265          35.1115   \n",
       "5               35.3374               0.0287          35.3307   \n",
       "6               35.2510               0.0297          35.2445   \n",
       "7               35.3081               0.0239          35.3002   \n",
       "8               35.2864               0.0196          35.2785   \n",
       "9               35.2967               0.0216          35.2957   \n",
       "\n",
       "   mae_target_pred_avg  mae_target_dap  \n",
       "0              35.1600         35.2153  \n",
       "1              35.2510         35.2153  \n",
       "2              35.2743         35.2153  \n",
       "3              35.2611         35.2153  \n",
       "4              35.1194         35.2153  \n",
       "5              35.3374         35.2153  \n",
       "6              35.2510         35.2153  \n",
       "7              35.3081         35.2153  \n",
       "8              35.2864         35.2153  \n",
       "9              35.2967         35.2153  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.2545 0.0668 0.026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35.261075"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(round(result_tab['Pred_last_10_ep_mean'].mean(), 4), round(result_tab['Pred_last_10_ep_mean'].std(), 4), \\\n",
    "      round(result_tab['Pred_last_10_ep_std'].mean(), 4))\n",
    "my_list = result_tab['Pred_last_10_ep_mean'].tolist()\n",
    "my_list.remove(max(my_list))\n",
    "my_list.remove(min(my_list))\n",
    "sum(my_list) / len(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time_shift</th>\n",
       "      <th>window_size</th>\n",
       "      <th>lr_lambda</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dropout</th>\n",
       "      <th>n_res_blocks</th>\n",
       "      <th>PL10_mean_mean</th>\n",
       "      <th>PL10_mean_std</th>\n",
       "      <th>PL10_std_mean</th>\n",
       "      <th>PL10_std_std</th>\n",
       "      <th>0.0030_mean</th>\n",
       "      <th>0.0020_mean</th>\n",
       "      <th>0.0010_mean</th>\n",
       "      <th>0.0009_mean</th>\n",
       "      <th>0.0008_mean</th>\n",
       "      <th>0.0007_mean</th>\n",
       "      <th>0.0006_mean</th>\n",
       "      <th>0.0005_mean</th>\n",
       "      <th>0.0004_mean</th>\n",
       "      <th>0.0003_mean</th>\n",
       "      <th>0.0030_std</th>\n",
       "      <th>0.0020_std</th>\n",
       "      <th>0.0010_std</th>\n",
       "      <th>0.0009_std</th>\n",
       "      <th>0.0008_std</th>\n",
       "      <th>0.0007_std</th>\n",
       "      <th>0.0006_std</th>\n",
       "      <th>0.0005_std</th>\n",
       "      <th>0.0004_std</th>\n",
       "      <th>0.0003_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>44</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.04296</td>\n",
       "      <td>0.296592</td>\n",
       "      <td>0.03677</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>33.8967</td>\n",
       "      <td>33.7666</td>\n",
       "      <td>34.1048</td>\n",
       "      <td>34.4722</td>\n",
       "      <td>34.3281</td>\n",
       "      <td>34.1199</td>\n",
       "      <td>34.1343</td>\n",
       "      <td>33.9614</td>\n",
       "      <td>34.2144</td>\n",
       "      <td>33.4312</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>52</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.08798</td>\n",
       "      <td>0.101572</td>\n",
       "      <td>0.02693</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>33.9794</td>\n",
       "      <td>34.2996</td>\n",
       "      <td>34.1232</td>\n",
       "      <td>34.1260</td>\n",
       "      <td>34.0834</td>\n",
       "      <td>34.0315</td>\n",
       "      <td>34.0361</td>\n",
       "      <td>34.1098</td>\n",
       "      <td>34.1540</td>\n",
       "      <td>33.9368</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.52822</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>34.8397</td>\n",
       "      <td>35.0390</td>\n",
       "      <td>35.1257</td>\n",
       "      <td>34.1310</td>\n",
       "      <td>34.6296</td>\n",
       "      <td>34.7337</td>\n",
       "      <td>34.0210</td>\n",
       "      <td>34.6563</td>\n",
       "      <td>34.0728</td>\n",
       "      <td>34.0334</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>0.0664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>33.41389</td>\n",
       "      <td>0.592093</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.085554</td>\n",
       "      <td>33.9495</td>\n",
       "      <td>34.1444</td>\n",
       "      <td>33.5586</td>\n",
       "      <td>33.4852</td>\n",
       "      <td>32.9203</td>\n",
       "      <td>34.1612</td>\n",
       "      <td>33.1922</td>\n",
       "      <td>33.3552</td>\n",
       "      <td>33.1304</td>\n",
       "      <td>32.2419</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>34.01628</td>\n",
       "      <td>0.334351</td>\n",
       "      <td>0.07227</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>34.3612</td>\n",
       "      <td>34.4437</td>\n",
       "      <td>34.0495</td>\n",
       "      <td>33.9382</td>\n",
       "      <td>33.3032</td>\n",
       "      <td>34.1230</td>\n",
       "      <td>33.9392</td>\n",
       "      <td>33.9461</td>\n",
       "      <td>33.7411</td>\n",
       "      <td>34.3176</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.09213</td>\n",
       "      <td>0.271552</td>\n",
       "      <td>0.04879</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>34.5063</td>\n",
       "      <td>34.4195</td>\n",
       "      <td>34.0148</td>\n",
       "      <td>33.8809</td>\n",
       "      <td>33.9621</td>\n",
       "      <td>33.8735</td>\n",
       "      <td>33.8598</td>\n",
       "      <td>33.7669</td>\n",
       "      <td>34.3787</td>\n",
       "      <td>34.2588</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.03229</td>\n",
       "      <td>0.147213</td>\n",
       "      <td>0.04455</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>33.8739</td>\n",
       "      <td>33.8561</td>\n",
       "      <td>34.0091</td>\n",
       "      <td>34.1713</td>\n",
       "      <td>34.1532</td>\n",
       "      <td>33.8691</td>\n",
       "      <td>34.1332</td>\n",
       "      <td>33.9426</td>\n",
       "      <td>34.0381</td>\n",
       "      <td>34.2763</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.20160</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.8877</td>\n",
       "      <td>33.8170</td>\n",
       "      <td>34.3975</td>\n",
       "      <td>34.1070</td>\n",
       "      <td>34.3238</td>\n",
       "      <td>34.2922</td>\n",
       "      <td>34.3360</td>\n",
       "      <td>34.2902</td>\n",
       "      <td>34.3743</td>\n",
       "      <td>34.1906</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>33.77220</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.09480</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>33.7382</td>\n",
       "      <td>33.8192</td>\n",
       "      <td>33.5882</td>\n",
       "      <td>33.7218</td>\n",
       "      <td>33.4791</td>\n",
       "      <td>33.9450</td>\n",
       "      <td>33.9421</td>\n",
       "      <td>33.8468</td>\n",
       "      <td>33.8459</td>\n",
       "      <td>33.7960</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>60</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.16810</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.05370</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.9715</td>\n",
       "      <td>34.0325</td>\n",
       "      <td>34.5261</td>\n",
       "      <td>34.3204</td>\n",
       "      <td>34.4775</td>\n",
       "      <td>34.1714</td>\n",
       "      <td>34.3446</td>\n",
       "      <td>33.7550</td>\n",
       "      <td>33.9080</td>\n",
       "      <td>34.1736</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.02170</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.02940</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>33.8616</td>\n",
       "      <td>33.6905</td>\n",
       "      <td>34.1431</td>\n",
       "      <td>34.0800</td>\n",
       "      <td>34.0269</td>\n",
       "      <td>34.1495</td>\n",
       "      <td>34.0671</td>\n",
       "      <td>34.1982</td>\n",
       "      <td>34.0890</td>\n",
       "      <td>33.9112</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>33.95390</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>33.6247</td>\n",
       "      <td>34.0442</td>\n",
       "      <td>34.1751</td>\n",
       "      <td>34.0652</td>\n",
       "      <td>33.6538</td>\n",
       "      <td>34.0870</td>\n",
       "      <td>33.9074</td>\n",
       "      <td>34.0052</td>\n",
       "      <td>33.8273</td>\n",
       "      <td>34.1494</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.40610</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.06490</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>34.8906</td>\n",
       "      <td>35.8110</td>\n",
       "      <td>34.8277</td>\n",
       "      <td>35.1031</td>\n",
       "      <td>35.3820</td>\n",
       "      <td>35.3879</td>\n",
       "      <td>35.0688</td>\n",
       "      <td>35.8009</td>\n",
       "      <td>35.7868</td>\n",
       "      <td>36.0026</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.0748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.42660</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.06220</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>36.2859</td>\n",
       "      <td>35.4809</td>\n",
       "      <td>34.7119</td>\n",
       "      <td>35.2623</td>\n",
       "      <td>35.0421</td>\n",
       "      <td>35.1546</td>\n",
       "      <td>35.3421</td>\n",
       "      <td>34.9806</td>\n",
       "      <td>36.9482</td>\n",
       "      <td>35.0570</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.42250</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.02160</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>35.6900</td>\n",
       "      <td>35.4300</td>\n",
       "      <td>35.7616</td>\n",
       "      <td>35.6318</td>\n",
       "      <td>35.3681</td>\n",
       "      <td>35.4666</td>\n",
       "      <td>35.5247</td>\n",
       "      <td>35.2215</td>\n",
       "      <td>35.0339</td>\n",
       "      <td>35.0971</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.00700</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.01540</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>35.1352</td>\n",
       "      <td>35.0797</td>\n",
       "      <td>34.9388</td>\n",
       "      <td>35.0725</td>\n",
       "      <td>35.2059</td>\n",
       "      <td>34.7787</td>\n",
       "      <td>34.8483</td>\n",
       "      <td>35.0720</td>\n",
       "      <td>34.9944</td>\n",
       "      <td>34.9447</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.74070</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.02400</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>35.7633</td>\n",
       "      <td>35.4769</td>\n",
       "      <td>35.6850</td>\n",
       "      <td>35.3690</td>\n",
       "      <td>35.8015</td>\n",
       "      <td>36.3752</td>\n",
       "      <td>35.0792</td>\n",
       "      <td>36.0552</td>\n",
       "      <td>35.7728</td>\n",
       "      <td>36.0289</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.10310</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>35.9709</td>\n",
       "      <td>36.0299</td>\n",
       "      <td>35.8589</td>\n",
       "      <td>36.1879</td>\n",
       "      <td>35.8114</td>\n",
       "      <td>36.1512</td>\n",
       "      <td>36.2648</td>\n",
       "      <td>36.2854</td>\n",
       "      <td>36.2138</td>\n",
       "      <td>36.2563</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.94890</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.02310</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>35.8952</td>\n",
       "      <td>36.0750</td>\n",
       "      <td>35.8183</td>\n",
       "      <td>36.1429</td>\n",
       "      <td>36.3012</td>\n",
       "      <td>36.1458</td>\n",
       "      <td>36.0727</td>\n",
       "      <td>35.5236</td>\n",
       "      <td>36.0581</td>\n",
       "      <td>35.4566</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.85040</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.03040</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>35.9462</td>\n",
       "      <td>36.2186</td>\n",
       "      <td>36.3577</td>\n",
       "      <td>36.4732</td>\n",
       "      <td>35.6464</td>\n",
       "      <td>35.5069</td>\n",
       "      <td>35.6691</td>\n",
       "      <td>35.3255</td>\n",
       "      <td>34.9703</td>\n",
       "      <td>36.3903</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.06580</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>37.4658</td>\n",
       "      <td>35.7302</td>\n",
       "      <td>35.6559</td>\n",
       "      <td>35.9850</td>\n",
       "      <td>36.3132</td>\n",
       "      <td>35.6608</td>\n",
       "      <td>36.2174</td>\n",
       "      <td>36.0467</td>\n",
       "      <td>36.2851</td>\n",
       "      <td>35.2974</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.1231</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.1557</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.83560</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>36.5582</td>\n",
       "      <td>36.5673</td>\n",
       "      <td>36.6765</td>\n",
       "      <td>36.7727</td>\n",
       "      <td>36.6858</td>\n",
       "      <td>36.9320</td>\n",
       "      <td>36.8448</td>\n",
       "      <td>37.0639</td>\n",
       "      <td>37.2044</td>\n",
       "      <td>37.0501</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.50060</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.01570</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>36.5280</td>\n",
       "      <td>36.2985</td>\n",
       "      <td>36.4604</td>\n",
       "      <td>36.2603</td>\n",
       "      <td>36.5788</td>\n",
       "      <td>36.4379</td>\n",
       "      <td>36.7048</td>\n",
       "      <td>36.6045</td>\n",
       "      <td>36.2259</td>\n",
       "      <td>36.9071</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.64240</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.03690</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>36.4319</td>\n",
       "      <td>36.3303</td>\n",
       "      <td>36.3830</td>\n",
       "      <td>36.8548</td>\n",
       "      <td>36.7580</td>\n",
       "      <td>36.6907</td>\n",
       "      <td>36.6623</td>\n",
       "      <td>37.0027</td>\n",
       "      <td>36.9593</td>\n",
       "      <td>36.3512</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.0510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.60880</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.01380</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>35.8735</td>\n",
       "      <td>35.6273</td>\n",
       "      <td>35.4106</td>\n",
       "      <td>35.6164</td>\n",
       "      <td>35.5813</td>\n",
       "      <td>35.8044</td>\n",
       "      <td>35.5934</td>\n",
       "      <td>35.4620</td>\n",
       "      <td>35.5980</td>\n",
       "      <td>35.5212</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.03820</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.02240</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>35.8008</td>\n",
       "      <td>35.3342</td>\n",
       "      <td>34.9129</td>\n",
       "      <td>35.2834</td>\n",
       "      <td>34.9831</td>\n",
       "      <td>35.0062</td>\n",
       "      <td>34.9329</td>\n",
       "      <td>34.7155</td>\n",
       "      <td>34.5654</td>\n",
       "      <td>34.8480</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.15710</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>35.8405</td>\n",
       "      <td>36.5209</td>\n",
       "      <td>36.6719</td>\n",
       "      <td>35.8548</td>\n",
       "      <td>35.7488</td>\n",
       "      <td>36.3303</td>\n",
       "      <td>36.2801</td>\n",
       "      <td>36.0733</td>\n",
       "      <td>36.2363</td>\n",
       "      <td>36.0139</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.12760</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.01960</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>36.4712</td>\n",
       "      <td>36.2225</td>\n",
       "      <td>35.9032</td>\n",
       "      <td>35.9038</td>\n",
       "      <td>35.8451</td>\n",
       "      <td>36.0860</td>\n",
       "      <td>36.3342</td>\n",
       "      <td>36.1656</td>\n",
       "      <td>36.3281</td>\n",
       "      <td>36.0159</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.70450</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.01080</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>35.9445</td>\n",
       "      <td>35.7613</td>\n",
       "      <td>35.7849</td>\n",
       "      <td>35.4040</td>\n",
       "      <td>35.5220</td>\n",
       "      <td>35.7389</td>\n",
       "      <td>35.6312</td>\n",
       "      <td>35.9214</td>\n",
       "      <td>35.6775</td>\n",
       "      <td>35.6592</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.64510</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>0.01470</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>34.7826</td>\n",
       "      <td>34.7688</td>\n",
       "      <td>34.6555</td>\n",
       "      <td>34.8770</td>\n",
       "      <td>34.8246</td>\n",
       "      <td>34.8829</td>\n",
       "      <td>34.3052</td>\n",
       "      <td>34.7220</td>\n",
       "      <td>34.1011</td>\n",
       "      <td>34.5315</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.97620</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.01990</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>35.0502</td>\n",
       "      <td>35.2669</td>\n",
       "      <td>35.1015</td>\n",
       "      <td>35.1433</td>\n",
       "      <td>34.8681</td>\n",
       "      <td>35.0520</td>\n",
       "      <td>34.9108</td>\n",
       "      <td>34.8891</td>\n",
       "      <td>34.7084</td>\n",
       "      <td>34.7714</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>35.25450</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.02600</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>35.1600</td>\n",
       "      <td>35.2510</td>\n",
       "      <td>35.2743</td>\n",
       "      <td>35.2611</td>\n",
       "      <td>35.1194</td>\n",
       "      <td>35.3374</td>\n",
       "      <td>35.2510</td>\n",
       "      <td>35.3081</td>\n",
       "      <td>35.2864</td>\n",
       "      <td>35.2967</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  time_shift  window_size lr_lambda  epochs  dropout  \\\n",
       "0   2025-11-09           3           92       cos      44      0.3   \n",
       "1   2025-11-09           2           20       cos      52      0.3   \n",
       "2   2025-11-09           4           61       cos      36      0.3   \n",
       "3   2025-11-09           5           92       cos      36      0.3   \n",
       "4   2025-11-09           2           61      _cos      40      0.3   \n",
       "5   2025-11-09           1           61      _cos      40      0.4   \n",
       "6   2025-11-07           1           20       cos      48      0.4   \n",
       "7   2025-11-07           2           34       cos      48      0.4   \n",
       "8   2025-11-07           3           34       cos      48      0.4   \n",
       "9   2025-11-07           2           34       cos      60      0.3   \n",
       "10  2025-11-07           1           34       cos      64      0.3   \n",
       "11  2025-11-07           3           34       cos      64      0.4   \n",
       "12  2025-11-10           1           50      _cos      64      0.4   \n",
       "13  2025-11-10           1           50      _cos      64      0.4   \n",
       "14  2025-11-10           1          100       cos      64      0.4   \n",
       "15  2025-11-10           2          100       cos      64      0.4   \n",
       "16  2025-11-11           1           50       cos      64      0.4   \n",
       "17  2025-11-11           1           21       cos      64      0.4   \n",
       "18  2025-11-11           1          100       cos      64      0.4   \n",
       "19  2025-11-11           3           50       cos      64      0.4   \n",
       "20  2025-11-11           3           50      _cos      64      0.4   \n",
       "21  2025-11-12           1           21       cos      64      0.4   \n",
       "22  2025-11-12           1          100       cos      64      0.4   \n",
       "23  2025-11-12           2           21       cos      64      0.4   \n",
       "24  2025-11-13           1           21       cos      64      0.4   \n",
       "25  2025-11-13           1          128       cos      64      0.4   \n",
       "26  2025-11-13           1           21       cos      64      0.4   \n",
       "27  2025-11-13           1           21       cos      64      0.4   \n",
       "28  2025-11-13           1           21       cos      64      0.4   \n",
       "29  2025-11-14           1           60       cos      64      0.4   \n",
       "30  2025-11-14           1          128       cos      64      0.4   \n",
       "31  2025-11-14           1           21       cos      64      0.3   \n",
       "\n",
       "    n_res_blocks  PL10_mean_mean  PL10_mean_std  PL10_std_mean  PL10_std_std  \\\n",
       "0              3        34.04296       0.296592        0.03677      0.032337   \n",
       "1              3        34.08798       0.101572        0.02693      0.010317   \n",
       "2              1        34.52822       0.428319        0.07696      0.025820   \n",
       "3              3        33.41389       0.592093        0.13273      0.085554   \n",
       "4              2        34.01628       0.334351        0.07227      0.023782   \n",
       "5              2        34.09213       0.271552        0.04879      0.013215   \n",
       "6              2        34.03229       0.147213        0.04455      0.014940   \n",
       "7              2        34.20160       0.203500        0.03720      0.024600   \n",
       "8              2        33.77220       0.147600        0.09480      0.032400   \n",
       "9              3        34.16810       0.252500        0.05370      0.024600   \n",
       "10             3        34.02170       0.156300        0.02940      0.015400   \n",
       "11             3        33.95390       0.195700        0.03610      0.019900   \n",
       "12             3        35.40610       0.425200        0.06490      0.028400   \n",
       "13             3        35.42660       0.679800        0.06220      0.028500   \n",
       "14             2        35.42250       0.245600        0.02160      0.009300   \n",
       "15             2        35.00700       0.131600        0.01540      0.007600   \n",
       "16             2        35.74070       0.370900        0.02400      0.012900   \n",
       "17             2        36.10310       0.173900        0.03080      0.007500   \n",
       "18             2        35.94890       0.276300        0.02310      0.008400   \n",
       "19             2        35.85040       0.508200        0.03040      0.014800   \n",
       "20             2        36.06580       0.590100        0.10110      0.035500   \n",
       "21             2        36.83560       0.222200        0.01770      0.011500   \n",
       "22             2        36.50060       0.211900        0.01570      0.008800   \n",
       "23             2        36.64240       0.255000        0.03690      0.011500   \n",
       "24             2        35.60880       0.140800        0.01380      0.009400   \n",
       "25             2        35.03820       0.353400        0.02240      0.006900   \n",
       "26             2        36.15710       0.304500        0.01350      0.004600   \n",
       "27             2        36.12760       0.212400        0.01960      0.008400   \n",
       "28             2        35.70450       0.165800        0.01080      0.005000   \n",
       "29             2        34.64510       0.259700        0.01470      0.004900   \n",
       "30             2        34.97620       0.175300        0.01990      0.008300   \n",
       "31             2        35.25450       0.066800        0.02600      0.003600   \n",
       "\n",
       "    0.0030_mean  0.0020_mean  0.0010_mean  0.0009_mean  0.0008_mean  \\\n",
       "0       33.8967      33.7666      34.1048      34.4722      34.3281   \n",
       "1       33.9794      34.2996      34.1232      34.1260      34.0834   \n",
       "2       34.8397      35.0390      35.1257      34.1310      34.6296   \n",
       "3       33.9495      34.1444      33.5586      33.4852      32.9203   \n",
       "4       34.3612      34.4437      34.0495      33.9382      33.3032   \n",
       "5       34.5063      34.4195      34.0148      33.8809      33.9621   \n",
       "6       33.8739      33.8561      34.0091      34.1713      34.1532   \n",
       "7       33.8877      33.8170      34.3975      34.1070      34.3238   \n",
       "8       33.7382      33.8192      33.5882      33.7218      33.4791   \n",
       "9       33.9715      34.0325      34.5261      34.3204      34.4775   \n",
       "10      33.8616      33.6905      34.1431      34.0800      34.0269   \n",
       "11      33.6247      34.0442      34.1751      34.0652      33.6538   \n",
       "12      34.8906      35.8110      34.8277      35.1031      35.3820   \n",
       "13      36.2859      35.4809      34.7119      35.2623      35.0421   \n",
       "14      35.6900      35.4300      35.7616      35.6318      35.3681   \n",
       "15      35.1352      35.0797      34.9388      35.0725      35.2059   \n",
       "16      35.7633      35.4769      35.6850      35.3690      35.8015   \n",
       "17      35.9709      36.0299      35.8589      36.1879      35.8114   \n",
       "18      35.8952      36.0750      35.8183      36.1429      36.3012   \n",
       "19      35.9462      36.2186      36.3577      36.4732      35.6464   \n",
       "20      37.4658      35.7302      35.6559      35.9850      36.3132   \n",
       "21      36.5582      36.5673      36.6765      36.7727      36.6858   \n",
       "22      36.5280      36.2985      36.4604      36.2603      36.5788   \n",
       "23      36.4319      36.3303      36.3830      36.8548      36.7580   \n",
       "24      35.8735      35.6273      35.4106      35.6164      35.5813   \n",
       "25      35.8008      35.3342      34.9129      35.2834      34.9831   \n",
       "26      35.8405      36.5209      36.6719      35.8548      35.7488   \n",
       "27      36.4712      36.2225      35.9032      35.9038      35.8451   \n",
       "28      35.9445      35.7613      35.7849      35.4040      35.5220   \n",
       "29      34.7826      34.7688      34.6555      34.8770      34.8246   \n",
       "30      35.0502      35.2669      35.1015      35.1433      34.8681   \n",
       "31      35.1600      35.2510      35.2743      35.2611      35.1194   \n",
       "\n",
       "    0.0007_mean  0.0006_mean  0.0005_mean  0.0004_mean  0.0003_mean  \\\n",
       "0       34.1199      34.1343      33.9614      34.2144      33.4312   \n",
       "1       34.0315      34.0361      34.1098      34.1540      33.9368   \n",
       "2       34.7337      34.0210      34.6563      34.0728      34.0334   \n",
       "3       34.1612      33.1922      33.3552      33.1304      32.2419   \n",
       "4       34.1230      33.9392      33.9461      33.7411      34.3176   \n",
       "5       33.8735      33.8598      33.7669      34.3787      34.2588   \n",
       "6       33.8691      34.1332      33.9426      34.0381      34.2763   \n",
       "7       34.2922      34.3360      34.2902      34.3743      34.1906   \n",
       "8       33.9450      33.9421      33.8468      33.8459      33.7960   \n",
       "9       34.1714      34.3446      33.7550      33.9080      34.1736   \n",
       "10      34.1495      34.0671      34.1982      34.0890      33.9112   \n",
       "11      34.0870      33.9074      34.0052      33.8273      34.1494   \n",
       "12      35.3879      35.0688      35.8009      35.7868      36.0026   \n",
       "13      35.1546      35.3421      34.9806      36.9482      35.0570   \n",
       "14      35.4666      35.5247      35.2215      35.0339      35.0971   \n",
       "15      34.7787      34.8483      35.0720      34.9944      34.9447   \n",
       "16      36.3752      35.0792      36.0552      35.7728      36.0289   \n",
       "17      36.1512      36.2648      36.2854      36.2138      36.2563   \n",
       "18      36.1458      36.0727      35.5236      36.0581      35.4566   \n",
       "19      35.5069      35.6691      35.3255      34.9703      36.3903   \n",
       "20      35.6608      36.2174      36.0467      36.2851      35.2974   \n",
       "21      36.9320      36.8448      37.0639      37.2044      37.0501   \n",
       "22      36.4379      36.7048      36.6045      36.2259      36.9071   \n",
       "23      36.6907      36.6623      37.0027      36.9593      36.3512   \n",
       "24      35.8044      35.5934      35.4620      35.5980      35.5212   \n",
       "25      35.0062      34.9329      34.7155      34.5654      34.8480   \n",
       "26      36.3303      36.2801      36.0733      36.2363      36.0139   \n",
       "27      36.0860      36.3342      36.1656      36.3281      36.0159   \n",
       "28      35.7389      35.6312      35.9214      35.6775      35.6592   \n",
       "29      34.8829      34.3052      34.7220      34.1011      34.5315   \n",
       "30      35.0520      34.9108      34.8891      34.7084      34.7714   \n",
       "31      35.3374      35.2510      35.3081      35.2864      35.2967   \n",
       "\n",
       "    0.0030_std  0.0020_std  0.0010_std  0.0009_std  0.0008_std  0.0007_std  \\\n",
       "0       0.0230      0.0407      0.0196      0.0111      0.0304      0.0233   \n",
       "1       0.0222      0.0314      0.0139      0.0264      0.0359      0.0211   \n",
       "2       0.1019      0.0576      0.1342      0.0958      0.0539      0.0594   \n",
       "3       0.0545      0.0252      0.0810      0.0973      0.1441      0.1443   \n",
       "4       0.0815      0.0663      0.0519      0.0770      0.1233      0.0834   \n",
       "5       0.0596      0.0394      0.0345      0.0579      0.0617      0.0406   \n",
       "6       0.0277      0.0379      0.0543      0.0614      0.0696      0.0501   \n",
       "7       0.0247      0.0238      0.0367      0.0367      0.0163      0.1037   \n",
       "8       0.1201      0.0930      0.0927      0.0939      0.0571      0.1116   \n",
       "9       0.0252      0.0654      0.0168      0.0602      0.0510      0.0906   \n",
       "10      0.0285      0.0686      0.0262      0.0238      0.0237      0.0255   \n",
       "11      0.0803      0.0378      0.0197      0.0215      0.0566      0.0425   \n",
       "12      0.1141      0.0480      0.0449      0.0483      0.0288      0.0532   \n",
       "13      0.0740      0.1258      0.0430      0.0395      0.0343      0.0541   \n",
       "14      0.0226      0.0113      0.0224      0.0411      0.0326      0.0214   \n",
       "15      0.0196      0.0094      0.0108      0.0224      0.0152      0.0295   \n",
       "16      0.0557      0.0277      0.0070      0.0238      0.0135      0.0244   \n",
       "17      0.0293      0.0425      0.0360      0.0318      0.0261      0.0377   \n",
       "18      0.0132      0.0238      0.0274      0.0188      0.0418      0.0263   \n",
       "19      0.0168      0.0146      0.0139      0.0141      0.0369      0.0426   \n",
       "20      0.1523      0.1100      0.1231      0.0650      0.1557      0.0533   \n",
       "21      0.0242      0.0138      0.0453      0.0081      0.0230      0.0085   \n",
       "22      0.0220      0.0328      0.0128      0.0114      0.0093      0.0247   \n",
       "23      0.0305      0.0271      0.0450      0.0486      0.0504      0.0256   \n",
       "24      0.0309      0.0315      0.0111      0.0111      0.0099      0.0111   \n",
       "25      0.0236      0.0117      0.0273      0.0172      0.0299      0.0318   \n",
       "26      0.0205      0.0157      0.0219      0.0098      0.0129      0.0092   \n",
       "27      0.0312      0.0262      0.0103      0.0229      0.0122      0.0320   \n",
       "28      0.0193      0.0196      0.0139      0.0079      0.0076      0.0077   \n",
       "29      0.0164      0.0144      0.0194      0.0240      0.0114      0.0150   \n",
       "30      0.0161      0.0244      0.0327      0.0221      0.0253      0.0275   \n",
       "31      0.0296      0.0296      0.0241      0.0268      0.0265      0.0287   \n",
       "\n",
       "    0.0006_std  0.0005_std  0.0004_std  0.0003_std  \n",
       "0       0.0211      0.0476      0.0271      0.1238  \n",
       "1       0.0506      0.0239      0.0195      0.0244  \n",
       "2       0.0603      0.0749      0.0652      0.0664  \n",
       "3       0.1279      0.3019      0.2520      0.0991  \n",
       "4       0.0848      0.0614      0.0567      0.0364  \n",
       "5       0.0459      0.0678      0.0274      0.0531  \n",
       "6       0.0495      0.0227      0.0357      0.0366  \n",
       "7       0.0428      0.0314      0.0265      0.0291  \n",
       "8       0.1641      0.0793      0.0508      0.0850  \n",
       "9       0.0771      0.0743      0.0499      0.0266  \n",
       "10      0.0406      0.0228      0.0130      0.0208  \n",
       "11      0.0172      0.0261      0.0376      0.0219  \n",
       "12      0.0634      0.0602      0.1130      0.0748  \n",
       "13      0.0580      0.0909      0.0645      0.0382  \n",
       "14      0.0177      0.0183      0.0104      0.0185  \n",
       "15      0.0069      0.0128      0.0066      0.0213  \n",
       "16      0.0185      0.0214      0.0284      0.0194  \n",
       "17      0.0252      0.0345      0.0285      0.0162  \n",
       "18      0.0274      0.0184      0.0202      0.0138  \n",
       "19      0.0397      0.0289      0.0412      0.0551  \n",
       "20      0.0653      0.0926      0.0876      0.1060  \n",
       "21      0.0208      0.0079      0.0152      0.0101  \n",
       "22      0.0030      0.0176      0.0089      0.0148  \n",
       "23      0.0393      0.0193      0.0325      0.0510  \n",
       "24      0.0100      0.0062      0.0108      0.0057  \n",
       "25      0.0171      0.0290      0.0198      0.0161  \n",
       "26      0.0124      0.0083      0.0112      0.0132  \n",
       "27      0.0176      0.0177      0.0184      0.0080  \n",
       "28      0.0069      0.0093      0.0097      0.0065  \n",
       "29      0.0178      0.0098      0.0097      0.0091  \n",
       "30      0.0196      0.0108      0.0157      0.0050  \n",
       "31      0.0297      0.0239      0.0196      0.0216  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
