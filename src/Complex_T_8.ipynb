{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas_ta as ta\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import numpy as np\n",
    "from data_loader_c1 import TwoStreamDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from data_loader_c1 import load_dataloader\n",
    "from model_real_1 import RealCNNLSTM\n",
    "from model_complex_1 import ComplexCNNLSTM\n",
    "from model_fusion_1 import FusionGated\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results_acc(results_acc, result_tab, config):\n",
    "    \"\"\"Запись результатов\"\"\"\n",
    "    results_acc.loc[len(results_acc)] = [result_tab['date'][0], config['data']['time_shift'], config['data']['window_size'], \\\n",
    "    config['data']['lr_lambda'], config['train']['epochs'], config['model']['dropout'], config['model']['n_res_blocks'], \\\n",
    "    round(result_tab['Pred_last_10_ep_mean'].mean(), 4), round(result_tab['Pred_last_10_ep_mean'].std(), 4), \\\n",
    "    round(result_tab['Pred_last_10_ep_std'].mean(), 4), round(result_tab['Pred_last_10_ep_std'].std(), 4)] + \\\n",
    "    result_tab['Pred_last_10_ep_mean'].tolist() + result_tab['Pred_last_10_ep_std'].tolist()\n",
    "    return results_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_time_series_split_4(df_len, time_shift=1, interval_days=1, main_splits_number=2, initial_train_frac=0.90):\n",
    "    splits = []\n",
    "    df_len_sh = df_len - interval_days - time_shift + 1\n",
    "    initial_train_size = int(df_len_sh * initial_train_frac)\n",
    "\n",
    "    initial_train_size += (df_len_sh - initial_train_size) % main_splits_number\n",
    "    val_size = int((df_len_sh - initial_train_size) / main_splits_number)\n",
    "    assert val_size not in range(1)\n",
    "    \n",
    "    train_end = initial_train_size - time_shift + 1\n",
    "    val_start = initial_train_size\n",
    "    val_end = val_start + val_size\n",
    "\n",
    "    while val_end <= df_len_sh:\n",
    "        \n",
    "        train_idx = list(range(0, train_end))\n",
    "        val_idx = list(range(val_start, val_end))\n",
    "        splits.append((train_idx, val_idx))\n",
    "        train_end = val_end\n",
    "        val_start = train_end\n",
    "        val_end = val_start + val_size\n",
    "\n",
    "    # print(splits)\n",
    "    one_day_start = splits[-1][1][-1]\n",
    "\n",
    "    for i in range(interval_days):\n",
    "        train_idx = list(range(0, one_day_start + i + 1))\n",
    "        val_idx = list(range(one_day_start + i + time_shift, one_day_start + i + time_shift + 1))\n",
    "        splits.append((train_idx, val_idx))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = custom_time_series_split_4(55, time_shift=2, interval_days=1, main_splits_number=1, initial_train_frac=0.90)\n",
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Вариант с двумя фолдами!!!\n",
    "\n",
    "# ---------- EarlyStopping ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=1e-2, mode=\"min\", restore_best=True, rel_delta=True):\n",
    "        assert mode in (\"min\", \"max\")\n",
    "        self.patience, self.min_delta, self.mode = patience, float(min_delta), mode\n",
    "        self.restore_best, self.rel_delta = restore_best, rel_delta\n",
    "        self.best, self.wait, self.best_states = None, 0, {}\n",
    "\n",
    "    def _improved(self, cur, best):\n",
    "        if self.mode == \"min\":\n",
    "            return cur < (best*(1-self.min_delta) if self.rel_delta else best - self.min_delta)\n",
    "        else:\n",
    "            return cur > (best*(1+self.min_delta) if self.rel_delta else best + self.min_delta)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, metric: float, models: Dict[str, torch.nn.Module]) -> bool:\n",
    "        if self.best is None or self._improved(metric, self.best):\n",
    "            self.best, self.wait = metric, 0\n",
    "            self.best_states = {k:{n:v.detach().cpu().clone() for n,v in m.state_dict().items()}\n",
    "                                for k,m in models.items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, models: Dict[str, torch.nn.Module]) -> None:\n",
    "        if not self.best_states: return\n",
    "        for k,m in models.items():\n",
    "            if k in self.best_states:\n",
    "                m.load_state_dict(self.best_states[k])\n",
    "\n",
    "\n",
    "# ---------- вспомогательные утилиты ----------\n",
    "def _set_seed(seed=42):\n",
    "    import os, random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _reset_weights(m: nn.Module):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "        m.reset_parameters()\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, p in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(p.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(p.data)\n",
    "                h = p.shape[0]//4\n",
    "                p.data[h:2*h] = 1.0  # forget-bias\n",
    "\n",
    "\n",
    "# ---------- основная функция ----------\n",
    "def train_tscv_model_7(config: dict, df: pd.DataFrame, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Обучение с кастомным TSCV, EMA-сглаживанием, EarlyStopping, LR-Plateau, grad-accum, clip-grad.\n",
    "    Возвращает:\n",
    "        results: dict с метриками по фолдам и сериями предсказаний\n",
    "    \"\"\"\n",
    "    _set_seed(config.get(\"seed\", 42))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- даты и срез df ---\n",
    "    start_preds_date = pd.to_datetime(config['data']['start_preds_date'], errors='coerce')\n",
    "    stop_preds_date  = pd.to_datetime(config['data']['stop_preds_date'],  errors='coerce')\n",
    "    time_shift       = int(config['data']['time_shift'])\n",
    "    window_size      = int(config['data']['window_size'])\n",
    "    lr_lambda_type = config['data']['lr_lambda']\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df[pd.to_datetime(df['date']) <= stop_preds_date].reset_index(drop=True)\n",
    "\n",
    "    interval_days = int((stop_preds_date - start_preds_date).days)\n",
    "    if verbose:\n",
    "        print(\"Dates:\", start_preds_date, \"→\", stop_preds_date,\n",
    "              \"| interval_days:\", interval_days, \"| time_shift:\", time_shift)\n",
    "\n",
    "    # --- сплиты ---\n",
    "    # splits = custom_time_series_split_3(\n",
    "    #     len(df),\n",
    "    #     time_shift=time_shift,\n",
    "    #     interval_days=interval_days,\n",
    "    #     main_splits_number=0,\n",
    "    #     initial_train_frac=0.99\n",
    "    # )\n",
    "    # splits = custom_time_series_split_2(len(df), time_shift=time_shift, interval_days=interval_days, main_splits_number=2, initial_train_frac=0.90)\n",
    "    splits = custom_time_series_split_4(len(df), time_shift=time_shift, interval_days=1, main_splits_number=1, initial_train_frac=0.90)\n",
    "    if verbose: print('num of splits =', len(splits))\n",
    "\n",
    "    # --- bootstrap-loader (определяем F и C до инициализации моделей) ---\n",
    "    first_train_idx, _ = splits[0]\n",
    "    bootstrap_loader = load_dataloader(\n",
    "        df.iloc[first_train_idx], window_size=window_size,\n",
    "        batch_size=config['train']['batch_size'],\n",
    "        shuffle=False, drop_last=False\n",
    "    )\n",
    "    first_batch = next(iter(bootstrap_loader))\n",
    "    F = first_batch['real_feats'].shape[-1]\n",
    "    C = (first_batch['complex_time_real'].shape[-1]\n",
    "         if ('complex_time_real' in first_batch and 'complex_time_imag' in first_batch) else 1)\n",
    "\n",
    "    # --- модели ---\n",
    "    real_net = RealCNNLSTM(\n",
    "        num_real_features=F,\n",
    "        hidden_dim=config['model']['real_hidden_dim'],\n",
    "        lstm_layers=config['model'].get('real_lstm_layers', 2),\n",
    "        dropout=config['model'].get('dropout', 0.3),\n",
    "        kernel_size=3,\n",
    "        bidirectional=config['model'].get('bidirectional', True),\n",
    "        take=config['model'].get('take', \"last_timestep\"),\n",
    "        proj_out=True\n",
    "    ).to(device)\n",
    "\n",
    "    complex_net = ComplexCNNLSTM(\n",
    "        in_channels=C,\n",
    "        hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        num_layers=config['model']['n_res_blocks'],\n",
    "        dropout=config['model']['dropout'],\n",
    "        kernel_size=3,\n",
    "        proj_out=False\n",
    "    ).to(device)\n",
    "\n",
    "    fusion_net = FusionGated(\n",
    "        real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "        complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        output_dim=1, hidden=128, dropout=0.2, use_softmax=True\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # --- ✅ Инициализация bias последнего линейного слоя ---\n",
    "    # with torch.no_grad():\n",
    "    #     for mod in fusion_net.modules():\n",
    "    #         if isinstance(mod, torch.nn.Linear) and mod.out_features == 1:\n",
    "    #             mod.bias.fill_(0.0)   # так как таргет уже нормирован (среднее ~ 0)\n",
    "\n",
    "    # --- оптимизатор/лоссы/планировщик/ES ---\n",
    "    optimizer = optim.Adam(\n",
    "        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "        lr=config['train']['lr'], weight_decay=1e-4\n",
    "    )\n",
    "    criterion = nn.MSELoss() if config['train'].get('loss', 'mse') == 'mse' else nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "    epochs = int(config['train']['epochs'])\n",
    "    # warmup: 20% эпох, но не менее 1 и не более epochs-1\n",
    "    warmup_epochs = max(1, min(epochs - 1, int(0.2 * epochs)))\n",
    "    min_factor = 1e-3\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t))\n",
    "    else: \n",
    "        def lr_lambda(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 1.\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5\n",
    "            else:\n",
    "                return 0.1\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda_fold2(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs * 0.2\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t)) * 0.2\n",
    "    else: \n",
    "        def lr_lambda_fold2(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 0.4\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5 * 0.4\n",
    "            else:\n",
    "                return 0.1 * 0.4\n",
    "            \n",
    "    # else: \n",
    "    #     def lr_lambda(epoch: int):\n",
    "    #         if epoch < epochs // 4:\n",
    "    #             return 1.\n",
    "    #         elif epochs // 4 <= epoch < (epochs // 4) * 2:\n",
    "    #             return 0.5\n",
    "    #         elif (epochs // 4) * 2 <= epoch < (epochs // 4) * 3:\n",
    "    #             return 0.1\n",
    "    #         else:\n",
    "    #             return 0.05\n",
    "            \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    early = EarlyStopping(\n",
    "        patience=config['train'].get('early_stop_patience', 10),\n",
    "        min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "        mode=\"min\", restore_best=True, rel_delta=True\n",
    "    )\n",
    "    ema_beta = float(config['train'].get('ema_beta', 0.8))\n",
    "\n",
    "    # --- тренинг-опции ---\n",
    "    epochs         = int(config['train']['epochs'])\n",
    "    batch_size     = int(config['train']['batch_size'])\n",
    "    accum_steps    = int(config['train'].get('accum_steps', 1))\n",
    "    max_grad_norm  = config['train'].get('max_grad_norm', 1.0)\n",
    "    min_bs_for_bn  = int(config['train'].get('min_bs_for_bn', 2))\n",
    "    reset_each_fold= bool(config['train'].get('reset_each_fold', False))\n",
    "\n",
    "    if verbose: print('size of df:', df.shape)\n",
    "\n",
    "    # --- нормализация (фиксируем μ,σ по первому train-сплиту) ---\n",
    "    cols_to_scale = config['data'].get('cols_to_scale', [\n",
    "        'DayAvgPrice','IntradayStd','Volume','Log_Profit','DayAvgPrice_diff',\n",
    "        'DAP_1','DAP_2','DAP_3','DAP_4','DAP_5','DAP_6',\n",
    "        'POLY_1','POLY_2','POLY_3','lambda_C3','lambda_C2','lambda_C1'\n",
    "    ])\n",
    "\n",
    "    mu = sigma = None\n",
    "    mu_target = sigma_target = None\n",
    "\n",
    "    # --- сбор результатов по фолдам ---\n",
    "    fold_metrics: List[Dict] = []\n",
    "    fold_pred_series: List[pd.Series] = []\n",
    "    preds_last_10 = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        # опциональный ресет весов (для «честной» CV)\n",
    "        if reset_each_fold and (fold + 1) > 1:\n",
    "            # real_net.apply(_reset_weights)\n",
    "            # complex_net.apply(_reset_weights)\n",
    "            # fusion_net.apply(_reset_weights)\n",
    "            # optimizer = optim.Adam(\n",
    "            #     list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "            #     lr=config['train']['lr']\n",
    "            # )\n",
    "            # сбросить состояние LR-схем, ES, EMA\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=100, threshold=1e-3, threshold_mode='rel',\n",
    "            #     cooldown=2, verbose=True\n",
    "            # )\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=5, verbose=verbose\n",
    "            # )\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fold2)\n",
    "            early = EarlyStopping(\n",
    "                patience=config['train'].get('early_stop_patience', 10),\n",
    "                min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "                mode=\"min\", restore_best=True, rel_delta=True\n",
    "            )\n",
    "            ema_val = None\n",
    "        else:\n",
    "            ema_val = None  # сбрасываем EMA в любом случае на новый фолд (иначе метрики не сопоставимы)\n",
    "\n",
    "        train_df = df.iloc[train_idx].copy()\n",
    "\n",
    "        # расширим val назад до window_size, безопасно по границам\n",
    "        if len(val_idx) <= window_size:\n",
    "            end = val_idx[-1]\n",
    "            start = max(0, end - window_size)\n",
    "            val_idx = list(range(start, end + 1))\n",
    "\n",
    "        val_df = df.iloc[val_idx].copy()\n",
    "\n",
    "        # μ,σ считаем по ПЕРВОМУ train-сплиту\n",
    "        if mu is None or sigma is None:\n",
    "            mu = train_df[cols_to_scale].mean()\n",
    "            sigma = train_df[cols_to_scale].std().replace(0, 1.0)\n",
    "\n",
    "        # применяем нормировку\n",
    "        for col in cols_to_scale:\n",
    "            train_df.loc[:, col] = (train_df[col] - mu[col]) / sigma[col]\n",
    "            val_df.loc[:,   col] = (val_df[col]   - mu[col]) / sigma[col]\n",
    "\n",
    "        # μ,σ для z-нормировки Target\n",
    "        if mu_target is None or sigma_target is None:\n",
    "            mu_target = train_df['Target'].mean()\n",
    "            sigma_target = train_df['Target'].std()\n",
    "\n",
    "        train_df['Target'] = (train_df['Target'] - mu_target) / sigma_target\n",
    "        val_df['Target'] = (val_df['Target']   - mu_target) / sigma_target \n",
    "        train_df['Target_smooth'] = (train_df['Target_smooth'] - mu_target) / sigma_target\n",
    "        val_df['Target_smooth'] = (val_df['Target_smooth']   - mu_target) / sigma_target  \n",
    "\n",
    "        #print(f\"\\ntrain_end {train_df.iloc[-1]}, val_end {val_df.iloc[-1]}\")  \n",
    "\n",
    "        # лоадеры\n",
    "        train_loader = load_dataloader(train_df, window_size, batch_size, shuffle=True,  drop_last=False)\n",
    "        val_loader   = load_dataloader(val_df,   window_size, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFold {fold+1}: train {len(train_df)}, val {len(val_df)}\",\n",
    "                  \"\\ntrain first...last :\", train_df['date'].min(), \"...\" , train_df['date'].max(),\n",
    "                  \"\\nval first...last   :\", val_df['date'].min(), \"...\" ,   val_df['date'].max())\n",
    "\n",
    "        # контрольный принт последнего окна\n",
    "            \n",
    "        dst   = TwoStreamDataset(train_df, window_size=window_size)\n",
    "        sampt = dst[len(dst) - 1]\n",
    "        last_DAP_in_t_window = float(sampt['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        target_pricet           = float(sampt['target']) * sigma_target + mu_target\n",
    "        target_smooth_pricet = train_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"train last row | DAP={last_DAP_in_t_window:.4f} | Target={target_pricet:.4f} | Target_smooth={target_smooth_pricet:.4f}\")\n",
    "\n",
    "        ds   = TwoStreamDataset(val_df, window_size=window_size)\n",
    "        samp = ds[len(ds) - 1]\n",
    "        # last_DAP_in_val_window = float(samp['real_feats'][-1, 0]) * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        last_DAP_in_val_window = float(samp['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        # last_DAP_in_val_window = val_df['imag_time'].iloc[-1] * sigma['imag_time'] + mu['imag_time']\n",
    "        # last_DAP_in_val_window = val_df['DayAvgPrice'].iloc[-1] * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        target_price           = float(samp['target']) * sigma_target + mu_target\n",
    "        target_smooth_price = val_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"val last row   | DAP={last_DAP_in_val_window:.4f} | Target={target_price:.4f} | Target_smooth={target_smooth_price:.4f}\")\n",
    "\n",
    "        # Графики\n",
    "        # from utils_live_plot import EMAMeter, TrainingPlotter  # если вынес в файл, иначе опусти импорт\n",
    "        # ema_meter    = EMAMeter(beta=config['train'].get('ema_beta', 0.8))\n",
    "        # live_plotter = TrainingPlotter(max_points=500)\n",
    "\n",
    "        # ----- ЭПОХИ -----\n",
    "        for epoch in range(epochs):\n",
    "            real_net.train(); complex_net.train(); fusion_net.train()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # TRAIN\n",
    "            step = 0\n",
    "            for step, batch in enumerate(train_loader, start=1):\n",
    "                x_real = batch['real_feats'].to(device)\n",
    "                if x_real.size(0) < min_bs_for_bn:\n",
    "                    continue\n",
    "                x_complex = torch.complex(\n",
    "                    batch['complex_time_real'].to(device),\n",
    "                    batch['complex_time_imag'].to(device)\n",
    "                )\n",
    "                y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                h_real = real_net(x_real)\n",
    "                h_r, h_i = complex_net(x_complex)\n",
    "                out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                loss = criterion(out, y) / accum_steps\n",
    "                if step == len(train_loader) - 1: train_loss_temp = loss\n",
    "                loss.backward()\n",
    "\n",
    "                if step % accum_steps == 0:\n",
    "                    if max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                            max_grad_norm\n",
    "                        )\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # доводим хвост аккумулирования\n",
    "            if step % accum_steps != 0:\n",
    "                if max_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                        max_grad_norm\n",
    "                    )\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # VALID\n",
    "            real_net.eval(); complex_net.eval(); fusion_net.eval()\n",
    "            val_loss_sum, n_val = 0.0, 0\n",
    "            pred_vals, pred_dates = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_real = batch['real_feats'].to(device)\n",
    "                    x_complex = torch.complex(\n",
    "                        batch['complex_time_real'].to(device),\n",
    "                        batch['complex_time_imag'].to(device)\n",
    "                    )\n",
    "                    y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                    h_real = real_net(x_real)\n",
    "                    h_r, h_i = complex_net(x_complex)\n",
    "                    out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                    val_loss_sum += criterion(out, y).item()\n",
    "                    n_val += 1\n",
    "\n",
    "                    # даты предиктов\n",
    "                    pred_vals.extend(out.squeeze(-1).cpu().numpy())\n",
    "\n",
    "                    if 'target_date' in batch:\n",
    "                        pred_dates.extend(list(batch['target_date']))\n",
    "                    else:\n",
    "                        # fallback: считаем по глобальному индексу\n",
    "                        batch_idx_local  = batch['row_idx'].cpu().numpy()   # индекс окна в val_df\n",
    "                        global_start     = val_idx[0]                        # смещение в исходном df\n",
    "                        batch_idx_global = global_start + batch_idx_local\n",
    "                        # y = target[idx + window_size] → цель на \"завтра\" относительно начала окна\n",
    "                        tgt_idx = batch_idx_global + window_size\n",
    "                        pred_dates.extend(df['date'].iloc[tgt_idx].tolist())\n",
    "\n",
    "            avg_val_loss = val_loss_sum / max(1, n_val)\n",
    "\n",
    "            # --- EMA для scheduler/early ---\n",
    "            ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            use_metric = ema_val\n",
    "\n",
    "            scheduler.step()\n",
    "            # if epoch+1 >= config['train'].get('early_stop_warmup', 0):\n",
    "            #     scheduler.step()\n",
    "            #     #scheduler.step(use_metric)\n",
    "            #     stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     # диагностика (разово/по условию)\n",
    "            #     # print(f\"[ES] e{epoch+1} metric={use_metric:.5f}, best={early.best}, wait={early.wait}\")\n",
    "            #     if stop:\n",
    "            #         if verbose:\n",
    "            #             print(f\"[EarlyStop] best(ema)={early.best:.4f} → restore best weights\")\n",
    "            #         early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #         break\n",
    "            # else:\n",
    "            #     pass\n",
    "\n",
    "            # EMA для scheduler/early\n",
    "            # ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            # use_metric = ema_val  # <— ключевая строка: ES и Plateau смотрят на одно и то же\n",
    "\n",
    "            # Графики\n",
    "            # если ты уже считаешь ema_val сам — можно синхронизировать:\n",
    "            # ema_val = ema_meter.update(avg_val_loss)\n",
    "            # # обновляем график каждые, скажем, 1-5 эпох\n",
    "            # live_plotter.update(epoch+1, avg_val_loss, ema_val)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Validation Loss vs EMA\")\n",
    "            # Графики\n",
    "            # pred_val_denorm = [x * sigma_target + mu_target for x in pred_vals][0]    # откат нормировки целевой переменной\n",
    "            # live_plotter.update(epoch+1, pred_val_denorm, last_DAP_in_val_window)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Predict vs DAP\")\n",
    "            # Графики\n",
    "\n",
    "            # scheduler.step(use_metric)\n",
    "            # stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "            if verbose and ((epoch+1) % 4 == 0 or (epoch+1) == 1):\n",
    "                print(f\"Epoch {epoch+1:03d} | train_loss={train_loss_temp:.6f} | val_loss={avg_val_loss:.4f} | ema={ema_val:.4f} | lr={optimizer.param_groups[0]['lr']:.5f} | pred = {pred_vals_denorm[0]:.4f}\")\n",
    "\n",
    "            if (epoch >= (epochs - 10)) and (fold + 1 == len(splits)):\n",
    "                preds_last_10.append(round(pred_vals_denorm[0], 4))\n",
    "            # if stop:\n",
    "            #     if verbose:\n",
    "            #         print(f\"[EarlyStop] best(ema)={early.best:.6f} → restore best weights\")\n",
    "            #     early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     break\n",
    "\n",
    "        # пост-валид метрики по датам target\n",
    "        pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "        print(f\"pred = {pred_vals_denorm[-1]:.4f}, {len(pred_vals_denorm)}\")\n",
    "        pred_series = pd.Series(pred_vals_denorm, index=pd.to_datetime(pred_dates))\n",
    "        gt_series   = pd.to_datetime(df['date']).map(pd.Timestamp).map(\n",
    "            lambda d: d\n",
    "        )  # просто для явности типов\n",
    "        # берём фактические target по тем же датам:\n",
    "        gt_series = df.set_index(pd.to_datetime(df['date']))['Target'].reindex(pred_series.index)\n",
    "\n",
    "        mae_target_pred = mean_absolute_error(gt_series, pred_series)\n",
    "        # dap_series = df.set_index(pd.to_datetime(df['date']))['DayAvgPrice'].reindex(pred_series.index)\n",
    "        dap_series = df.set_index(pd.to_datetime(df['date']))['c_month_real'].reindex(pred_series.index)\n",
    "        mae_target_dap  = mean_absolute_error(gt_series, dap_series)\n",
    "\n",
    "        if fold + 1 == len(splits):\n",
    "            fold_pred_series.append(pred_series)\n",
    "            fold_metrics.append({\n",
    "                \"fold\": fold + 1,\n",
    "                \"n_pred\": len(pred_series),\n",
    "                \"mae_target_pred\": float(mae_target_pred),\n",
    "                \"mae_target_dap\":  float(mae_target_dap),\n",
    "                \"val_loss_last_epoch\": float(avg_val_loss),\n",
    "            })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Fold {fold+1}] Target = {gt_series[-1]:.4f} | Predict = {pred_series[-1]:.4f}\")\n",
    "            print(f\"MAE(Target~DAP) = {mae_target_dap:.4f} | MAE(Target~Predict) = {mae_target_pred:.4f}\\n\")\n",
    "\n",
    "    torch.save(real_net.state_dict(), f\"../checkpoints/trained_real_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(complex_net.state_dict(), f\"../checkpoints/trained_complex_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(fusion_net.state_dict(), f\"../checkpoints/trained_fusion_net_{config['train']['lr']}.pth\")\n",
    "\n",
    "    # агрегированные результаты\n",
    "    results = {\n",
    "        \"fold_metrics\": fold_metrics,\n",
    "        \"pred_series_per_fold\": fold_pred_series,\n",
    "        \"preds_last_10\": preds_last_10,\n",
    "        \"mu\": mu, \"sigma\": sigma,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['open', 'high', 'low', 'close', 'DayAvgPrice', 'IntradayStd',\n",
    "       'Volume', 'day_of_week', 'day_of_year', 'Log_Profit',\n",
    "       'DayAvgPrice_diff', 'DayAvgPrice_2diff', 'POLY_1', 'POLY_2', 'POLY_3',\n",
    "       'parkinson_vol', 'parkinson_vol_ma5',\n",
    "       'parkinson_vol_ma20', 'parkinson_vol_diff1', 'parkinson_vol_lag1',\n",
    "       'DAP_1', 'DAP_2', 'DAP_3', 'DAP_4', 'DAP_5', 'DAP_6', 'DAP_7', 'DAP_10', \n",
    "       'DayAvgPrice_roll5', 'DayAvgPrice_roll10', 'DayAvgPrice_roll20',\n",
    "       'DayAvgPrice_ema5', 'DayAvgPrice_ema20', 'IntradayStd_roll5',\n",
    "       'IntradayStd_roll10', 'IntradayStd_roll20', 'IntradayStd_ema5',\n",
    "       'IntradayStd_ema20', 'mean_w', 'std_w',\n",
    "       'q10_w', 'q90_w', 'slope_w', 'vol_w', 'macd', 'macd_signal', 'macd_hist',\n",
    "       'bb_low', 'bb_mid', 'bb_up', 'atr14', 'dphi_hilbert',\n",
    "       'stft_energy_low', 'stft_energy_mid', 'stft_energy_high',\n",
    "       'gk_sigma', 'rs_sigma', 'yz_sigma', 'adx', 'chop', 'kalm_slope', 'rv20', 'vol_of_vol', \n",
    "       'bb_pct_b', 'bb_bandwidth', 'ret_overnight', 'ret_intraday', 'corr_ret_dlogvol', \n",
    "       'c_month_real',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"seed\": 42,\n",
    "  \"data\": {\n",
    "    \"start_preds_date\": \"2025-11-11\",\n",
    "    \"stop_preds_date\": \"2025-11-12\",\n",
    "    \"time_shift\": 2,\n",
    "    \"window_size\": 21,\n",
    "    \"cols_to_scale\": cols_to_scale,\n",
    "    \"lr_lambda\": \"cos\"\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"real_hidden_dim\": 128,\n",
    "    \"complex_hidden_dim\": 96,\n",
    "    \"n_res_blocks\": 2,\n",
    "    \"dropout\": 0.4,\n",
    "    \"bidirectional\": True,\n",
    "    \"take\": \"last_timestep\",\n",
    "    \"real_lstm_layers\": 2\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"lr\": 0.001,\n",
    "    \"loss\": \"SmoothL1Loss\",\n",
    "    \"epochs\": 64,\n",
    "    \"batch_size\": 96,\n",
    "    \"accum_steps\": 1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"min_bs_for_bn\": 2,\n",
    "    \"early_stop_patience\": 200,\n",
    "    \"early_stop_min_delta\": 1e-2,\n",
    "    \"ema_beta\": 0.8,\n",
    "    \"reset_each_fold\": True\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем конфигурацию\n",
    "# df = pd.read_csv('../data_archiv/DTG/DTG_new_fea_to_12_11_2025_1d_w21_noweekend.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем конфигурацию\n",
    "df = pd.read_csv(f\"../data_archiv/DTG/DTG_new_fea_to_12_11_2025_1d_w{config['data']['window_size']}_noweekend.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DayAvgPrice</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>35.070833</td>\n",
       "      <td>35.094444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>2025-10-24</td>\n",
       "      <td>35.294445</td>\n",
       "      <td>34.881389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>35.094444</td>\n",
       "      <td>35.007222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>34.881389</td>\n",
       "      <td>34.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>35.007222</td>\n",
       "      <td>34.751945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>34.865000</td>\n",
       "      <td>34.606944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>34.751945</td>\n",
       "      <td>33.886111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>34.606944</td>\n",
       "      <td>34.891389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>2025-11-04</td>\n",
       "      <td>33.886111</td>\n",
       "      <td>35.163611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>34.891389</td>\n",
       "      <td>34.328333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>2025-11-06</td>\n",
       "      <td>35.163611</td>\n",
       "      <td>35.753889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>34.328333</td>\n",
       "      <td>35.886111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>35.753889</td>\n",
       "      <td>36.299444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>35.886111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>36.299444</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  DayAvgPrice     Target\n",
       "916  2025-10-23    35.070833  35.094444\n",
       "917  2025-10-24    35.294445  34.881389\n",
       "918  2025-10-27    35.094444  35.007222\n",
       "919  2025-10-28    34.881389  34.865000\n",
       "920  2025-10-29    35.007222  34.751945\n",
       "921  2025-10-30    34.865000  34.606944\n",
       "922  2025-10-31    34.751945  33.886111\n",
       "923  2025-11-03    34.606944  34.891389\n",
       "924  2025-11-04    33.886111  35.163611\n",
       "925  2025-11-05    34.891389  34.328333\n",
       "926  2025-11-06    35.163611  35.753889\n",
       "927  2025-11-07    34.328333  35.886111\n",
       "928  2025-11-10    35.753889  36.299444\n",
       "929  2025-11-11    35.886111   0.000000\n",
       "930  2025-11-12    36.299444   0.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаём Target, пустые последние значения заменяются 0.\n",
    "H = config['data']['time_shift']     # time shift\n",
    "# df['Target'] = df['DayAvgPrice'].shift( - H).fillna(method='ffill')\n",
    "# df['Target'].iloc[-H:] = 0\n",
    "df['Target'] = df['DayAvgPrice'].shift( - H).fillna(0)\n",
    "df[['date', 'DayAvgPrice', 'Target']].tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c_month_real'] = df['DayAvgPrice'].copy()\n",
    "df['c_month_imag'] = df['IntradayStd'].copy()\n",
    "df['DayAvgPrice'] = df['c_week_real'].copy()\n",
    "df['IntradayStd'] = df['c_week_imag'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.446030 | val_loss=0.1710 | ema=0.1710 | lr=0.00050 | pred = 35.5744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:   0%|          | 0/10 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m df_temp \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     21\u001b[0m config[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m start_lr\n\u001b[0;32m---> 23\u001b[0m results \u001b[39m=\u001b[39m train_tscv_model_7(config, df_temp, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m DAP_ \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(\u001b[39mfloat\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mc_month_real\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[start_index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]), \u001b[39m4\u001b[39m)\n\u001b[1;32m     26\u001b[0m mu_DAP \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(\u001b[39mfloat\u001b[39m(results[\u001b[39m\"\u001b[39m\u001b[39mmu\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mc_month_real\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 349\u001b[0m, in \u001b[0;36mtrain_tscv_model_7\u001b[0;34m(config, df, verbose)\u001b[0m\n\u001b[1;32m    347\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, y) \u001b[39m/\u001b[39m accum_steps\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(train_loader) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m: train_loss_temp \u001b[39m=\u001b[39m loss\n\u001b[0;32m--> 349\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m accum_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m max_grad_norm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lr_list predict: START! \n",
    "\n",
    "start_preds_date = pd.to_datetime(config['data']['start_preds_date'], errors='coerce')\n",
    "stop_preds_date = pd.to_datetime(config['data']['stop_preds_date'], errors='coerce')\n",
    "interval_days = int((stop_preds_date - start_preds_date).days)\n",
    "start_index = df.index[df['date']==config['data']['start_preds_date']].tolist()[0]\n",
    "result_list = []\n",
    "\n",
    "cum_mae_pred = 0.0\n",
    "cum_mae_dap  = 0.0\n",
    "steps = 0\n",
    "\n",
    "start_lr_list = [0.003, 0.002, 0.001, 0.0009, 0.0008, 0.0007, 0.0006, 0.0005, 0.0004, 0.0003]\n",
    "results_acc = pd.read_csv('../data_archiv/DTG/res/DTG_res_fresh.csv')\n",
    "total_iters = len(start_lr_list)\n",
    "\n",
    "with tqdm(total=total_iters, desc=\"Rolling forecast\") as pbar:\n",
    "    for start_lr in start_lr_list:\n",
    "        df_temp = df.copy()\n",
    "\n",
    "        config['train']['lr'] = start_lr\n",
    "\n",
    "        results = train_tscv_model_7(config, df_temp, verbose=True)\n",
    "\n",
    "        DAP_ = round(float(df['c_month_real'].iloc[start_index + 1]), 4)\n",
    "        mu_DAP = round(float(results[\"mu\"]['c_month_real']), 4)\n",
    "        sigma_DAP = round(float(results[\"sigma\"]['c_month_real']), 4)\n",
    "        # DAP_ = DAP * sigma_DAP + mu_DAP\n",
    "\n",
    "        Target_ = round(float(df_temp['Target'].iloc[start_index + 1]), 4)\n",
    "        Pred = round(float(results[\"pred_series_per_fold\"][0]), 4)\n",
    "        preds_last_10_mean = round(pd.Series(results[\"preds_last_10\"]).mean(), 4)\n",
    "        preds_last_10_std = round(pd.Series(results[\"preds_last_10\"]).std(), 4)\n",
    "        print(results[\"preds_last_10\"], f\" | mean={preds_last_10_mean:.4f} | std={preds_last_10_std:.4f}\\n\")\n",
    "\n",
    "        mae_target_pred = results[\"fold_metrics\"][-1][\"mae_target_pred\"]\n",
    "        mae_target_dap = abs(DAP_ - Target_)\n",
    "        mae_target_pred_avg = abs(preds_last_10_mean - Target_)\n",
    "        # mae_target_dap = results[\"fold_metrics\"][-1][\"mae_target_dap\"]\n",
    "\n",
    "        result_list.append({\n",
    "            'date': config['data']['stop_preds_date'],\n",
    "            'lr': config['train']['lr'],\n",
    "            'last_DAP': DAP_,\n",
    "            'mu_DAP': round(float(results[\"mu\"]['c_month_real']), 4),\n",
    "            'Target': Target_,\n",
    "            'Predict': Pred,\n",
    "            'Pred_last_10_ep_mean': preds_last_10_mean,\n",
    "            'Pred_last_10_ep_std': preds_last_10_std,\n",
    "            'mae_target_pred': round(mae_target_pred, 4),\n",
    "            'mae_target_pred_avg': round(mae_target_pred_avg, 4),\n",
    "            'mae_target_dap': round(mae_target_dap, 4)\n",
    "        })\n",
    "\n",
    "        # обновляем кумулятивные средние\n",
    "        steps += 1\n",
    "        cum_mae_pred = ((cum_mae_pred * (steps - 1)) + mae_target_pred) / steps\n",
    "        cum_mae_dap  = ((cum_mae_dap  * (steps - 1)) + mae_target_dap)  / steps\n",
    "\n",
    "        # обновляем прогресс-бар: последние и средние MAE\n",
    "        pbar.set_postfix({\n",
    "            \"last_mae_pred\": f\"{mae_target_pred:.4f}\",\n",
    "            \"last_mae_dap\":  f\"{mae_target_dap:.4f}\",\n",
    "            \"avg_mae_pred\":  f\"{cum_mae_pred:.4f}\",\n",
    "            \"avg_mae_dap\":   f\"{cum_mae_dap:.4f}\"\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "# собираем итоговый датафрейм один раз\n",
    "result_tab = pd.DataFrame(result_list)\n",
    "make_results_acc(results_acc, result_tab, config)\n",
    "results_acc.to_csv('../data_archiv/DTG/res/DTG_res_fresh.csv', index=False)\n",
    "display(results_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lr</th>\n",
       "      <th>last_DAP</th>\n",
       "      <th>mu_DAP</th>\n",
       "      <th>Target</th>\n",
       "      <th>Predict</th>\n",
       "      <th>Pred_last_10_ep_mean</th>\n",
       "      <th>Pred_last_10_ep_std</th>\n",
       "      <th>mae_target_pred</th>\n",
       "      <th>mae_target_pred_avg</th>\n",
       "      <th>mae_target_dap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.4423</td>\n",
       "      <td>36.4319</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>36.4423</td>\n",
       "      <td>36.4319</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3372</td>\n",
       "      <td>36.3303</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>36.3372</td>\n",
       "      <td>36.3303</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3900</td>\n",
       "      <td>36.3830</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>36.3900</td>\n",
       "      <td>36.3830</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8480</td>\n",
       "      <td>36.8548</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>36.8480</td>\n",
       "      <td>36.8548</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.7379</td>\n",
       "      <td>36.7580</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>36.7379</td>\n",
       "      <td>36.7580</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.6947</td>\n",
       "      <td>36.6907</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>36.6947</td>\n",
       "      <td>36.6907</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.6517</td>\n",
       "      <td>36.6623</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>36.6517</td>\n",
       "      <td>36.6623</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.9879</td>\n",
       "      <td>37.0027</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>36.9879</td>\n",
       "      <td>37.0027</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.9453</td>\n",
       "      <td>36.9593</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>36.9453</td>\n",
       "      <td>36.9593</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3090</td>\n",
       "      <td>36.3512</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>36.3090</td>\n",
       "      <td>36.3512</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      lr  last_DAP   mu_DAP  Target  Predict  \\\n",
       "0  2025-11-12  0.0030   36.2994  33.1234     0.0  36.4423   \n",
       "1  2025-11-12  0.0020   36.2994  33.1234     0.0  36.3372   \n",
       "2  2025-11-12  0.0010   36.2994  33.1234     0.0  36.3900   \n",
       "3  2025-11-12  0.0009   36.2994  33.1234     0.0  36.8480   \n",
       "4  2025-11-12  0.0008   36.2994  33.1234     0.0  36.7379   \n",
       "5  2025-11-12  0.0007   36.2994  33.1234     0.0  36.6947   \n",
       "6  2025-11-12  0.0006   36.2994  33.1234     0.0  36.6517   \n",
       "7  2025-11-12  0.0005   36.2994  33.1234     0.0  36.9879   \n",
       "8  2025-11-12  0.0004   36.2994  33.1234     0.0  36.9453   \n",
       "9  2025-11-12  0.0003   36.2994  33.1234     0.0  36.3090   \n",
       "\n",
       "   Pred_last_10_ep_mean  Pred_last_10_ep_std  mae_target_pred  \\\n",
       "0               36.4319               0.0305          36.4423   \n",
       "1               36.3303               0.0271          36.3372   \n",
       "2               36.3830               0.0450          36.3900   \n",
       "3               36.8548               0.0486          36.8480   \n",
       "4               36.7580               0.0504          36.7379   \n",
       "5               36.6907               0.0256          36.6947   \n",
       "6               36.6623               0.0393          36.6517   \n",
       "7               37.0027               0.0193          36.9879   \n",
       "8               36.9593               0.0325          36.9453   \n",
       "9               36.3512               0.0510          36.3090   \n",
       "\n",
       "   mae_target_pred_avg  mae_target_dap  \n",
       "0              36.4319         36.2994  \n",
       "1              36.3303         36.2994  \n",
       "2              36.3830         36.2994  \n",
       "3              36.8548         36.2994  \n",
       "4              36.7580         36.2994  \n",
       "5              36.6907         36.2994  \n",
       "6              36.6623         36.2994  \n",
       "7              37.0027         36.2994  \n",
       "8              36.9593         36.2994  \n",
       "9              36.3512         36.2994  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.6424 0.255 0.0369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.636399999999995"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(round(result_tab['Pred_last_10_ep_mean'].mean(), 4), round(result_tab['Pred_last_10_ep_mean'].std(), 4), \\\n",
    "      round(result_tab['Pred_last_10_ep_std'].mean(), 4))\n",
    "my_list = result_tab['Pred_last_10_ep_mean'].tolist()\n",
    "my_list.remove(max(my_list))\n",
    "my_list.remove(min(my_list))\n",
    "sum(my_list) / len(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_tab['Predict_mean_10_last_epochs'] = result_tab['Pred_last_10_ep_mean'].copy()\n",
    "# result_tab['Predict_std_10_last_epochs'] = result_tab['Pred_last_10_ep_std'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_tab[['lr', 'Predict_mean_10_last_epochs', 'Predict_std_10_last_epochs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time_shift</th>\n",
       "      <th>window_size</th>\n",
       "      <th>lr_lambda</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dropout</th>\n",
       "      <th>n_res_blocks</th>\n",
       "      <th>PL10_mean_mean</th>\n",
       "      <th>PL10_mean_std</th>\n",
       "      <th>PL10_std_mean</th>\n",
       "      <th>PL10_std_std</th>\n",
       "      <th>0.0030_mean</th>\n",
       "      <th>0.0020_mean</th>\n",
       "      <th>0.0010_mean</th>\n",
       "      <th>0.0009_mean</th>\n",
       "      <th>0.0008_mean</th>\n",
       "      <th>0.0007_mean</th>\n",
       "      <th>0.0006_mean</th>\n",
       "      <th>0.0005_mean</th>\n",
       "      <th>0.0004_mean</th>\n",
       "      <th>0.0003_mean</th>\n",
       "      <th>0.0030_std</th>\n",
       "      <th>0.0020_std</th>\n",
       "      <th>0.0010_std</th>\n",
       "      <th>0.0009_std</th>\n",
       "      <th>0.0008_std</th>\n",
       "      <th>0.0007_std</th>\n",
       "      <th>0.0006_std</th>\n",
       "      <th>0.0005_std</th>\n",
       "      <th>0.0004_std</th>\n",
       "      <th>0.0003_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>44</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.04296</td>\n",
       "      <td>0.296592</td>\n",
       "      <td>0.03677</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>33.8967</td>\n",
       "      <td>33.7666</td>\n",
       "      <td>34.1048</td>\n",
       "      <td>34.4722</td>\n",
       "      <td>34.3281</td>\n",
       "      <td>34.1199</td>\n",
       "      <td>34.1343</td>\n",
       "      <td>33.9614</td>\n",
       "      <td>34.2144</td>\n",
       "      <td>33.4312</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>52</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.08798</td>\n",
       "      <td>0.101572</td>\n",
       "      <td>0.02693</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>33.9794</td>\n",
       "      <td>34.2996</td>\n",
       "      <td>34.1232</td>\n",
       "      <td>34.1260</td>\n",
       "      <td>34.0834</td>\n",
       "      <td>34.0315</td>\n",
       "      <td>34.0361</td>\n",
       "      <td>34.1098</td>\n",
       "      <td>34.1540</td>\n",
       "      <td>33.9368</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.52822</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>34.8397</td>\n",
       "      <td>35.0390</td>\n",
       "      <td>35.1257</td>\n",
       "      <td>34.1310</td>\n",
       "      <td>34.6296</td>\n",
       "      <td>34.7337</td>\n",
       "      <td>34.0210</td>\n",
       "      <td>34.6563</td>\n",
       "      <td>34.0728</td>\n",
       "      <td>34.0334</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>0.0664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>33.41389</td>\n",
       "      <td>0.592093</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.085554</td>\n",
       "      <td>33.9495</td>\n",
       "      <td>34.1444</td>\n",
       "      <td>33.5586</td>\n",
       "      <td>33.4852</td>\n",
       "      <td>32.9203</td>\n",
       "      <td>34.1612</td>\n",
       "      <td>33.1922</td>\n",
       "      <td>33.3552</td>\n",
       "      <td>33.1304</td>\n",
       "      <td>32.2419</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>34.01628</td>\n",
       "      <td>0.334351</td>\n",
       "      <td>0.07227</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>34.3612</td>\n",
       "      <td>34.4437</td>\n",
       "      <td>34.0495</td>\n",
       "      <td>33.9382</td>\n",
       "      <td>33.3032</td>\n",
       "      <td>34.1230</td>\n",
       "      <td>33.9392</td>\n",
       "      <td>33.9461</td>\n",
       "      <td>33.7411</td>\n",
       "      <td>34.3176</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.09213</td>\n",
       "      <td>0.271552</td>\n",
       "      <td>0.04879</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>34.5063</td>\n",
       "      <td>34.4195</td>\n",
       "      <td>34.0148</td>\n",
       "      <td>33.8809</td>\n",
       "      <td>33.9621</td>\n",
       "      <td>33.8735</td>\n",
       "      <td>33.8598</td>\n",
       "      <td>33.7669</td>\n",
       "      <td>34.3787</td>\n",
       "      <td>34.2588</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.03229</td>\n",
       "      <td>0.147213</td>\n",
       "      <td>0.04455</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>33.8739</td>\n",
       "      <td>33.8561</td>\n",
       "      <td>34.0091</td>\n",
       "      <td>34.1713</td>\n",
       "      <td>34.1532</td>\n",
       "      <td>33.8691</td>\n",
       "      <td>34.1332</td>\n",
       "      <td>33.9426</td>\n",
       "      <td>34.0381</td>\n",
       "      <td>34.2763</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.20160</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.8877</td>\n",
       "      <td>33.8170</td>\n",
       "      <td>34.3975</td>\n",
       "      <td>34.1070</td>\n",
       "      <td>34.3238</td>\n",
       "      <td>34.2922</td>\n",
       "      <td>34.3360</td>\n",
       "      <td>34.2902</td>\n",
       "      <td>34.3743</td>\n",
       "      <td>34.1906</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>33.77220</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.09480</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>33.7382</td>\n",
       "      <td>33.8192</td>\n",
       "      <td>33.5882</td>\n",
       "      <td>33.7218</td>\n",
       "      <td>33.4791</td>\n",
       "      <td>33.9450</td>\n",
       "      <td>33.9421</td>\n",
       "      <td>33.8468</td>\n",
       "      <td>33.8459</td>\n",
       "      <td>33.7960</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>60</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.16810</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.05370</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.9715</td>\n",
       "      <td>34.0325</td>\n",
       "      <td>34.5261</td>\n",
       "      <td>34.3204</td>\n",
       "      <td>34.4775</td>\n",
       "      <td>34.1714</td>\n",
       "      <td>34.3446</td>\n",
       "      <td>33.7550</td>\n",
       "      <td>33.9080</td>\n",
       "      <td>34.1736</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.02170</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.02940</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>33.8616</td>\n",
       "      <td>33.6905</td>\n",
       "      <td>34.1431</td>\n",
       "      <td>34.0800</td>\n",
       "      <td>34.0269</td>\n",
       "      <td>34.1495</td>\n",
       "      <td>34.0671</td>\n",
       "      <td>34.1982</td>\n",
       "      <td>34.0890</td>\n",
       "      <td>33.9112</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>33.95390</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>33.6247</td>\n",
       "      <td>34.0442</td>\n",
       "      <td>34.1751</td>\n",
       "      <td>34.0652</td>\n",
       "      <td>33.6538</td>\n",
       "      <td>34.0870</td>\n",
       "      <td>33.9074</td>\n",
       "      <td>34.0052</td>\n",
       "      <td>33.8273</td>\n",
       "      <td>34.1494</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.40610</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.06490</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>34.8906</td>\n",
       "      <td>35.8110</td>\n",
       "      <td>34.8277</td>\n",
       "      <td>35.1031</td>\n",
       "      <td>35.3820</td>\n",
       "      <td>35.3879</td>\n",
       "      <td>35.0688</td>\n",
       "      <td>35.8009</td>\n",
       "      <td>35.7868</td>\n",
       "      <td>36.0026</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.0748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.42660</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.06220</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>36.2859</td>\n",
       "      <td>35.4809</td>\n",
       "      <td>34.7119</td>\n",
       "      <td>35.2623</td>\n",
       "      <td>35.0421</td>\n",
       "      <td>35.1546</td>\n",
       "      <td>35.3421</td>\n",
       "      <td>34.9806</td>\n",
       "      <td>36.9482</td>\n",
       "      <td>35.0570</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.42250</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.02160</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>35.6900</td>\n",
       "      <td>35.4300</td>\n",
       "      <td>35.7616</td>\n",
       "      <td>35.6318</td>\n",
       "      <td>35.3681</td>\n",
       "      <td>35.4666</td>\n",
       "      <td>35.5247</td>\n",
       "      <td>35.2215</td>\n",
       "      <td>35.0339</td>\n",
       "      <td>35.0971</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.00700</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.01540</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>35.1352</td>\n",
       "      <td>35.0797</td>\n",
       "      <td>34.9388</td>\n",
       "      <td>35.0725</td>\n",
       "      <td>35.2059</td>\n",
       "      <td>34.7787</td>\n",
       "      <td>34.8483</td>\n",
       "      <td>35.0720</td>\n",
       "      <td>34.9944</td>\n",
       "      <td>34.9447</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.74070</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.02400</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>35.7633</td>\n",
       "      <td>35.4769</td>\n",
       "      <td>35.6850</td>\n",
       "      <td>35.3690</td>\n",
       "      <td>35.8015</td>\n",
       "      <td>36.3752</td>\n",
       "      <td>35.0792</td>\n",
       "      <td>36.0552</td>\n",
       "      <td>35.7728</td>\n",
       "      <td>36.0289</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.10310</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>35.9709</td>\n",
       "      <td>36.0299</td>\n",
       "      <td>35.8589</td>\n",
       "      <td>36.1879</td>\n",
       "      <td>35.8114</td>\n",
       "      <td>36.1512</td>\n",
       "      <td>36.2648</td>\n",
       "      <td>36.2854</td>\n",
       "      <td>36.2138</td>\n",
       "      <td>36.2563</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.94890</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.02310</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>35.8952</td>\n",
       "      <td>36.0750</td>\n",
       "      <td>35.8183</td>\n",
       "      <td>36.1429</td>\n",
       "      <td>36.3012</td>\n",
       "      <td>36.1458</td>\n",
       "      <td>36.0727</td>\n",
       "      <td>35.5236</td>\n",
       "      <td>36.0581</td>\n",
       "      <td>35.4566</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.85040</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.03040</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>35.9462</td>\n",
       "      <td>36.2186</td>\n",
       "      <td>36.3577</td>\n",
       "      <td>36.4732</td>\n",
       "      <td>35.6464</td>\n",
       "      <td>35.5069</td>\n",
       "      <td>35.6691</td>\n",
       "      <td>35.3255</td>\n",
       "      <td>34.9703</td>\n",
       "      <td>36.3903</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.06580</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>37.4658</td>\n",
       "      <td>35.7302</td>\n",
       "      <td>35.6559</td>\n",
       "      <td>35.9850</td>\n",
       "      <td>36.3132</td>\n",
       "      <td>35.6608</td>\n",
       "      <td>36.2174</td>\n",
       "      <td>36.0467</td>\n",
       "      <td>36.2851</td>\n",
       "      <td>35.2974</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.1231</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.1557</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.83560</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>36.5582</td>\n",
       "      <td>36.5673</td>\n",
       "      <td>36.6765</td>\n",
       "      <td>36.7727</td>\n",
       "      <td>36.6858</td>\n",
       "      <td>36.9320</td>\n",
       "      <td>36.8448</td>\n",
       "      <td>37.0639</td>\n",
       "      <td>37.2044</td>\n",
       "      <td>37.0501</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.50060</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.01570</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>36.5280</td>\n",
       "      <td>36.2985</td>\n",
       "      <td>36.4604</td>\n",
       "      <td>36.2603</td>\n",
       "      <td>36.5788</td>\n",
       "      <td>36.4379</td>\n",
       "      <td>36.7048</td>\n",
       "      <td>36.6045</td>\n",
       "      <td>36.2259</td>\n",
       "      <td>36.9071</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  time_shift  window_size lr_lambda  epochs  dropout  \\\n",
       "0   2025-11-09           3           92       cos      44      0.3   \n",
       "1   2025-11-09           2           20       cos      52      0.3   \n",
       "2   2025-11-09           4           61       cos      36      0.3   \n",
       "3   2025-11-09           5           92       cos      36      0.3   \n",
       "4   2025-11-09           2           61      _cos      40      0.3   \n",
       "5   2025-11-09           1           61      _cos      40      0.4   \n",
       "6   2025-11-07           1           20       cos      48      0.4   \n",
       "7   2025-11-07           2           34       cos      48      0.4   \n",
       "8   2025-11-07           3           34       cos      48      0.4   \n",
       "9   2025-11-07           2           34       cos      60      0.3   \n",
       "10  2025-11-07           1           34       cos      64      0.3   \n",
       "11  2025-11-07           3           34       cos      64      0.4   \n",
       "12  2025-11-10           1           50      _cos      64      0.4   \n",
       "13  2025-11-10           1           50      _cos      64      0.4   \n",
       "14  2025-11-10           1          100       cos      64      0.4   \n",
       "15  2025-11-10           2          100       cos      64      0.4   \n",
       "16  2025-11-11           1           50       cos      64      0.4   \n",
       "17  2025-11-11           1           21       cos      64      0.4   \n",
       "18  2025-11-11           1          100       cos      64      0.4   \n",
       "19  2025-11-11           3           50       cos      64      0.4   \n",
       "20  2025-11-11           3           50      _cos      64      0.4   \n",
       "21  2025-11-12           1           21       cos      64      0.4   \n",
       "22  2025-11-12           1          100       cos      64      0.4   \n",
       "\n",
       "    n_res_blocks  PL10_mean_mean  PL10_mean_std  PL10_std_mean  PL10_std_std  \\\n",
       "0              3        34.04296       0.296592        0.03677      0.032337   \n",
       "1              3        34.08798       0.101572        0.02693      0.010317   \n",
       "2              1        34.52822       0.428319        0.07696      0.025820   \n",
       "3              3        33.41389       0.592093        0.13273      0.085554   \n",
       "4              2        34.01628       0.334351        0.07227      0.023782   \n",
       "5              2        34.09213       0.271552        0.04879      0.013215   \n",
       "6              2        34.03229       0.147213        0.04455      0.014940   \n",
       "7              2        34.20160       0.203500        0.03720      0.024600   \n",
       "8              2        33.77220       0.147600        0.09480      0.032400   \n",
       "9              3        34.16810       0.252500        0.05370      0.024600   \n",
       "10             3        34.02170       0.156300        0.02940      0.015400   \n",
       "11             3        33.95390       0.195700        0.03610      0.019900   \n",
       "12             3        35.40610       0.425200        0.06490      0.028400   \n",
       "13             3        35.42660       0.679800        0.06220      0.028500   \n",
       "14             2        35.42250       0.245600        0.02160      0.009300   \n",
       "15             2        35.00700       0.131600        0.01540      0.007600   \n",
       "16             2        35.74070       0.370900        0.02400      0.012900   \n",
       "17             2        36.10310       0.173900        0.03080      0.007500   \n",
       "18             2        35.94890       0.276300        0.02310      0.008400   \n",
       "19             2        35.85040       0.508200        0.03040      0.014800   \n",
       "20             2        36.06580       0.590100        0.10110      0.035500   \n",
       "21             2        36.83560       0.222200        0.01770      0.011500   \n",
       "22             2        36.50060       0.211900        0.01570      0.008800   \n",
       "\n",
       "    0.0030_mean  0.0020_mean  0.0010_mean  0.0009_mean  0.0008_mean  \\\n",
       "0       33.8967      33.7666      34.1048      34.4722      34.3281   \n",
       "1       33.9794      34.2996      34.1232      34.1260      34.0834   \n",
       "2       34.8397      35.0390      35.1257      34.1310      34.6296   \n",
       "3       33.9495      34.1444      33.5586      33.4852      32.9203   \n",
       "4       34.3612      34.4437      34.0495      33.9382      33.3032   \n",
       "5       34.5063      34.4195      34.0148      33.8809      33.9621   \n",
       "6       33.8739      33.8561      34.0091      34.1713      34.1532   \n",
       "7       33.8877      33.8170      34.3975      34.1070      34.3238   \n",
       "8       33.7382      33.8192      33.5882      33.7218      33.4791   \n",
       "9       33.9715      34.0325      34.5261      34.3204      34.4775   \n",
       "10      33.8616      33.6905      34.1431      34.0800      34.0269   \n",
       "11      33.6247      34.0442      34.1751      34.0652      33.6538   \n",
       "12      34.8906      35.8110      34.8277      35.1031      35.3820   \n",
       "13      36.2859      35.4809      34.7119      35.2623      35.0421   \n",
       "14      35.6900      35.4300      35.7616      35.6318      35.3681   \n",
       "15      35.1352      35.0797      34.9388      35.0725      35.2059   \n",
       "16      35.7633      35.4769      35.6850      35.3690      35.8015   \n",
       "17      35.9709      36.0299      35.8589      36.1879      35.8114   \n",
       "18      35.8952      36.0750      35.8183      36.1429      36.3012   \n",
       "19      35.9462      36.2186      36.3577      36.4732      35.6464   \n",
       "20      37.4658      35.7302      35.6559      35.9850      36.3132   \n",
       "21      36.5582      36.5673      36.6765      36.7727      36.6858   \n",
       "22      36.5280      36.2985      36.4604      36.2603      36.5788   \n",
       "\n",
       "    0.0007_mean  0.0006_mean  0.0005_mean  0.0004_mean  0.0003_mean  \\\n",
       "0       34.1199      34.1343      33.9614      34.2144      33.4312   \n",
       "1       34.0315      34.0361      34.1098      34.1540      33.9368   \n",
       "2       34.7337      34.0210      34.6563      34.0728      34.0334   \n",
       "3       34.1612      33.1922      33.3552      33.1304      32.2419   \n",
       "4       34.1230      33.9392      33.9461      33.7411      34.3176   \n",
       "5       33.8735      33.8598      33.7669      34.3787      34.2588   \n",
       "6       33.8691      34.1332      33.9426      34.0381      34.2763   \n",
       "7       34.2922      34.3360      34.2902      34.3743      34.1906   \n",
       "8       33.9450      33.9421      33.8468      33.8459      33.7960   \n",
       "9       34.1714      34.3446      33.7550      33.9080      34.1736   \n",
       "10      34.1495      34.0671      34.1982      34.0890      33.9112   \n",
       "11      34.0870      33.9074      34.0052      33.8273      34.1494   \n",
       "12      35.3879      35.0688      35.8009      35.7868      36.0026   \n",
       "13      35.1546      35.3421      34.9806      36.9482      35.0570   \n",
       "14      35.4666      35.5247      35.2215      35.0339      35.0971   \n",
       "15      34.7787      34.8483      35.0720      34.9944      34.9447   \n",
       "16      36.3752      35.0792      36.0552      35.7728      36.0289   \n",
       "17      36.1512      36.2648      36.2854      36.2138      36.2563   \n",
       "18      36.1458      36.0727      35.5236      36.0581      35.4566   \n",
       "19      35.5069      35.6691      35.3255      34.9703      36.3903   \n",
       "20      35.6608      36.2174      36.0467      36.2851      35.2974   \n",
       "21      36.9320      36.8448      37.0639      37.2044      37.0501   \n",
       "22      36.4379      36.7048      36.6045      36.2259      36.9071   \n",
       "\n",
       "    0.0030_std  0.0020_std  0.0010_std  0.0009_std  0.0008_std  0.0007_std  \\\n",
       "0       0.0230      0.0407      0.0196      0.0111      0.0304      0.0233   \n",
       "1       0.0222      0.0314      0.0139      0.0264      0.0359      0.0211   \n",
       "2       0.1019      0.0576      0.1342      0.0958      0.0539      0.0594   \n",
       "3       0.0545      0.0252      0.0810      0.0973      0.1441      0.1443   \n",
       "4       0.0815      0.0663      0.0519      0.0770      0.1233      0.0834   \n",
       "5       0.0596      0.0394      0.0345      0.0579      0.0617      0.0406   \n",
       "6       0.0277      0.0379      0.0543      0.0614      0.0696      0.0501   \n",
       "7       0.0247      0.0238      0.0367      0.0367      0.0163      0.1037   \n",
       "8       0.1201      0.0930      0.0927      0.0939      0.0571      0.1116   \n",
       "9       0.0252      0.0654      0.0168      0.0602      0.0510      0.0906   \n",
       "10      0.0285      0.0686      0.0262      0.0238      0.0237      0.0255   \n",
       "11      0.0803      0.0378      0.0197      0.0215      0.0566      0.0425   \n",
       "12      0.1141      0.0480      0.0449      0.0483      0.0288      0.0532   \n",
       "13      0.0740      0.1258      0.0430      0.0395      0.0343      0.0541   \n",
       "14      0.0226      0.0113      0.0224      0.0411      0.0326      0.0214   \n",
       "15      0.0196      0.0094      0.0108      0.0224      0.0152      0.0295   \n",
       "16      0.0557      0.0277      0.0070      0.0238      0.0135      0.0244   \n",
       "17      0.0293      0.0425      0.0360      0.0318      0.0261      0.0377   \n",
       "18      0.0132      0.0238      0.0274      0.0188      0.0418      0.0263   \n",
       "19      0.0168      0.0146      0.0139      0.0141      0.0369      0.0426   \n",
       "20      0.1523      0.1100      0.1231      0.0650      0.1557      0.0533   \n",
       "21      0.0242      0.0138      0.0453      0.0081      0.0230      0.0085   \n",
       "22      0.0220      0.0328      0.0128      0.0114      0.0093      0.0247   \n",
       "\n",
       "    0.0006_std  0.0005_std  0.0004_std  0.0003_std  \n",
       "0       0.0211      0.0476      0.0271      0.1238  \n",
       "1       0.0506      0.0239      0.0195      0.0244  \n",
       "2       0.0603      0.0749      0.0652      0.0664  \n",
       "3       0.1279      0.3019      0.2520      0.0991  \n",
       "4       0.0848      0.0614      0.0567      0.0364  \n",
       "5       0.0459      0.0678      0.0274      0.0531  \n",
       "6       0.0495      0.0227      0.0357      0.0366  \n",
       "7       0.0428      0.0314      0.0265      0.0291  \n",
       "8       0.1641      0.0793      0.0508      0.0850  \n",
       "9       0.0771      0.0743      0.0499      0.0266  \n",
       "10      0.0406      0.0228      0.0130      0.0208  \n",
       "11      0.0172      0.0261      0.0376      0.0219  \n",
       "12      0.0634      0.0602      0.1130      0.0748  \n",
       "13      0.0580      0.0909      0.0645      0.0382  \n",
       "14      0.0177      0.0183      0.0104      0.0185  \n",
       "15      0.0069      0.0128      0.0066      0.0213  \n",
       "16      0.0185      0.0214      0.0284      0.0194  \n",
       "17      0.0252      0.0345      0.0285      0.0162  \n",
       "18      0.0274      0.0184      0.0202      0.0138  \n",
       "19      0.0397      0.0289      0.0412      0.0551  \n",
       "20      0.0653      0.0926      0.0876      0.1060  \n",
       "21      0.0208      0.0079      0.0152      0.0101  \n",
       "22      0.0030      0.0176      0.0089      0.0148  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
