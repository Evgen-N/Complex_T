{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas_ta as ta\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import numpy as np\n",
    "from data_loader_c1 import TwoStreamDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from data_loader_c1 import load_dataloader\n",
    "from model_real_1 import RealCNNLSTM\n",
    "from model_complex_1 import ComplexCNNLSTM\n",
    "from model_fusion_1 import FusionGated\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results_acc(results_acc, result_tab, config):\n",
    "    \"\"\"Запись результатов\"\"\"\n",
    "    results_acc.loc[len(results_acc)] = [result_tab['date'][0], config['data']['time_shift'], config['data']['window_size'], \\\n",
    "    config['data']['lr_lambda'], config['train']['epochs'], config['model']['dropout'], config['model']['n_res_blocks'], \\\n",
    "    round(result_tab['Pred_last_10_ep_mean'].mean(), 4), round(result_tab['Pred_last_10_ep_mean'].std(), 4), \\\n",
    "    round(result_tab['Pred_last_10_ep_std'].mean(), 4), round(result_tab['Pred_last_10_ep_std'].std(), 4)] + \\\n",
    "    result_tab['Pred_last_10_ep_mean'].tolist() + result_tab['Pred_last_10_ep_std'].tolist()\n",
    "    return results_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_time_series_split_4(df_len, time_shift=1, interval_days=1, main_splits_number=2, initial_train_frac=0.90):\n",
    "    splits = []\n",
    "    df_len_sh = df_len - interval_days - time_shift + 1\n",
    "    initial_train_size = int(df_len_sh * initial_train_frac)\n",
    "\n",
    "    initial_train_size += (df_len_sh - initial_train_size) % main_splits_number\n",
    "    val_size = int((df_len_sh - initial_train_size) / main_splits_number)\n",
    "    assert val_size not in range(1)\n",
    "    \n",
    "    train_end = initial_train_size - time_shift + 1\n",
    "    val_start = initial_train_size\n",
    "    val_end = val_start + val_size\n",
    "\n",
    "    while val_end <= df_len_sh:\n",
    "        \n",
    "        train_idx = list(range(0, train_end))\n",
    "        val_idx = list(range(val_start, val_end))\n",
    "        splits.append((train_idx, val_idx))\n",
    "        train_end = val_end\n",
    "        val_start = train_end\n",
    "        val_end = val_start + val_size\n",
    "\n",
    "    # print(splits)\n",
    "    one_day_start = splits[-1][1][-1]\n",
    "\n",
    "    for i in range(interval_days):\n",
    "        train_idx = list(range(0, one_day_start + i + 1))\n",
    "        val_idx = list(range(one_day_start + i + time_shift, one_day_start + i + time_shift + 1))\n",
    "        splits.append((train_idx, val_idx))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = custom_time_series_split_4(55, time_shift=2, interval_days=1, main_splits_number=1, initial_train_frac=0.90)\n",
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Вариант с двумя фолдами!!!\n",
    "\n",
    "# ---------- EarlyStopping ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=1e-2, mode=\"min\", restore_best=True, rel_delta=True):\n",
    "        assert mode in (\"min\", \"max\")\n",
    "        self.patience, self.min_delta, self.mode = patience, float(min_delta), mode\n",
    "        self.restore_best, self.rel_delta = restore_best, rel_delta\n",
    "        self.best, self.wait, self.best_states = None, 0, {}\n",
    "\n",
    "    def _improved(self, cur, best):\n",
    "        if self.mode == \"min\":\n",
    "            return cur < (best*(1-self.min_delta) if self.rel_delta else best - self.min_delta)\n",
    "        else:\n",
    "            return cur > (best*(1+self.min_delta) if self.rel_delta else best + self.min_delta)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, metric: float, models: Dict[str, torch.nn.Module]) -> bool:\n",
    "        if self.best is None or self._improved(metric, self.best):\n",
    "            self.best, self.wait = metric, 0\n",
    "            self.best_states = {k:{n:v.detach().cpu().clone() for n,v in m.state_dict().items()}\n",
    "                                for k,m in models.items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, models: Dict[str, torch.nn.Module]) -> None:\n",
    "        if not self.best_states: return\n",
    "        for k,m in models.items():\n",
    "            if k in self.best_states:\n",
    "                m.load_state_dict(self.best_states[k])\n",
    "\n",
    "\n",
    "# ---------- вспомогательные утилиты ----------\n",
    "def _set_seed(seed=42):\n",
    "    import os, random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _reset_weights(m: nn.Module):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "        m.reset_parameters()\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, p in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(p.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(p.data)\n",
    "                h = p.shape[0]//4\n",
    "                p.data[h:2*h] = 1.0  # forget-bias\n",
    "\n",
    "\n",
    "# ---------- основная функция ----------\n",
    "def train_tscv_model_7(config: dict, df: pd.DataFrame, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Обучение с кастомным TSCV, EMA-сглаживанием, EarlyStopping, LR-Plateau, grad-accum, clip-grad.\n",
    "    Возвращает:\n",
    "        results: dict с метриками по фолдам и сериями предсказаний\n",
    "    \"\"\"\n",
    "    _set_seed(config.get(\"seed\", 42))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- даты и срез df ---\n",
    "    start_preds_date = pd.to_datetime(config['data']['start_preds_date'], errors='coerce')\n",
    "    stop_preds_date  = pd.to_datetime(config['data']['stop_preds_date'],  errors='coerce')\n",
    "    time_shift       = int(config['data']['time_shift'])\n",
    "    window_size      = int(config['data']['window_size'])\n",
    "    lr_lambda_type = config['data']['lr_lambda']\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df[pd.to_datetime(df['date']) <= stop_preds_date].reset_index(drop=True)\n",
    "\n",
    "    interval_days = int((stop_preds_date - start_preds_date).days)\n",
    "    if verbose:\n",
    "        print(\"Dates:\", start_preds_date, \"→\", stop_preds_date,\n",
    "              \"| interval_days:\", interval_days, \"| time_shift:\", time_shift)\n",
    "\n",
    "    # --- сплиты ---\n",
    "    # splits = custom_time_series_split_3(\n",
    "    #     len(df),\n",
    "    #     time_shift=time_shift,\n",
    "    #     interval_days=interval_days,\n",
    "    #     main_splits_number=0,\n",
    "    #     initial_train_frac=0.99\n",
    "    # )\n",
    "    # splits = custom_time_series_split_2(len(df), time_shift=time_shift, interval_days=interval_days, main_splits_number=2, initial_train_frac=0.90)\n",
    "    splits = custom_time_series_split_4(len(df), time_shift=time_shift, interval_days=1, main_splits_number=1, initial_train_frac=0.90)\n",
    "    if verbose: print('num of splits =', len(splits))\n",
    "\n",
    "    # --- bootstrap-loader (определяем F и C до инициализации моделей) ---\n",
    "    first_train_idx, _ = splits[0]\n",
    "    bootstrap_loader = load_dataloader(\n",
    "        df.iloc[first_train_idx], window_size=window_size,\n",
    "        batch_size=config['train']['batch_size'],\n",
    "        shuffle=False, drop_last=False\n",
    "    )\n",
    "    first_batch = next(iter(bootstrap_loader))\n",
    "    F = first_batch['real_feats'].shape[-1]\n",
    "    C = (first_batch['complex_time_real'].shape[-1]\n",
    "         if ('complex_time_real' in first_batch and 'complex_time_imag' in first_batch) else 1)\n",
    "\n",
    "    # --- модели ---\n",
    "    real_net = RealCNNLSTM(\n",
    "        num_real_features=F,\n",
    "        hidden_dim=config['model']['real_hidden_dim'],\n",
    "        lstm_layers=config['model'].get('real_lstm_layers', 2),\n",
    "        dropout=config['model'].get('dropout', 0.3),\n",
    "        kernel_size=3,\n",
    "        bidirectional=config['model'].get('bidirectional', True),\n",
    "        take=config['model'].get('take', \"last_timestep\"),\n",
    "        proj_out=True\n",
    "    ).to(device)\n",
    "\n",
    "    complex_net = ComplexCNNLSTM(\n",
    "        in_channels=C,\n",
    "        hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        num_layers=config['model']['n_res_blocks'],\n",
    "        dropout=config['model']['dropout'],\n",
    "        kernel_size=3,\n",
    "        proj_out=False\n",
    "    ).to(device)\n",
    "\n",
    "    fusion_net = FusionGated(\n",
    "        real_hidden_dim=config['model']['real_hidden_dim'],\n",
    "        complex_hidden_dim=config['model']['complex_hidden_dim'],\n",
    "        output_dim=1, hidden=128, dropout=0.2, use_softmax=True\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # --- ✅ Инициализация bias последнего линейного слоя ---\n",
    "    # with torch.no_grad():\n",
    "    #     for mod in fusion_net.modules():\n",
    "    #         if isinstance(mod, torch.nn.Linear) and mod.out_features == 1:\n",
    "    #             mod.bias.fill_(0.0)   # так как таргет уже нормирован (среднее ~ 0)\n",
    "\n",
    "    # --- оптимизатор/лоссы/планировщик/ES ---\n",
    "    optimizer = optim.Adam(\n",
    "        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "        lr=config['train']['lr'], weight_decay=1e-4\n",
    "    )\n",
    "    criterion = nn.MSELoss() if config['train'].get('loss', 'mse') == 'mse' else nn.SmoothL1Loss(beta=1.0)\n",
    "\n",
    "    epochs = int(config['train']['epochs'])\n",
    "    # warmup: 20% эпох, но не менее 1 и не более epochs-1\n",
    "    warmup_epochs = max(1, min(epochs - 1, int(0.2 * epochs)))\n",
    "    min_factor = 1e-3\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t))\n",
    "    else: \n",
    "        def lr_lambda(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 1.\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5\n",
    "            else:\n",
    "                return 0.1\n",
    "\n",
    "    if lr_lambda_type == 'cos':\n",
    "        def lr_lambda_fold2(e):\n",
    "            if e < warmup_epochs:\n",
    "                return (e+1)/warmup_epochs * 0.2\n",
    "            # cosine from 1 to ~0\n",
    "            t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "            return 0.5*(1 + math.cos(math.pi * t)) * 0.2\n",
    "    else: \n",
    "        def lr_lambda_fold2(epoch: int):\n",
    "            if epoch < epochs // 3:\n",
    "                return 0.4\n",
    "            elif epochs // 3 <= epoch < (epochs // 3) * 2:\n",
    "                return 0.5 * 0.4\n",
    "            else:\n",
    "                return 0.1 * 0.4\n",
    "            \n",
    "    # else: \n",
    "    #     def lr_lambda(epoch: int):\n",
    "    #         if epoch < epochs // 4:\n",
    "    #             return 1.\n",
    "    #         elif epochs // 4 <= epoch < (epochs // 4) * 2:\n",
    "    #             return 0.5\n",
    "    #         elif (epochs // 4) * 2 <= epoch < (epochs // 4) * 3:\n",
    "    #             return 0.1\n",
    "    #         else:\n",
    "    #             return 0.05\n",
    "            \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    early = EarlyStopping(\n",
    "        patience=config['train'].get('early_stop_patience', 10),\n",
    "        min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "        mode=\"min\", restore_best=True, rel_delta=True\n",
    "    )\n",
    "    ema_beta = float(config['train'].get('ema_beta', 0.8))\n",
    "\n",
    "    # --- тренинг-опции ---\n",
    "    epochs         = int(config['train']['epochs'])\n",
    "    batch_size     = int(config['train']['batch_size'])\n",
    "    accum_steps    = int(config['train'].get('accum_steps', 1))\n",
    "    max_grad_norm  = config['train'].get('max_grad_norm', 1.0)\n",
    "    min_bs_for_bn  = int(config['train'].get('min_bs_for_bn', 2))\n",
    "    reset_each_fold= bool(config['train'].get('reset_each_fold', False))\n",
    "\n",
    "    if verbose: print('size of df:', df.shape)\n",
    "\n",
    "    # --- нормализация (фиксируем μ,σ по первому train-сплиту) ---\n",
    "    cols_to_scale = config['data'].get('cols_to_scale', [\n",
    "        'DayAvgPrice','IntradayStd','Volume','Log_Profit','DayAvgPrice_diff',\n",
    "        'DAP_1','DAP_2','DAP_3','DAP_4','DAP_5','DAP_6',\n",
    "        'POLY_1','POLY_2','POLY_3','lambda_C3','lambda_C2','lambda_C1'\n",
    "    ])\n",
    "\n",
    "    mu = sigma = None\n",
    "    mu_target = sigma_target = None\n",
    "\n",
    "    # --- сбор результатов по фолдам ---\n",
    "    fold_metrics: List[Dict] = []\n",
    "    fold_pred_series: List[pd.Series] = []\n",
    "    preds_last_10 = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "        # опциональный ресет весов (для «честной» CV)\n",
    "        if reset_each_fold and (fold + 1) > 1:\n",
    "            # real_net.apply(_reset_weights)\n",
    "            # complex_net.apply(_reset_weights)\n",
    "            # fusion_net.apply(_reset_weights)\n",
    "            # optimizer = optim.Adam(\n",
    "            #     list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "            #     lr=config['train']['lr']\n",
    "            # )\n",
    "            # сбросить состояние LR-схем, ES, EMA\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=100, threshold=1e-3, threshold_mode='rel',\n",
    "            #     cooldown=2, verbose=True\n",
    "            # )\n",
    "            # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            #     optimizer, mode='min', factor=0.5, patience=5, verbose=verbose\n",
    "            # )\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fold2)\n",
    "            early = EarlyStopping(\n",
    "                patience=config['train'].get('early_stop_patience', 10),\n",
    "                min_delta=config['train'].get('early_stop_min_delta', 1e-2),\n",
    "                mode=\"min\", restore_best=True, rel_delta=True\n",
    "            )\n",
    "            ema_val = None\n",
    "        else:\n",
    "            ema_val = None  # сбрасываем EMA в любом случае на новый фолд (иначе метрики не сопоставимы)\n",
    "\n",
    "        train_df = df.iloc[train_idx].copy()\n",
    "\n",
    "        # расширим val назад до window_size, безопасно по границам\n",
    "        if len(val_idx) <= window_size:\n",
    "            end = val_idx[-1]\n",
    "            start = max(0, end - window_size)\n",
    "            val_idx = list(range(start, end + 1))\n",
    "\n",
    "        val_df = df.iloc[val_idx].copy()\n",
    "\n",
    "        # μ,σ считаем по ПЕРВОМУ train-сплиту\n",
    "        if mu is None or sigma is None:\n",
    "            mu = train_df[cols_to_scale].mean()\n",
    "            sigma = train_df[cols_to_scale].std().replace(0, 1.0)\n",
    "\n",
    "        # применяем нормировку\n",
    "        for col in cols_to_scale:\n",
    "            train_df.loc[:, col] = (train_df[col] - mu[col]) / sigma[col]\n",
    "            val_df.loc[:,   col] = (val_df[col]   - mu[col]) / sigma[col]\n",
    "\n",
    "        # μ,σ для z-нормировки Target\n",
    "        if mu_target is None or sigma_target is None:\n",
    "            mu_target = train_df['Target'].mean()\n",
    "            sigma_target = train_df['Target'].std()\n",
    "\n",
    "        train_df['Target'] = (train_df['Target'] - mu_target) / sigma_target\n",
    "        val_df['Target'] = (val_df['Target']   - mu_target) / sigma_target \n",
    "        train_df['Target_smooth'] = (train_df['Target_smooth'] - mu_target) / sigma_target\n",
    "        val_df['Target_smooth'] = (val_df['Target_smooth']   - mu_target) / sigma_target  \n",
    "\n",
    "        #print(f\"\\ntrain_end {train_df.iloc[-1]}, val_end {val_df.iloc[-1]}\")  \n",
    "\n",
    "        # лоадеры\n",
    "        train_loader = load_dataloader(train_df, window_size, batch_size, shuffle=True,  drop_last=False)\n",
    "        val_loader   = load_dataloader(val_df,   window_size, batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFold {fold+1}: train {len(train_df)}, val {len(val_df)}\",\n",
    "                  \"\\ntrain first...last :\", train_df['date'].min(), \"...\" , train_df['date'].max(),\n",
    "                  \"\\nval first...last   :\", val_df['date'].min(), \"...\" ,   val_df['date'].max())\n",
    "\n",
    "        # контрольный принт последнего окна\n",
    "            \n",
    "        dst   = TwoStreamDataset(train_df, window_size=window_size)\n",
    "        sampt = dst[len(dst) - 1]\n",
    "        last_DAP_in_t_window = float(sampt['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        target_pricet           = float(sampt['target']) * sigma_target + mu_target\n",
    "        target_smooth_pricet = train_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"train last row | DAP={last_DAP_in_t_window:.4f} | Target={target_pricet:.4f} | Target_smooth={target_smooth_pricet:.4f}\")\n",
    "\n",
    "        ds   = TwoStreamDataset(val_df, window_size=window_size)\n",
    "        samp = ds[len(ds) - 1]\n",
    "        # last_DAP_in_val_window = float(samp['real_feats'][-1, 0]) * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        last_DAP_in_val_window = float(samp['complex_time_real'][-1, 1]) * sigma['c_month_real'] + mu['c_month_real']\n",
    "        # last_DAP_in_val_window = val_df['imag_time'].iloc[-1] * sigma['imag_time'] + mu['imag_time']\n",
    "        # last_DAP_in_val_window = val_df['DayAvgPrice'].iloc[-1] * sigma['DayAvgPrice'] + mu['DayAvgPrice']\n",
    "        target_price           = float(samp['target']) * sigma_target + mu_target\n",
    "        target_smooth_price = val_df['Target_smooth'].iloc[-1] * sigma_target + mu_target\n",
    "        if verbose:\n",
    "            print(f\"val last row   | DAP={last_DAP_in_val_window:.4f} | Target={target_price:.4f} | Target_smooth={target_smooth_price:.4f}\")\n",
    "\n",
    "        # Графики\n",
    "        # from utils_live_plot import EMAMeter, TrainingPlotter  # если вынес в файл, иначе опусти импорт\n",
    "        # ema_meter    = EMAMeter(beta=config['train'].get('ema_beta', 0.8))\n",
    "        # live_plotter = TrainingPlotter(max_points=500)\n",
    "\n",
    "        # ----- ЭПОХИ -----\n",
    "        for epoch in range(epochs):\n",
    "            real_net.train(); complex_net.train(); fusion_net.train()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # TRAIN\n",
    "            step = 0\n",
    "            for step, batch in enumerate(train_loader, start=1):\n",
    "                x_real = batch['real_feats'].to(device)\n",
    "                if x_real.size(0) < min_bs_for_bn:\n",
    "                    continue\n",
    "                x_complex = torch.complex(\n",
    "                    batch['complex_time_real'].to(device),\n",
    "                    batch['complex_time_imag'].to(device)\n",
    "                )\n",
    "                y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                h_real = real_net(x_real)\n",
    "                h_r, h_i = complex_net(x_complex)\n",
    "                out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                loss = criterion(out, y) / accum_steps\n",
    "                if step == len(train_loader) - 1: train_loss_temp = loss\n",
    "                loss.backward()\n",
    "\n",
    "                if step % accum_steps == 0:\n",
    "                    if max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                            max_grad_norm\n",
    "                        )\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # доводим хвост аккумулирования\n",
    "            if step % accum_steps != 0:\n",
    "                if max_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(real_net.parameters()) + list(complex_net.parameters()) + list(fusion_net.parameters()),\n",
    "                        max_grad_norm\n",
    "                    )\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # VALID\n",
    "            real_net.eval(); complex_net.eval(); fusion_net.eval()\n",
    "            val_loss_sum, n_val = 0.0, 0\n",
    "            pred_vals, pred_dates = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    x_real = batch['real_feats'].to(device)\n",
    "                    x_complex = torch.complex(\n",
    "                        batch['complex_time_real'].to(device),\n",
    "                        batch['complex_time_imag'].to(device)\n",
    "                    )\n",
    "                    y = batch['target'].to(device).unsqueeze(-1)\n",
    "\n",
    "                    h_real = real_net(x_real)\n",
    "                    h_r, h_i = complex_net(x_complex)\n",
    "                    out = fusion_net(h_real, h_r, h_i)\n",
    "\n",
    "                    val_loss_sum += criterion(out, y).item()\n",
    "                    n_val += 1\n",
    "\n",
    "                    # даты предиктов\n",
    "                    pred_vals.extend(out.squeeze(-1).cpu().numpy())\n",
    "\n",
    "                    if 'target_date' in batch:\n",
    "                        pred_dates.extend(list(batch['target_date']))\n",
    "                    else:\n",
    "                        # fallback: считаем по глобальному индексу\n",
    "                        batch_idx_local  = batch['row_idx'].cpu().numpy()   # индекс окна в val_df\n",
    "                        global_start     = val_idx[0]                        # смещение в исходном df\n",
    "                        batch_idx_global = global_start + batch_idx_local\n",
    "                        # y = target[idx + window_size] → цель на \"завтра\" относительно начала окна\n",
    "                        tgt_idx = batch_idx_global + window_size\n",
    "                        pred_dates.extend(df['date'].iloc[tgt_idx].tolist())\n",
    "\n",
    "            avg_val_loss = val_loss_sum / max(1, n_val)\n",
    "\n",
    "            # --- EMA для scheduler/early ---\n",
    "            ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            use_metric = ema_val\n",
    "\n",
    "            scheduler.step()\n",
    "            # if epoch+1 >= config['train'].get('early_stop_warmup', 0):\n",
    "            #     scheduler.step()\n",
    "            #     #scheduler.step(use_metric)\n",
    "            #     stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     # диагностика (разово/по условию)\n",
    "            #     # print(f\"[ES] e{epoch+1} metric={use_metric:.5f}, best={early.best}, wait={early.wait}\")\n",
    "            #     if stop:\n",
    "            #         if verbose:\n",
    "            #             print(f\"[EarlyStop] best(ema)={early.best:.4f} → restore best weights\")\n",
    "            #         early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #         break\n",
    "            # else:\n",
    "            #     pass\n",
    "\n",
    "            # EMA для scheduler/early\n",
    "            # ema_val = avg_val_loss if (ema_val is None) else (ema_beta*ema_val + (1-ema_beta)*avg_val_loss)\n",
    "            # use_metric = ema_val  # <— ключевая строка: ES и Plateau смотрят на одно и то же\n",
    "\n",
    "            # Графики\n",
    "            # если ты уже считаешь ema_val сам — можно синхронизировать:\n",
    "            # ema_val = ema_meter.update(avg_val_loss)\n",
    "            # # обновляем график каждые, скажем, 1-5 эпох\n",
    "            # live_plotter.update(epoch+1, avg_val_loss, ema_val)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Validation Loss vs EMA\")\n",
    "            # Графики\n",
    "            # pred_val_denorm = [x * sigma_target + mu_target for x in pred_vals][0]    # откат нормировки целевой переменной\n",
    "            # live_plotter.update(epoch+1, pred_val_denorm, last_DAP_in_val_window)\n",
    "            # if (epoch+1) % 1 == 0:   # частота обновления\n",
    "            #     live_plotter.show(title=f\"Fold {fold+1} — Predict vs DAP\")\n",
    "            # Графики\n",
    "\n",
    "            # scheduler.step(use_metric)\n",
    "            # stop = early.step(use_metric, {\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "            if verbose and ((epoch+1) % 4 == 0 or (epoch+1) == 1):\n",
    "                print(f\"Epoch {epoch+1:03d} | train_loss={train_loss_temp:.6f} | val_loss={avg_val_loss:.4f} | ema={ema_val:.4f} | lr={optimizer.param_groups[0]['lr']:.5f} | pred = {pred_vals_denorm[0]:.4f}\")\n",
    "\n",
    "            if (epoch >= (epochs - 10)) and (fold + 1 == len(splits)):\n",
    "                preds_last_10.append(round(pred_vals_denorm[0], 4))\n",
    "            # if stop:\n",
    "            #     if verbose:\n",
    "            #         print(f\"[EarlyStop] best(ema)={early.best:.6f} → restore best weights\")\n",
    "            #     early.restore({\"real\": real_net, \"complex\": complex_net, \"fusion\": fusion_net})\n",
    "            #     break\n",
    "\n",
    "        # пост-валид метрики по датам target\n",
    "        pred_vals_denorm = [x * sigma_target + mu_target for x in pred_vals]    # откат нормировки целевой переменной\n",
    "        print(f\"pred = {pred_vals_denorm[-1]:.4f}, {len(pred_vals_denorm)}\")\n",
    "        pred_series = pd.Series(pred_vals_denorm, index=pd.to_datetime(pred_dates))\n",
    "        gt_series   = pd.to_datetime(df['date']).map(pd.Timestamp).map(\n",
    "            lambda d: d\n",
    "        )  # просто для явности типов\n",
    "        # берём фактические target по тем же датам:\n",
    "        gt_series = df.set_index(pd.to_datetime(df['date']))['Target'].reindex(pred_series.index)\n",
    "\n",
    "        mae_target_pred = mean_absolute_error(gt_series, pred_series)\n",
    "        # dap_series = df.set_index(pd.to_datetime(df['date']))['DayAvgPrice'].reindex(pred_series.index)\n",
    "        dap_series = df.set_index(pd.to_datetime(df['date']))['c_month_real'].reindex(pred_series.index)\n",
    "        mae_target_dap  = mean_absolute_error(gt_series, dap_series)\n",
    "\n",
    "        if fold + 1 == len(splits):\n",
    "            fold_pred_series.append(pred_series)\n",
    "            fold_metrics.append({\n",
    "                \"fold\": fold + 1,\n",
    "                \"n_pred\": len(pred_series),\n",
    "                \"mae_target_pred\": float(mae_target_pred),\n",
    "                \"mae_target_dap\":  float(mae_target_dap),\n",
    "                \"val_loss_last_epoch\": float(avg_val_loss),\n",
    "            })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Fold {fold+1}] Target = {gt_series[-1]:.4f} | Predict = {pred_series[-1]:.4f}\")\n",
    "            print(f\"MAE(Target~DAP) = {mae_target_dap:.4f} | MAE(Target~Predict) = {mae_target_pred:.4f}\\n\")\n",
    "\n",
    "    torch.save(real_net.state_dict(), f\"../checkpoints/trained_real_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(complex_net.state_dict(), f\"../checkpoints/trained_complex_net_{config['train']['lr']}.pth\")\n",
    "    torch.save(fusion_net.state_dict(), f\"../checkpoints/trained_fusion_net_{config['train']['lr']}.pth\")\n",
    "\n",
    "    # агрегированные результаты\n",
    "    results = {\n",
    "        \"fold_metrics\": fold_metrics,\n",
    "        \"pred_series_per_fold\": fold_pred_series,\n",
    "        \"preds_last_10\": preds_last_10,\n",
    "        \"mu\": mu, \"sigma\": sigma,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['open', 'high', 'low', 'close', 'DayAvgPrice', 'IntradayStd',\n",
    "       'Volume', 'day_of_week', 'day_of_year', 'Log_Profit',\n",
    "       'DayAvgPrice_diff', 'DayAvgPrice_2diff', 'POLY_1', 'POLY_2', 'POLY_3',\n",
    "       'parkinson_vol', 'parkinson_vol_ma5',\n",
    "       'parkinson_vol_ma20', 'parkinson_vol_diff1', 'parkinson_vol_lag1',\n",
    "       'DAP_1', 'DAP_2', 'DAP_3', 'DAP_4', 'DAP_5', 'DAP_6', 'DAP_7', 'DAP_10', \n",
    "       'DayAvgPrice_roll5', 'DayAvgPrice_roll10', 'DayAvgPrice_roll20',\n",
    "       'DayAvgPrice_ema5', 'DayAvgPrice_ema20', 'IntradayStd_roll5',\n",
    "       'IntradayStd_roll10', 'IntradayStd_roll20', 'IntradayStd_ema5',\n",
    "       'IntradayStd_ema20', 'mean_w', 'std_w',\n",
    "       'q10_w', 'q90_w', 'slope_w', 'vol_w', 'macd', 'macd_signal', 'macd_hist',\n",
    "       'bb_low', 'bb_mid', 'bb_up', 'atr14', 'dphi_hilbert',\n",
    "       'stft_energy_low', 'stft_energy_mid', 'stft_energy_high',\n",
    "       'gk_sigma', 'rs_sigma', 'yz_sigma', 'adx', 'chop', 'kalm_slope', 'rv20', 'vol_of_vol', \n",
    "       'bb_pct_b', 'bb_bandwidth', 'ret_overnight', 'ret_intraday', 'corr_ret_dlogvol', \n",
    "       'c_month_real',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"seed\": 42,\n",
    "  \"data\": {\n",
    "    \"start_preds_date\": \"2025-11-11\",\n",
    "    \"stop_preds_date\": \"2025-11-12\",\n",
    "    \"time_shift\": 2,\n",
    "    \"window_size\": 21,\n",
    "    \"cols_to_scale\": cols_to_scale,\n",
    "    \"lr_lambda\": \"cos\"\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"real_hidden_dim\": 128,\n",
    "    \"complex_hidden_dim\": 96,\n",
    "    \"n_res_blocks\": 2,\n",
    "    \"dropout\": 0.4,\n",
    "    \"bidirectional\": True,\n",
    "    \"take\": \"last_timestep\",\n",
    "    \"real_lstm_layers\": 2\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"lr\": 0.001,\n",
    "    \"loss\": \"SmoothL1Loss\",\n",
    "    \"epochs\": 64,\n",
    "    \"batch_size\": 96,\n",
    "    \"accum_steps\": 1,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"min_bs_for_bn\": 2,\n",
    "    \"early_stop_patience\": 200,\n",
    "    \"early_stop_min_delta\": 1e-2,\n",
    "    \"ema_beta\": 0.8,\n",
    "    \"reset_each_fold\": True\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем конфигурацию\n",
    "# df = pd.read_csv('../data_archiv/DTG/DTG_new_fea_to_12_11_2025_1d_w21_noweekend.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем конфигурацию\n",
    "df = pd.read_csv(f\"../data_archiv/DTG/DTG_new_fea_to_12_11_2025_1d_w{config['data']['window_size']}_noweekend.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DayAvgPrice</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>35.070833</td>\n",
       "      <td>35.094444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>2025-10-24</td>\n",
       "      <td>35.294445</td>\n",
       "      <td>34.881389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>35.094444</td>\n",
       "      <td>35.007222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>34.881389</td>\n",
       "      <td>34.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>35.007222</td>\n",
       "      <td>34.751945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>34.865000</td>\n",
       "      <td>34.606944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>34.751945</td>\n",
       "      <td>33.886111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>2025-11-03</td>\n",
       "      <td>34.606944</td>\n",
       "      <td>34.891389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>2025-11-04</td>\n",
       "      <td>33.886111</td>\n",
       "      <td>35.163611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>34.891389</td>\n",
       "      <td>34.328333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>2025-11-06</td>\n",
       "      <td>35.163611</td>\n",
       "      <td>35.753889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>34.328333</td>\n",
       "      <td>35.886111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>35.753889</td>\n",
       "      <td>36.299444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>35.886111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>36.299444</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  DayAvgPrice     Target\n",
       "916  2025-10-23    35.070833  35.094444\n",
       "917  2025-10-24    35.294445  34.881389\n",
       "918  2025-10-27    35.094444  35.007222\n",
       "919  2025-10-28    34.881389  34.865000\n",
       "920  2025-10-29    35.007222  34.751945\n",
       "921  2025-10-30    34.865000  34.606944\n",
       "922  2025-10-31    34.751945  33.886111\n",
       "923  2025-11-03    34.606944  34.891389\n",
       "924  2025-11-04    33.886111  35.163611\n",
       "925  2025-11-05    34.891389  34.328333\n",
       "926  2025-11-06    35.163611  35.753889\n",
       "927  2025-11-07    34.328333  35.886111\n",
       "928  2025-11-10    35.753889  36.299444\n",
       "929  2025-11-11    35.886111   0.000000\n",
       "930  2025-11-12    36.299444   0.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаём Target, пустые последние значения заменяются 0.\n",
    "H = config['data']['time_shift']     # time shift\n",
    "# df['Target'] = df['DayAvgPrice'].shift( - H).fillna(method='ffill')\n",
    "# df['Target'].iloc[-H:] = 0\n",
    "df['Target'] = df['DayAvgPrice'].shift( - H).fillna(0)\n",
    "df[['date', 'DayAvgPrice', 'Target']].tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c_month_real'] = df['DayAvgPrice'].copy()\n",
    "df['c_month_imag'] = df['IntradayStd'].copy()\n",
    "df['DayAvgPrice'] = df['c_week_real'].copy()\n",
    "df['IntradayStd'] = df['c_week_imag'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.446030 | val_loss=0.1710 | ema=0.1710 | lr=0.00050 | pred = 35.5744\n",
      "Epoch 004 | train_loss=0.065899 | val_loss=0.1196 | ema=0.1418 | lr=0.00125 | pred = 43.6171\n",
      "Epoch 008 | train_loss=0.097971 | val_loss=0.0143 | ema=0.0775 | lr=0.00225 | pred = 41.3635\n",
      "Epoch 012 | train_loss=0.100443 | val_loss=0.0819 | ema=0.0635 | lr=0.00300 | pred = 40.9599\n",
      "Epoch 016 | train_loss=0.026600 | val_loss=0.0217 | ema=0.0417 | lr=0.00296 | pred = 41.1615\n",
      "Epoch 020 | train_loss=0.031063 | val_loss=0.0304 | ema=0.0333 | lr=0.00283 | pred = 38.8893\n",
      "Epoch 024 | train_loss=0.027910 | val_loss=0.0686 | ema=0.0458 | lr=0.00262 | pred = 37.3969\n",
      "Epoch 028 | train_loss=0.018444 | val_loss=0.0371 | ema=0.0439 | lr=0.00235 | pred = 38.5908\n",
      "Epoch 032 | train_loss=0.018015 | val_loss=0.0413 | ema=0.0430 | lr=0.00203 | pred = 38.8152\n",
      "Epoch 036 | train_loss=0.019461 | val_loss=0.0294 | ema=0.0343 | lr=0.00168 | pred = 39.1231\n",
      "Epoch 040 | train_loss=0.020732 | val_loss=0.0373 | ema=0.0380 | lr=0.00132 | pred = 38.7337\n",
      "Epoch 044 | train_loss=0.017669 | val_loss=0.0397 | ema=0.0365 | lr=0.00097 | pred = 38.9495\n",
      "Epoch 048 | train_loss=0.017791 | val_loss=0.0243 | ema=0.0336 | lr=0.00065 | pred = 39.5305\n",
      "Epoch 052 | train_loss=0.013466 | val_loss=0.0261 | ema=0.0339 | lr=0.00038 | pred = 39.6530\n",
      "Epoch 056 | train_loss=0.009613 | val_loss=0.0307 | ema=0.0325 | lr=0.00017 | pred = 39.2737\n",
      "Epoch 060 | train_loss=0.011772 | val_loss=0.0328 | ema=0.0320 | lr=0.00004 | pred = 39.2082\n",
      "Epoch 064 | train_loss=0.013568 | val_loss=0.0322 | ema=0.0322 | lr=0.00000 | pred = 39.2177\n",
      "pred = 34.9806, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 34.9806\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 1.0386\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.013919 | val_loss=6.2007 | ema=6.2007 | lr=0.00010 | pred = 36.1275\n",
      "Epoch 004 | train_loss=0.016306 | val_loss=6.1177 | ema=6.1720 | lr=0.00025 | pred = 35.6803\n",
      "Epoch 008 | train_loss=0.012431 | val_loss=6.2252 | ema=6.2028 | lr=0.00045 | pred = 36.2599\n",
      "Epoch 012 | train_loss=0.014533 | val_loss=6.2118 | ema=6.2063 | lr=0.00060 | pred = 36.1873\n",
      "Epoch 016 | train_loss=0.015725 | val_loss=6.1473 | ema=6.1514 | lr=0.00059 | pred = 35.8400\n",
      "Epoch 020 | train_loss=0.015051 | val_loss=6.2134 | ema=6.2106 | lr=0.00057 | pred = 36.1960\n",
      "Epoch 024 | train_loss=0.014185 | val_loss=6.3084 | ema=6.2462 | lr=0.00052 | pred = 36.7082\n",
      "Epoch 028 | train_loss=0.010159 | val_loss=6.2622 | ema=6.2411 | lr=0.00047 | pred = 36.4595\n",
      "Epoch 032 | train_loss=0.012795 | val_loss=6.2330 | ema=6.2541 | lr=0.00041 | pred = 36.3019\n",
      "Epoch 036 | train_loss=0.011927 | val_loss=6.2585 | ema=6.2319 | lr=0.00034 | pred = 36.4395\n",
      "Epoch 040 | train_loss=0.013097 | val_loss=6.2215 | ema=6.2203 | lr=0.00026 | pred = 36.2399\n",
      "Epoch 044 | train_loss=0.013307 | val_loss=6.2805 | ema=6.2289 | lr=0.00019 | pred = 36.5581\n",
      "Epoch 048 | train_loss=0.012738 | val_loss=6.2360 | ema=6.2434 | lr=0.00013 | pred = 36.3180\n",
      "Epoch 052 | train_loss=0.010057 | val_loss=6.2531 | ema=6.2498 | lr=0.00008 | pred = 36.4101\n",
      "Epoch 056 | train_loss=0.010873 | val_loss=6.2689 | ema=6.2537 | lr=0.00003 | pred = 36.4952\n",
      "Epoch 060 | train_loss=0.010830 | val_loss=6.2537 | ema=6.2526 | lr=0.00001 | pred = 36.4135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  10%|█         | 1/10 [06:47<1:01:09, 407.78s/it, last_mae_pred=36.4423, last_mae_dap=36.2994, avg_mae_pred=36.4423, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.011616 | val_loss=6.2591 | ema=6.2563 | lr=0.00000 | pred = 36.4423\n",
      "pred = 36.4423, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.4423\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.4423\n",
      "\n",
      "[36.4461, 36.4952, 36.4104, 36.3961, 36.3912, 36.4135, 36.4369, 36.4444, 36.4424, 36.4423]  | mean=36.4319 | std=0.0305\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.466154 | val_loss=0.2850 | ema=0.2850 | lr=0.00033 | pred = 34.5400\n",
      "Epoch 004 | train_loss=0.113900 | val_loss=0.1527 | ema=0.2110 | lr=0.00083 | pred = 41.2000\n",
      "Epoch 008 | train_loss=0.153910 | val_loss=0.0520 | ema=0.1376 | lr=0.00150 | pred = 43.7978\n",
      "Epoch 012 | train_loss=0.033240 | val_loss=0.0769 | ema=0.1162 | lr=0.00200 | pred = 44.5610\n",
      "Epoch 016 | train_loss=0.037989 | val_loss=0.0375 | ema=0.0648 | lr=0.00197 | pred = 42.6836\n",
      "Epoch 020 | train_loss=0.025480 | val_loss=0.0128 | ema=0.0446 | lr=0.00189 | pred = 40.7493\n",
      "Epoch 024 | train_loss=0.023947 | val_loss=0.0452 | ema=0.0351 | lr=0.00175 | pred = 38.3834\n",
      "Epoch 028 | train_loss=0.023904 | val_loss=0.0436 | ema=0.0335 | lr=0.00157 | pred = 38.7969\n",
      "Epoch 032 | train_loss=0.021062 | val_loss=0.0141 | ema=0.0273 | lr=0.00135 | pred = 41.5472\n",
      "Epoch 036 | train_loss=0.024870 | val_loss=0.0418 | ema=0.0287 | lr=0.00112 | pred = 39.1129\n",
      "Epoch 040 | train_loss=0.020407 | val_loss=0.0211 | ema=0.0283 | lr=0.00088 | pred = 40.0794\n",
      "Epoch 044 | train_loss=0.016509 | val_loss=0.0300 | ema=0.0285 | lr=0.00065 | pred = 40.0517\n",
      "Epoch 048 | train_loss=0.014367 | val_loss=0.0199 | ema=0.0260 | lr=0.00043 | pred = 39.9422\n",
      "Epoch 052 | train_loss=0.012304 | val_loss=0.0260 | ema=0.0277 | lr=0.00025 | pred = 39.8073\n",
      "Epoch 056 | train_loss=0.009806 | val_loss=0.0299 | ema=0.0267 | lr=0.00011 | pred = 39.5895\n",
      "Epoch 060 | train_loss=0.014118 | val_loss=0.0280 | ema=0.0272 | lr=0.00003 | pred = 39.6576\n",
      "Epoch 064 | train_loss=0.014597 | val_loss=0.0275 | ema=0.0276 | lr=0.00000 | pred = 39.6576\n",
      "pred = 34.6975, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 34.6975\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.9867\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.015615 | val_loss=6.1755 | ema=6.1755 | lr=0.00007 | pred = 35.9920\n",
      "Epoch 004 | train_loss=0.017403 | val_loss=6.1588 | ema=6.1630 | lr=0.00017 | pred = 35.9016\n",
      "Epoch 008 | train_loss=0.017049 | val_loss=6.2470 | ema=6.1947 | lr=0.00030 | pred = 36.3772\n",
      "Epoch 012 | train_loss=0.014830 | val_loss=6.2067 | ema=6.2076 | lr=0.00040 | pred = 36.1602\n",
      "Epoch 016 | train_loss=0.018305 | val_loss=6.2541 | ema=6.2074 | lr=0.00039 | pred = 36.4154\n",
      "Epoch 020 | train_loss=0.016348 | val_loss=6.2012 | ema=6.2224 | lr=0.00038 | pred = 36.1301\n",
      "Epoch 024 | train_loss=0.016294 | val_loss=6.3372 | ema=6.2563 | lr=0.00035 | pred = 36.8638\n",
      "Epoch 028 | train_loss=0.008972 | val_loss=6.2501 | ema=6.2422 | lr=0.00031 | pred = 36.3938\n",
      "Epoch 032 | train_loss=0.016432 | val_loss=6.2622 | ema=6.2428 | lr=0.00027 | pred = 36.4591\n",
      "Epoch 036 | train_loss=0.011769 | val_loss=6.2383 | ema=6.2443 | lr=0.00022 | pred = 36.3303\n",
      "Epoch 040 | train_loss=0.013833 | val_loss=6.2588 | ema=6.2479 | lr=0.00018 | pred = 36.4409\n",
      "Epoch 044 | train_loss=0.013856 | val_loss=6.2527 | ema=6.2377 | lr=0.00013 | pred = 36.4078\n",
      "Epoch 048 | train_loss=0.014156 | val_loss=6.2427 | ema=6.2381 | lr=0.00009 | pred = 36.3540\n",
      "Epoch 052 | train_loss=0.010102 | val_loss=6.2292 | ema=6.2348 | lr=0.00005 | pred = 36.2816\n",
      "Epoch 056 | train_loss=0.011470 | val_loss=6.2470 | ema=6.2360 | lr=0.00002 | pred = 36.3772\n",
      "Epoch 060 | train_loss=0.012025 | val_loss=6.2368 | ema=6.2364 | lr=0.00001 | pred = 36.3224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  20%|██        | 2/10 [13:43<54:58, 412.29s/it, last_mae_pred=36.3372, last_mae_dap=36.2994, avg_mae_pred=36.3898, avg_mae_dap=36.2994]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.012804 | val_loss=6.2396 | ema=6.2384 | lr=0.00000 | pred = 36.3372\n",
      "pred = 36.3372, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.3372\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.3372\n",
      "\n",
      "[36.277, 36.3772, 36.3496, 36.3088, 36.3113, 36.3224, 36.3419, 36.3405, 36.337, 36.3372]  | mean=36.3303 | std=0.0271\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.484978 | val_loss=0.2976 | ema=0.2976 | lr=0.00017 | pred = 34.3028\n",
      "Epoch 004 | train_loss=0.158395 | val_loss=0.1030 | ema=0.2135 | lr=0.00042 | pred = 40.9596\n",
      "Epoch 008 | train_loss=0.085854 | val_loss=0.0379 | ema=0.1409 | lr=0.00075 | pred = 44.4828\n",
      "Epoch 012 | train_loss=0.049689 | val_loss=0.0279 | ema=0.0788 | lr=0.00100 | pred = 42.5405\n",
      "Epoch 016 | train_loss=0.059200 | val_loss=0.0220 | ema=0.0530 | lr=0.00099 | pred = 39.7410\n",
      "Epoch 020 | train_loss=0.034016 | val_loss=0.0198 | ema=0.0335 | lr=0.00094 | pred = 40.5570\n",
      "Epoch 024 | train_loss=0.023624 | val_loss=0.0221 | ema=0.0236 | lr=0.00087 | pred = 40.1621\n",
      "Epoch 028 | train_loss=0.019764 | val_loss=0.0343 | ema=0.0242 | lr=0.00078 | pred = 39.0228\n",
      "Epoch 032 | train_loss=0.021471 | val_loss=0.0182 | ema=0.0214 | lr=0.00068 | pred = 40.3330\n",
      "Epoch 036 | train_loss=0.018333 | val_loss=0.0164 | ema=0.0199 | lr=0.00056 | pred = 40.1365\n",
      "Epoch 040 | train_loss=0.021781 | val_loss=0.0207 | ema=0.0213 | lr=0.00044 | pred = 40.2185\n",
      "Epoch 044 | train_loss=0.013693 | val_loss=0.0167 | ema=0.0197 | lr=0.00032 | pred = 40.5681\n",
      "Epoch 048 | train_loss=0.015871 | val_loss=0.0186 | ema=0.0199 | lr=0.00022 | pred = 40.3204\n",
      "Epoch 052 | train_loss=0.012105 | val_loss=0.0208 | ema=0.0215 | lr=0.00013 | pred = 40.0787\n",
      "Epoch 056 | train_loss=0.011057 | val_loss=0.0169 | ema=0.0192 | lr=0.00006 | pred = 40.3483\n",
      "Epoch 060 | train_loss=0.014050 | val_loss=0.0171 | ema=0.0179 | lr=0.00001 | pred = 40.3017\n",
      "Epoch 064 | train_loss=0.016303 | val_loss=0.0176 | ema=0.0178 | lr=0.00000 | pred = 40.2323\n",
      "pred = 34.8728, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 34.8728\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.8283\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.017269 | val_loss=6.1720 | ema=6.1720 | lr=0.00003 | pred = 35.9731\n",
      "Epoch 004 | train_loss=0.019840 | val_loss=6.1207 | ema=6.1495 | lr=0.00008 | pred = 35.6962\n",
      "Epoch 008 | train_loss=0.014884 | val_loss=6.2066 | ema=6.1611 | lr=0.00015 | pred = 36.1593\n",
      "Epoch 012 | train_loss=0.019972 | val_loss=6.1525 | ema=6.1561 | lr=0.00020 | pred = 35.8675\n",
      "Epoch 016 | train_loss=0.020052 | val_loss=6.1794 | ema=6.1447 | lr=0.00020 | pred = 36.0127\n",
      "Epoch 020 | train_loss=0.015932 | val_loss=6.1791 | ema=6.1735 | lr=0.00019 | pred = 36.0112\n",
      "Epoch 024 | train_loss=0.013748 | val_loss=6.2897 | ema=6.2155 | lr=0.00017 | pred = 36.6077\n",
      "Epoch 028 | train_loss=0.012698 | val_loss=6.2286 | ema=6.2023 | lr=0.00016 | pred = 36.2783\n",
      "Epoch 032 | train_loss=0.016532 | val_loss=6.2223 | ema=6.2237 | lr=0.00014 | pred = 36.2439\n",
      "Epoch 036 | train_loss=0.014679 | val_loss=6.2837 | ema=6.2577 | lr=0.00011 | pred = 36.5751\n",
      "Epoch 040 | train_loss=0.012038 | val_loss=6.2969 | ema=6.2632 | lr=0.00009 | pred = 36.6461\n",
      "Epoch 044 | train_loss=0.014225 | val_loss=6.2165 | ema=6.2379 | lr=0.00006 | pred = 36.2128\n",
      "Epoch 048 | train_loss=0.014092 | val_loss=6.2126 | ema=6.2407 | lr=0.00004 | pred = 36.1919\n",
      "Epoch 052 | train_loss=0.010585 | val_loss=6.2423 | ema=6.2378 | lr=0.00003 | pred = 36.3522\n",
      "Epoch 056 | train_loss=0.011550 | val_loss=6.2658 | ema=6.2461 | lr=0.00001 | pred = 36.4786\n",
      "Epoch 060 | train_loss=0.011725 | val_loss=6.2388 | ema=6.2444 | lr=0.00000 | pred = 36.3330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  30%|███       | 3/10 [20:38<48:14, 413.56s/it, last_mae_pred=36.3900, last_mae_dap=36.2994, avg_mae_pred=36.3899, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.014785 | val_loss=6.2494 | ema=6.2469 | lr=0.00000 | pred = 36.3900\n",
      "pred = 36.3900, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.3900\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.3900\n",
      "\n",
      "[36.351, 36.4786, 36.4314, 36.3674, 36.3314, 36.333, 36.3691, 36.3886, 36.389, 36.39]  | mean=36.3830 | std=0.0450\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.488586 | val_loss=0.2996 | ema=0.2996 | lr=0.00015 | pred = 34.2460\n",
      "Epoch 004 | train_loss=0.110408 | val_loss=0.1440 | ema=0.2203 | lr=0.00038 | pred = 40.2812\n",
      "Epoch 008 | train_loss=0.085387 | val_loss=0.1307 | ema=0.1476 | lr=0.00068 | pred = 44.2908\n",
      "Epoch 012 | train_loss=0.035893 | val_loss=0.0262 | ema=0.1092 | lr=0.00090 | pred = 43.3869\n",
      "Epoch 016 | train_loss=0.031480 | val_loss=0.0187 | ema=0.0575 | lr=0.00089 | pred = 40.1950\n",
      "Epoch 020 | train_loss=0.024048 | val_loss=0.0311 | ema=0.0380 | lr=0.00085 | pred = 38.7419\n",
      "Epoch 024 | train_loss=0.037117 | val_loss=0.0223 | ema=0.0269 | lr=0.00079 | pred = 39.5331\n",
      "Epoch 028 | train_loss=0.028384 | val_loss=0.0204 | ema=0.0218 | lr=0.00071 | pred = 40.0417\n",
      "Epoch 032 | train_loss=0.019985 | val_loss=0.0185 | ema=0.0203 | lr=0.00061 | pred = 40.1869\n",
      "Epoch 036 | train_loss=0.020084 | val_loss=0.0171 | ema=0.0191 | lr=0.00050 | pred = 40.4923\n",
      "Epoch 040 | train_loss=0.021002 | val_loss=0.0177 | ema=0.0189 | lr=0.00040 | pred = 40.2561\n",
      "Epoch 044 | train_loss=0.014211 | val_loss=0.0199 | ema=0.0187 | lr=0.00029 | pred = 40.8957\n",
      "Epoch 048 | train_loss=0.018798 | val_loss=0.0226 | ema=0.0199 | lr=0.00019 | pred = 40.2451\n",
      "Epoch 052 | train_loss=0.016990 | val_loss=0.0181 | ema=0.0196 | lr=0.00011 | pred = 40.3958\n",
      "Epoch 056 | train_loss=0.010616 | val_loss=0.0176 | ema=0.0184 | lr=0.00005 | pred = 40.4740\n",
      "Epoch 060 | train_loss=0.012159 | val_loss=0.0169 | ema=0.0176 | lr=0.00001 | pred = 40.5297\n",
      "Epoch 064 | train_loss=0.013993 | val_loss=0.0171 | ema=0.0173 | lr=0.00000 | pred = 40.4626\n",
      "pred = 35.1628, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 35.1628\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.8302\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.017441 | val_loss=6.2773 | ema=6.2773 | lr=0.00003 | pred = 36.5408\n",
      "Epoch 004 | train_loss=0.019897 | val_loss=6.2216 | ema=6.2532 | lr=0.00008 | pred = 36.2405\n",
      "Epoch 008 | train_loss=0.013138 | val_loss=6.3088 | ema=6.2578 | lr=0.00014 | pred = 36.7105\n",
      "Epoch 012 | train_loss=0.016713 | val_loss=6.2751 | ema=6.2664 | lr=0.00018 | pred = 36.5286\n",
      "Epoch 016 | train_loss=0.018505 | val_loss=6.2613 | ema=6.2460 | lr=0.00018 | pred = 36.4542\n",
      "Epoch 020 | train_loss=0.015478 | val_loss=6.3024 | ema=6.2910 | lr=0.00017 | pred = 36.6759\n",
      "Epoch 024 | train_loss=0.015837 | val_loss=6.3386 | ema=6.2819 | lr=0.00016 | pred = 36.8710\n",
      "Epoch 028 | train_loss=0.014215 | val_loss=6.2528 | ema=6.2620 | lr=0.00014 | pred = 36.4085\n",
      "Epoch 032 | train_loss=0.014751 | val_loss=6.2970 | ema=6.2750 | lr=0.00012 | pred = 36.6468\n",
      "Epoch 036 | train_loss=0.013507 | val_loss=6.3344 | ema=6.3157 | lr=0.00010 | pred = 36.8487\n",
      "Epoch 040 | train_loss=0.012898 | val_loss=6.3699 | ema=6.3249 | lr=0.00008 | pred = 37.0398\n",
      "Epoch 044 | train_loss=0.014353 | val_loss=6.2891 | ema=6.3091 | lr=0.00006 | pred = 36.6043\n",
      "Epoch 048 | train_loss=0.013800 | val_loss=6.3207 | ema=6.3256 | lr=0.00004 | pred = 36.7748\n",
      "Epoch 052 | train_loss=0.013652 | val_loss=6.3126 | ema=6.3241 | lr=0.00002 | pred = 36.7310\n",
      "Epoch 056 | train_loss=0.013291 | val_loss=6.3483 | ema=6.3332 | lr=0.00001 | pred = 36.9235\n",
      "Epoch 060 | train_loss=0.013248 | val_loss=6.3297 | ema=6.3346 | lr=0.00000 | pred = 36.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  40%|████      | 4/10 [27:33<41:25, 414.23s/it, last_mae_pred=36.8480, last_mae_dap=36.2994, avg_mae_pred=36.5044, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.015150 | val_loss=6.3343 | ema=6.3343 | lr=0.00000 | pred = 36.8480\n",
      "pred = 36.8480, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.8480\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.8480\n",
      "\n",
      "[36.7752, 36.9235, 36.9432, 36.8703, 36.826, 36.8234, 36.842, 36.8491, 36.8476, 36.848]  | mean=36.8548 | std=0.0486\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.489786 | val_loss=0.3014 | ema=0.3014 | lr=0.00013 | pred = 34.1982\n",
      "Epoch 004 | train_loss=0.120942 | val_loss=0.2270 | ema=0.2349 | lr=0.00033 | pred = 40.4398\n",
      "Epoch 008 | train_loss=0.104411 | val_loss=0.1119 | ema=0.1480 | lr=0.00060 | pred = 43.3581\n",
      "Epoch 012 | train_loss=0.072029 | val_loss=0.0431 | ema=0.0804 | lr=0.00080 | pred = 40.5440\n",
      "Epoch 016 | train_loss=0.068020 | val_loss=0.0260 | ema=0.0481 | lr=0.00079 | pred = 39.8179\n",
      "Epoch 020 | train_loss=0.025548 | val_loss=0.0274 | ema=0.0326 | lr=0.00075 | pred = 39.1418\n",
      "Epoch 024 | train_loss=0.033127 | val_loss=0.0183 | ema=0.0286 | lr=0.00070 | pred = 40.4381\n",
      "Epoch 028 | train_loss=0.019135 | val_loss=0.0248 | ema=0.0247 | lr=0.00063 | pred = 39.7625\n",
      "Epoch 032 | train_loss=0.019591 | val_loss=0.0201 | ema=0.0229 | lr=0.00054 | pred = 39.8213\n",
      "Epoch 036 | train_loss=0.021370 | val_loss=0.0230 | ema=0.0236 | lr=0.00045 | pred = 39.5589\n",
      "Epoch 040 | train_loss=0.023455 | val_loss=0.0279 | ema=0.0275 | lr=0.00035 | pred = 39.2208\n",
      "Epoch 044 | train_loss=0.013495 | val_loss=0.0163 | ema=0.0234 | lr=0.00026 | pred = 40.3208\n",
      "Epoch 048 | train_loss=0.019741 | val_loss=0.0228 | ema=0.0244 | lr=0.00017 | pred = 39.6761\n",
      "Epoch 052 | train_loss=0.013019 | val_loss=0.0246 | ema=0.0254 | lr=0.00010 | pred = 39.5713\n",
      "Epoch 056 | train_loss=0.012678 | val_loss=0.0232 | ema=0.0244 | lr=0.00005 | pred = 39.7062\n",
      "Epoch 060 | train_loss=0.014451 | val_loss=0.0226 | ema=0.0234 | lr=0.00001 | pred = 39.6955\n",
      "Epoch 064 | train_loss=0.017043 | val_loss=0.0230 | ema=0.0232 | lr=0.00000 | pred = 39.6548\n",
      "pred = 35.1242, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 35.1242\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.9214\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.022128 | val_loss=6.2679 | ema=6.2679 | lr=0.00003 | pred = 36.4900\n",
      "Epoch 004 | train_loss=0.021398 | val_loss=6.2008 | ema=6.2415 | lr=0.00007 | pred = 36.1283\n",
      "Epoch 008 | train_loss=0.016193 | val_loss=6.2660 | ema=6.2387 | lr=0.00012 | pred = 36.4799\n",
      "Epoch 012 | train_loss=0.019839 | val_loss=6.2180 | ema=6.2305 | lr=0.00016 | pred = 36.2207\n",
      "Epoch 016 | train_loss=0.019395 | val_loss=6.2393 | ema=6.2355 | lr=0.00016 | pred = 36.3360\n",
      "Epoch 020 | train_loss=0.015422 | val_loss=6.2643 | ema=6.2588 | lr=0.00015 | pred = 36.4708\n",
      "Epoch 024 | train_loss=0.017393 | val_loss=6.2935 | ema=6.2606 | lr=0.00014 | pred = 36.6280\n",
      "Epoch 028 | train_loss=0.014098 | val_loss=6.2617 | ema=6.2600 | lr=0.00013 | pred = 36.4565\n",
      "Epoch 032 | train_loss=0.015176 | val_loss=6.2738 | ema=6.2892 | lr=0.00011 | pred = 36.5217\n",
      "Epoch 036 | train_loss=0.014101 | val_loss=6.3243 | ema=6.3091 | lr=0.00009 | pred = 36.7942\n",
      "Epoch 040 | train_loss=0.012992 | val_loss=6.3522 | ema=6.3205 | lr=0.00007 | pred = 36.9445\n",
      "Epoch 044 | train_loss=0.014836 | val_loss=6.2998 | ema=6.3065 | lr=0.00005 | pred = 36.6620\n",
      "Epoch 048 | train_loss=0.016044 | val_loss=6.2844 | ema=6.3055 | lr=0.00003 | pred = 36.5787\n",
      "Epoch 052 | train_loss=0.012836 | val_loss=6.3000 | ema=6.3047 | lr=0.00002 | pred = 36.6628\n",
      "Epoch 056 | train_loss=0.010145 | val_loss=6.3337 | ema=6.3154 | lr=0.00001 | pred = 36.8446\n",
      "Epoch 060 | train_loss=0.012138 | val_loss=6.3106 | ema=6.3172 | lr=0.00000 | pred = 36.7204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  50%|█████     | 5/10 [34:22<34:22, 412.49s/it, last_mae_pred=36.7379, last_mae_dap=36.2994, avg_mae_pred=36.5511, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.015565 | val_loss=6.3139 | ema=6.3151 | lr=0.00000 | pred = 36.7379\n",
      "pred = 36.7379, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.7379\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.7379\n",
      "\n",
      "[36.6943, 36.8446, 36.8424, 36.7864, 36.7453, 36.7204, 36.7344, 36.7376, 36.7366, 36.7379]  | mean=36.7580 | std=0.0504\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.492557 | val_loss=0.3025 | ema=0.3025 | lr=0.00012 | pred = 34.2119\n",
      "Epoch 004 | train_loss=0.123683 | val_loss=0.2219 | ema=0.2361 | lr=0.00029 | pred = 40.0635\n",
      "Epoch 008 | train_loss=0.107952 | val_loss=0.0555 | ema=0.1484 | lr=0.00052 | pred = 41.4380\n",
      "Epoch 012 | train_loss=0.030772 | val_loss=0.0175 | ema=0.0945 | lr=0.00070 | pred = 41.8514\n",
      "Epoch 016 | train_loss=0.047700 | val_loss=0.0177 | ema=0.0567 | lr=0.00069 | pred = 40.0092\n",
      "Epoch 020 | train_loss=0.031305 | val_loss=0.0300 | ema=0.0441 | lr=0.00066 | pred = 39.1650\n",
      "Epoch 024 | train_loss=0.032706 | val_loss=0.0324 | ema=0.0329 | lr=0.00061 | pred = 40.6652\n",
      "Epoch 028 | train_loss=0.022775 | val_loss=0.0193 | ema=0.0259 | lr=0.00055 | pred = 40.7402\n",
      "Epoch 032 | train_loss=0.022038 | val_loss=0.0205 | ema=0.0232 | lr=0.00047 | pred = 40.8144\n",
      "Epoch 036 | train_loss=0.022187 | val_loss=0.0278 | ema=0.0258 | lr=0.00039 | pred = 39.2538\n",
      "Epoch 040 | train_loss=0.022157 | val_loss=0.0231 | ema=0.0248 | lr=0.00031 | pred = 40.1459\n",
      "Epoch 044 | train_loss=0.013258 | val_loss=0.0178 | ema=0.0210 | lr=0.00023 | pred = 40.5875\n",
      "Epoch 048 | train_loss=0.017095 | val_loss=0.0187 | ema=0.0210 | lr=0.00015 | pred = 40.2357\n",
      "Epoch 052 | train_loss=0.015152 | val_loss=0.0147 | ema=0.0191 | lr=0.00009 | pred = 40.6097\n",
      "Epoch 056 | train_loss=0.011855 | val_loss=0.0152 | ema=0.0171 | lr=0.00004 | pred = 40.5258\n",
      "Epoch 060 | train_loss=0.013467 | val_loss=0.0147 | ema=0.0160 | lr=0.00001 | pred = 40.5463\n",
      "Epoch 064 | train_loss=0.016205 | val_loss=0.0152 | ema=0.0154 | lr=0.00000 | pred = 40.4939\n",
      "pred = 34.9353, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 34.9353\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.7665\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.018477 | val_loss=6.2399 | ema=6.2399 | lr=0.00002 | pred = 36.3391\n",
      "Epoch 004 | train_loss=0.022559 | val_loss=6.1753 | ema=6.2197 | lr=0.00006 | pred = 35.9905\n",
      "Epoch 008 | train_loss=0.015329 | val_loss=6.2505 | ema=6.2213 | lr=0.00011 | pred = 36.3960\n",
      "Epoch 012 | train_loss=0.018399 | val_loss=6.2941 | ema=6.2294 | lr=0.00014 | pred = 36.6312\n",
      "Epoch 016 | train_loss=0.019750 | val_loss=6.2607 | ema=6.2409 | lr=0.00014 | pred = 36.4510\n",
      "Epoch 020 | train_loss=0.014476 | val_loss=6.2586 | ema=6.2580 | lr=0.00013 | pred = 36.4397\n",
      "Epoch 024 | train_loss=0.017166 | val_loss=6.3151 | ema=6.2680 | lr=0.00012 | pred = 36.7444\n",
      "Epoch 028 | train_loss=0.012427 | val_loss=6.2237 | ema=6.2391 | lr=0.00011 | pred = 36.2519\n",
      "Epoch 032 | train_loss=0.019907 | val_loss=6.2753 | ema=6.2743 | lr=0.00009 | pred = 36.5299\n",
      "Epoch 036 | train_loss=0.014511 | val_loss=6.3217 | ema=6.3016 | lr=0.00008 | pred = 36.7802\n",
      "Epoch 040 | train_loss=0.013625 | val_loss=6.3962 | ema=6.3230 | lr=0.00006 | pred = 37.1818\n",
      "Epoch 044 | train_loss=0.014633 | val_loss=6.2744 | ema=6.3024 | lr=0.00005 | pred = 36.5251\n",
      "Epoch 048 | train_loss=0.016994 | val_loss=6.2757 | ema=6.2992 | lr=0.00003 | pred = 36.5323\n",
      "Epoch 052 | train_loss=0.012732 | val_loss=6.2822 | ema=6.2970 | lr=0.00002 | pred = 36.5673\n",
      "Epoch 056 | train_loss=0.011435 | val_loss=6.3114 | ema=6.3021 | lr=0.00001 | pred = 36.7246\n",
      "Epoch 060 | train_loss=0.013085 | val_loss=6.3043 | ema=6.3038 | lr=0.00000 | pred = 36.6862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  60%|██████    | 6/10 [41:02<27:12, 408.21s/it, last_mae_pred=36.6947, last_mae_dap=36.2994, avg_mae_pred=36.5750, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.015505 | val_loss=6.3059 | ema=6.3052 | lr=0.00000 | pred = 36.6947\n",
      "pred = 36.6947, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.6947\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.6947\n",
      "\n",
      "[36.6267, 36.7246, 36.7052, 36.7003, 36.6774, 36.6862, 36.6974, 36.6984, 36.6957, 36.6947]  | mean=36.6907 | std=0.0256\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.494476 | val_loss=0.2990 | ema=0.2990 | lr=0.00010 | pred = 34.2217\n",
      "Epoch 004 | train_loss=0.120619 | val_loss=0.1914 | ema=0.2291 | lr=0.00025 | pred = 39.6605\n",
      "Epoch 008 | train_loss=0.088555 | val_loss=0.0648 | ema=0.1326 | lr=0.00045 | pred = 44.1478\n",
      "Epoch 012 | train_loss=0.031865 | val_loss=0.0249 | ema=0.0788 | lr=0.00060 | pred = 43.0365\n",
      "Epoch 016 | train_loss=0.054908 | val_loss=0.0325 | ema=0.0480 | lr=0.00059 | pred = 39.8772\n",
      "Epoch 020 | train_loss=0.030194 | val_loss=0.0342 | ema=0.0429 | lr=0.00057 | pred = 39.8310\n",
      "Epoch 024 | train_loss=0.031226 | val_loss=0.0179 | ema=0.0367 | lr=0.00052 | pred = 41.7353\n",
      "Epoch 028 | train_loss=0.022922 | val_loss=0.0217 | ema=0.0271 | lr=0.00047 | pred = 40.5669\n",
      "Epoch 032 | train_loss=0.019482 | val_loss=0.0182 | ema=0.0203 | lr=0.00041 | pred = 41.0922\n",
      "Epoch 036 | train_loss=0.020217 | val_loss=0.0204 | ema=0.0210 | lr=0.00034 | pred = 40.0332\n",
      "Epoch 040 | train_loss=0.025299 | val_loss=0.0194 | ema=0.0207 | lr=0.00026 | pred = 40.5113\n",
      "Epoch 044 | train_loss=0.013709 | val_loss=0.0138 | ema=0.0174 | lr=0.00019 | pred = 40.9585\n",
      "Epoch 048 | train_loss=0.020382 | val_loss=0.0190 | ema=0.0185 | lr=0.00013 | pred = 40.5097\n",
      "Epoch 052 | train_loss=0.014125 | val_loss=0.0175 | ema=0.0187 | lr=0.00008 | pred = 40.5473\n",
      "Epoch 056 | train_loss=0.011041 | val_loss=0.0154 | ema=0.0177 | lr=0.00003 | pred = 40.7681\n",
      "Epoch 060 | train_loss=0.014334 | val_loss=0.0154 | ema=0.0165 | lr=0.00001 | pred = 40.7292\n",
      "Epoch 064 | train_loss=0.015872 | val_loss=0.0162 | ema=0.0163 | lr=0.00000 | pred = 40.6178\n",
      "pred = 35.0239, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 35.0239\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.7978\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.023915 | val_loss=6.2446 | ema=6.2446 | lr=0.00002 | pred = 36.3641\n",
      "Epoch 004 | train_loss=0.024487 | val_loss=6.1825 | ema=6.2210 | lr=0.00005 | pred = 36.0295\n",
      "Epoch 008 | train_loss=0.016406 | val_loss=6.2584 | ema=6.2253 | lr=0.00009 | pred = 36.4385\n",
      "Epoch 012 | train_loss=0.019585 | val_loss=6.3105 | ema=6.2401 | lr=0.00012 | pred = 36.7199\n",
      "Epoch 016 | train_loss=0.019907 | val_loss=6.2899 | ema=6.2480 | lr=0.00012 | pred = 36.6088\n",
      "Epoch 020 | train_loss=0.015621 | val_loss=6.2447 | ema=6.2652 | lr=0.00011 | pred = 36.3647\n",
      "Epoch 024 | train_loss=0.018336 | val_loss=6.2866 | ema=6.2715 | lr=0.00010 | pred = 36.5909\n",
      "Epoch 028 | train_loss=0.015929 | val_loss=6.2898 | ema=6.2681 | lr=0.00009 | pred = 36.6078\n",
      "Epoch 032 | train_loss=0.016553 | val_loss=6.2895 | ema=6.2751 | lr=0.00008 | pred = 36.6067\n",
      "Epoch 036 | train_loss=0.015572 | val_loss=6.2763 | ema=6.2874 | lr=0.00007 | pred = 36.5353\n",
      "Epoch 040 | train_loss=0.012700 | val_loss=6.3316 | ema=6.3073 | lr=0.00005 | pred = 36.8334\n",
      "Epoch 044 | train_loss=0.013827 | val_loss=6.3046 | ema=6.2935 | lr=0.00004 | pred = 36.6877\n",
      "Epoch 048 | train_loss=0.017422 | val_loss=6.2718 | ema=6.2977 | lr=0.00003 | pred = 36.5111\n",
      "Epoch 052 | train_loss=0.012117 | val_loss=6.2912 | ema=6.2940 | lr=0.00002 | pred = 36.6155\n",
      "Epoch 056 | train_loss=0.013844 | val_loss=6.3076 | ema=6.2971 | lr=0.00001 | pred = 36.7038\n",
      "Epoch 060 | train_loss=0.013137 | val_loss=6.2945 | ema=6.3000 | lr=0.00000 | pred = 36.6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  70%|███████   | 7/10 [47:43<20:17, 405.79s/it, last_mae_pred=36.6517, last_mae_dap=36.2994, avg_mae_pred=36.5860, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.018292 | val_loss=6.2979 | ema=6.2986 | lr=0.00000 | pred = 36.6517\n",
      "pred = 36.6517, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.6517\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.6517\n",
      "\n",
      "[36.5913, 36.7038, 36.723, 36.7058, 36.6677, 36.6336, 36.6425, 36.6522, 36.6518, 36.6517]  | mean=36.6623 | std=0.0393\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.496214 | val_loss=0.2987 | ema=0.2987 | lr=0.00008 | pred = 34.1652\n",
      "Epoch 004 | train_loss=0.124521 | val_loss=0.1616 | ema=0.2247 | lr=0.00021 | pred = 38.9771\n",
      "Epoch 008 | train_loss=0.115317 | val_loss=0.1030 | ema=0.1564 | lr=0.00038 | pred = 42.4205\n",
      "Epoch 012 | train_loss=0.036835 | val_loss=0.0234 | ema=0.0956 | lr=0.00050 | pred = 41.7253\n",
      "Epoch 016 | train_loss=0.051679 | val_loss=0.0384 | ema=0.0582 | lr=0.00049 | pred = 42.5469\n",
      "Epoch 020 | train_loss=0.034634 | val_loss=0.0397 | ema=0.0449 | lr=0.00047 | pred = 39.7414\n",
      "Epoch 024 | train_loss=0.031316 | val_loss=0.0271 | ema=0.0325 | lr=0.00044 | pred = 41.9258\n",
      "Epoch 028 | train_loss=0.022511 | val_loss=0.0175 | ema=0.0228 | lr=0.00039 | pred = 40.9481\n",
      "Epoch 032 | train_loss=0.019848 | val_loss=0.0135 | ema=0.0171 | lr=0.00034 | pred = 41.2726\n",
      "Epoch 036 | train_loss=0.019455 | val_loss=0.0127 | ema=0.0167 | lr=0.00028 | pred = 40.9640\n",
      "Epoch 040 | train_loss=0.020717 | val_loss=0.0143 | ema=0.0157 | lr=0.00022 | pred = 41.3278\n",
      "Epoch 044 | train_loss=0.012208 | val_loss=0.0165 | ema=0.0172 | lr=0.00016 | pred = 41.6784\n",
      "Epoch 048 | train_loss=0.022363 | val_loss=0.0165 | ema=0.0181 | lr=0.00011 | pred = 40.7653\n",
      "Epoch 052 | train_loss=0.014704 | val_loss=0.0129 | ema=0.0154 | lr=0.00006 | pred = 41.4001\n",
      "Epoch 056 | train_loss=0.013677 | val_loss=0.0135 | ema=0.0146 | lr=0.00003 | pred = 41.2990\n",
      "Epoch 060 | train_loss=0.016853 | val_loss=0.0134 | ema=0.0138 | lr=0.00001 | pred = 41.3428\n",
      "Epoch 064 | train_loss=0.016818 | val_loss=0.0136 | ema=0.0136 | lr=0.00000 | pred = 41.3416\n",
      "pred = 35.3707, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 35.3707\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.6799\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.022792 | val_loss=6.3094 | ema=6.3094 | lr=0.00002 | pred = 36.7137\n",
      "Epoch 004 | train_loss=0.021320 | val_loss=6.2763 | ema=6.2919 | lr=0.00004 | pred = 36.5354\n",
      "Epoch 008 | train_loss=0.017985 | val_loss=6.3215 | ema=6.2883 | lr=0.00008 | pred = 36.7788\n",
      "Epoch 012 | train_loss=0.020937 | val_loss=6.3417 | ema=6.2901 | lr=0.00010 | pred = 36.8879\n",
      "Epoch 016 | train_loss=0.020480 | val_loss=6.3437 | ema=6.3064 | lr=0.00010 | pred = 36.8986\n",
      "Epoch 020 | train_loss=0.017010 | val_loss=6.3335 | ema=6.3229 | lr=0.00009 | pred = 36.8436\n",
      "Epoch 024 | train_loss=0.019201 | val_loss=6.3856 | ema=6.3379 | lr=0.00009 | pred = 37.1246\n",
      "Epoch 028 | train_loss=0.015958 | val_loss=6.3688 | ema=6.3408 | lr=0.00008 | pred = 37.0339\n",
      "Epoch 032 | train_loss=0.019386 | val_loss=6.3692 | ema=6.3590 | lr=0.00007 | pred = 37.0360\n",
      "Epoch 036 | train_loss=0.017255 | val_loss=6.3207 | ema=6.3477 | lr=0.00006 | pred = 36.7747\n",
      "Epoch 040 | train_loss=0.015092 | val_loss=6.4221 | ema=6.3653 | lr=0.00004 | pred = 37.3214\n",
      "Epoch 044 | train_loss=0.015068 | val_loss=6.3394 | ema=6.3599 | lr=0.00003 | pred = 36.8753\n",
      "Epoch 048 | train_loss=0.017695 | val_loss=6.3550 | ema=6.3631 | lr=0.00002 | pred = 36.9597\n",
      "Epoch 052 | train_loss=0.013363 | val_loss=6.3427 | ema=6.3548 | lr=0.00001 | pred = 36.8933\n",
      "Epoch 056 | train_loss=0.016245 | val_loss=6.3654 | ema=6.3607 | lr=0.00001 | pred = 37.0156\n",
      "Epoch 060 | train_loss=0.014105 | val_loss=6.3620 | ema=6.3635 | lr=0.00000 | pred = 36.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  80%|████████  | 8/10 [54:25<13:29, 404.60s/it, last_mae_pred=36.9879, last_mae_dap=36.2994, avg_mae_pred=36.6362, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.017508 | val_loss=6.3603 | ema=6.3617 | lr=0.00000 | pred = 36.9879\n",
      "pred = 36.9879, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.9879\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.9879\n",
      "\n",
      "[36.979, 37.0156, 37.0246, 37.0382, 37.0148, 36.9971, 36.9925, 36.9897, 36.9876, 36.9879]  | mean=37.0027 | std=0.0193\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.497861 | val_loss=0.3031 | ema=0.3031 | lr=0.00007 | pred = 34.1282\n",
      "Epoch 004 | train_loss=0.167555 | val_loss=0.1111 | ema=0.2233 | lr=0.00017 | pred = 38.0643\n",
      "Epoch 008 | train_loss=0.133991 | val_loss=0.0926 | ema=0.1444 | lr=0.00030 | pred = 42.5749\n",
      "Epoch 012 | train_loss=0.035983 | val_loss=0.0513 | ema=0.0945 | lr=0.00040 | pred = 43.9516\n",
      "Epoch 016 | train_loss=0.053832 | val_loss=0.0180 | ema=0.0576 | lr=0.00039 | pred = 41.0765\n",
      "Epoch 020 | train_loss=0.029078 | val_loss=0.0223 | ema=0.0404 | lr=0.00038 | pred = 40.0889\n",
      "Epoch 024 | train_loss=0.030691 | val_loss=0.0155 | ema=0.0270 | lr=0.00035 | pred = 41.1546\n",
      "Epoch 028 | train_loss=0.022000 | val_loss=0.0224 | ema=0.0225 | lr=0.00031 | pred = 40.2935\n",
      "Epoch 032 | train_loss=0.023920 | val_loss=0.0394 | ema=0.0232 | lr=0.00027 | pred = 41.8062\n",
      "Epoch 036 | train_loss=0.022073 | val_loss=0.0185 | ema=0.0216 | lr=0.00022 | pred = 40.3819\n",
      "Epoch 040 | train_loss=0.026389 | val_loss=0.0214 | ema=0.0221 | lr=0.00018 | pred = 39.8818\n",
      "Epoch 044 | train_loss=0.014788 | val_loss=0.0201 | ema=0.0221 | lr=0.00013 | pred = 41.3621\n",
      "Epoch 048 | train_loss=0.021468 | val_loss=0.0276 | ema=0.0242 | lr=0.00009 | pred = 40.4806\n",
      "Epoch 052 | train_loss=0.018031 | val_loss=0.0204 | ema=0.0231 | lr=0.00005 | pred = 40.8664\n",
      "Epoch 056 | train_loss=0.013119 | val_loss=0.0201 | ema=0.0223 | lr=0.00002 | pred = 41.0427\n",
      "Epoch 060 | train_loss=0.017716 | val_loss=0.0207 | ema=0.0207 | lr=0.00001 | pred = 40.9491\n",
      "Epoch 064 | train_loss=0.015950 | val_loss=0.0206 | ema=0.0207 | lr=0.00000 | pred = 40.8747\n",
      "pred = 34.8588, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 34.8588\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.9152\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.022694 | val_loss=6.2561 | ema=6.2561 | lr=0.00001 | pred = 36.4263\n",
      "Epoch 004 | train_loss=0.026783 | val_loss=6.2192 | ema=6.2296 | lr=0.00003 | pred = 36.2272\n",
      "Epoch 008 | train_loss=0.016274 | val_loss=6.2709 | ema=6.2204 | lr=0.00006 | pred = 36.5062\n",
      "Epoch 012 | train_loss=0.019607 | val_loss=6.3303 | ema=6.2441 | lr=0.00008 | pred = 36.8262\n",
      "Epoch 016 | train_loss=0.022844 | val_loss=6.3133 | ema=6.2669 | lr=0.00008 | pred = 36.7346\n",
      "Epoch 020 | train_loss=0.017923 | val_loss=6.3090 | ema=6.2993 | lr=0.00008 | pred = 36.7113\n",
      "Epoch 024 | train_loss=0.021362 | val_loss=6.3418 | ema=6.3137 | lr=0.00007 | pred = 36.8884\n",
      "Epoch 028 | train_loss=0.015007 | val_loss=6.3160 | ema=6.3066 | lr=0.00006 | pred = 36.7494\n",
      "Epoch 032 | train_loss=0.019218 | val_loss=6.3163 | ema=6.3207 | lr=0.00005 | pred = 36.7510\n",
      "Epoch 036 | train_loss=0.016294 | val_loss=6.3035 | ema=6.3188 | lr=0.00004 | pred = 36.6817\n",
      "Epoch 040 | train_loss=0.015256 | val_loss=6.3992 | ema=6.3456 | lr=0.00004 | pred = 37.1977\n",
      "Epoch 044 | train_loss=0.017186 | val_loss=6.3290 | ema=6.3347 | lr=0.00003 | pred = 36.8195\n",
      "Epoch 048 | train_loss=0.017270 | val_loss=6.3414 | ema=6.3456 | lr=0.00002 | pred = 36.8863\n",
      "Epoch 052 | train_loss=0.015367 | val_loss=6.3504 | ema=6.3476 | lr=0.00001 | pred = 36.9349\n",
      "Epoch 056 | train_loss=0.014258 | val_loss=6.3597 | ema=6.3515 | lr=0.00000 | pred = 36.9850\n",
      "Epoch 060 | train_loss=0.015830 | val_loss=6.3512 | ema=6.3548 | lr=0.00000 | pred = 36.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast:  90%|█████████ | 9/10 [1:01:08<06:43, 403.95s/it, last_mae_pred=36.9453, last_mae_dap=36.2994, avg_mae_pred=36.6706, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.018663 | val_loss=6.3524 | ema=6.3533 | lr=0.00000 | pred = 36.9453\n",
      "pred = 36.9453, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.9453\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.9453\n",
      "\n",
      "[36.9137, 36.985, 37.0252, 36.9929, 36.9579, 36.9392, 36.9428, 36.9455, 36.9451, 36.9453]  | mean=36.9593 | std=0.0325\n",
      "\n",
      "Dates: 2025-11-11 00:00:00 → 2025-11-12 00:00:00 | interval_days: 1 | time_shift: 2\n",
      "num of splits = 2\n",
      "size of df: (931, 107)\n",
      "\n",
      "Fold 1: train 835, val 93 \n",
      "train first...last : 2022-03-23 ... 2025-07-01 \n",
      "val first...last   : 2025-07-03 ... 2025-11-10\n",
      "train last row | DAP=39.4764 | Target=40.7472 | Target_smooth=39.5991\n",
      "val last row   | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "Epoch 001 | train_loss=0.499901 | val_loss=0.3166 | ema=0.3166 | lr=0.00005 | pred = 33.9467\n",
      "Epoch 004 | train_loss=0.231702 | val_loss=0.1074 | ema=0.2449 | lr=0.00013 | pred = 37.0653\n",
      "Epoch 008 | train_loss=0.148661 | val_loss=0.0922 | ema=0.1680 | lr=0.00022 | pred = 41.8339\n",
      "Epoch 012 | train_loss=0.045278 | val_loss=0.0914 | ema=0.1466 | lr=0.00030 | pred = 43.4034\n",
      "Epoch 016 | train_loss=0.049731 | val_loss=0.0778 | ema=0.1312 | lr=0.00030 | pred = 45.1689\n",
      "Epoch 020 | train_loss=0.041164 | val_loss=0.0499 | ema=0.0770 | lr=0.00028 | pred = 40.1443\n",
      "Epoch 024 | train_loss=0.031120 | val_loss=0.0309 | ema=0.0451 | lr=0.00026 | pred = 40.8204\n",
      "Epoch 028 | train_loss=0.027806 | val_loss=0.0188 | ema=0.0323 | lr=0.00024 | pred = 39.8562\n",
      "Epoch 032 | train_loss=0.029608 | val_loss=0.0168 | ema=0.0231 | lr=0.00020 | pred = 41.5962\n",
      "Epoch 036 | train_loss=0.026518 | val_loss=0.0211 | ema=0.0215 | lr=0.00017 | pred = 40.6504\n",
      "Epoch 040 | train_loss=0.024493 | val_loss=0.0164 | ema=0.0200 | lr=0.00013 | pred = 40.6745\n",
      "Epoch 044 | train_loss=0.016844 | val_loss=0.0146 | ema=0.0187 | lr=0.00010 | pred = 40.8933\n",
      "Epoch 048 | train_loss=0.020016 | val_loss=0.0235 | ema=0.0208 | lr=0.00006 | pred = 41.3484\n",
      "Epoch 052 | train_loss=0.018102 | val_loss=0.0164 | ema=0.0179 | lr=0.00004 | pred = 41.1708\n",
      "Epoch 056 | train_loss=0.017037 | val_loss=0.0183 | ema=0.0180 | lr=0.00002 | pred = 41.2220\n",
      "Epoch 060 | train_loss=0.019309 | val_loss=0.0162 | ema=0.0164 | lr=0.00000 | pred = 41.1636\n",
      "Epoch 064 | train_loss=0.020006 | val_loss=0.0163 | ema=0.0164 | lr=0.00000 | pred = 41.1513\n",
      "pred = 35.2752, 72\n",
      "[Fold 1] Target = 36.2994 | Predict = 35.2752\n",
      "MAE(Target~DAP) = 0.5732 | MAE(Target~Predict) = 0.7454\n",
      "\n",
      "\n",
      "Fold 2: train 929, val 22 \n",
      "train first...last : 2022-03-23 ... 2025-11-10 \n",
      "val first...last   : 2025-10-14 ... 2025-11-12\n",
      "train last row | DAP=35.7539 | Target=36.2994 | Target_smooth=35.3047\n",
      "val last row   | DAP=36.2994 | Target=0.0000 | Target_smooth=35.8574\n",
      "Epoch 001 | train_loss=0.023198 | val_loss=6.2586 | ema=6.2586 | lr=0.00001 | pred = 36.4398\n",
      "Epoch 004 | train_loss=0.029482 | val_loss=6.2289 | ema=6.2490 | lr=0.00003 | pred = 36.2795\n",
      "Epoch 008 | train_loss=0.024636 | val_loss=6.2182 | ema=6.2196 | lr=0.00005 | pred = 36.2222\n",
      "Epoch 012 | train_loss=0.023002 | val_loss=6.2231 | ema=6.2170 | lr=0.00006 | pred = 36.2483\n",
      "Epoch 016 | train_loss=0.025808 | val_loss=6.2755 | ema=6.2371 | lr=0.00006 | pred = 36.5309\n",
      "Epoch 020 | train_loss=0.023276 | val_loss=6.3063 | ema=6.2448 | lr=0.00006 | pred = 36.6968\n",
      "Epoch 024 | train_loss=0.019263 | val_loss=6.2668 | ema=6.2411 | lr=0.00005 | pred = 36.4840\n",
      "Epoch 028 | train_loss=0.019526 | val_loss=6.2176 | ema=6.2353 | lr=0.00005 | pred = 36.2188\n",
      "Epoch 032 | train_loss=0.021455 | val_loss=6.2601 | ema=6.2412 | lr=0.00004 | pred = 36.4479\n",
      "Epoch 036 | train_loss=0.019773 | val_loss=6.1991 | ema=6.2443 | lr=0.00003 | pred = 36.1193\n",
      "Epoch 040 | train_loss=0.018022 | val_loss=6.2562 | ema=6.2461 | lr=0.00003 | pred = 36.4268\n",
      "Epoch 044 | train_loss=0.019783 | val_loss=6.2118 | ema=6.2326 | lr=0.00002 | pred = 36.1875\n",
      "Epoch 048 | train_loss=0.022038 | val_loss=6.2396 | ema=6.2383 | lr=0.00001 | pred = 36.3372\n",
      "Epoch 052 | train_loss=0.019168 | val_loss=6.2222 | ema=6.2385 | lr=0.00001 | pred = 36.2438\n",
      "Epoch 056 | train_loss=0.016163 | val_loss=6.2541 | ema=6.2426 | lr=0.00000 | pred = 36.4154\n",
      "Epoch 060 | train_loss=0.016631 | val_loss=6.2349 | ema=6.2434 | lr=0.00000 | pred = 36.3122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling forecast: 100%|██████████| 10/10 [1:07:49<00:00, 406.96s/it, last_mae_pred=36.3090, last_mae_dap=36.2994, avg_mae_pred=36.6344, avg_mae_dap=36.2994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | train_loss=0.020220 | val_loss=6.2343 | ema=6.2380 | lr=0.00000 | pred = 36.3090\n",
      "pred = 36.3090, 1\n",
      "[Fold 2] Target = 0.0000 | Predict = 36.3090\n",
      "MAE(Target~DAP) = 36.2994 | MAE(Target~Predict) = 36.3090\n",
      "\n",
      "[36.3675, 36.4154, 36.4404, 36.3996, 36.3399, 36.3122, 36.311, 36.309, 36.3079, 36.309]  | mean=36.3512 | std=0.0510\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time_shift</th>\n",
       "      <th>window_size</th>\n",
       "      <th>lr_lambda</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dropout</th>\n",
       "      <th>n_res_blocks</th>\n",
       "      <th>PL10_mean_mean</th>\n",
       "      <th>PL10_mean_std</th>\n",
       "      <th>PL10_std_mean</th>\n",
       "      <th>PL10_std_std</th>\n",
       "      <th>0.0030_mean</th>\n",
       "      <th>0.0020_mean</th>\n",
       "      <th>0.0010_mean</th>\n",
       "      <th>0.0009_mean</th>\n",
       "      <th>0.0008_mean</th>\n",
       "      <th>0.0007_mean</th>\n",
       "      <th>0.0006_mean</th>\n",
       "      <th>0.0005_mean</th>\n",
       "      <th>0.0004_mean</th>\n",
       "      <th>0.0003_mean</th>\n",
       "      <th>0.0030_std</th>\n",
       "      <th>0.0020_std</th>\n",
       "      <th>0.0010_std</th>\n",
       "      <th>0.0009_std</th>\n",
       "      <th>0.0008_std</th>\n",
       "      <th>0.0007_std</th>\n",
       "      <th>0.0006_std</th>\n",
       "      <th>0.0005_std</th>\n",
       "      <th>0.0004_std</th>\n",
       "      <th>0.0003_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>44</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.04296</td>\n",
       "      <td>0.296592</td>\n",
       "      <td>0.03677</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>33.8967</td>\n",
       "      <td>33.7666</td>\n",
       "      <td>34.1048</td>\n",
       "      <td>34.4722</td>\n",
       "      <td>34.3281</td>\n",
       "      <td>34.1199</td>\n",
       "      <td>34.1343</td>\n",
       "      <td>33.9614</td>\n",
       "      <td>34.2144</td>\n",
       "      <td>33.4312</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>52</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.08798</td>\n",
       "      <td>0.101572</td>\n",
       "      <td>0.02693</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>33.9794</td>\n",
       "      <td>34.2996</td>\n",
       "      <td>34.1232</td>\n",
       "      <td>34.1260</td>\n",
       "      <td>34.0834</td>\n",
       "      <td>34.0315</td>\n",
       "      <td>34.0361</td>\n",
       "      <td>34.1098</td>\n",
       "      <td>34.1540</td>\n",
       "      <td>33.9368</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.52822</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>34.8397</td>\n",
       "      <td>35.0390</td>\n",
       "      <td>35.1257</td>\n",
       "      <td>34.1310</td>\n",
       "      <td>34.6296</td>\n",
       "      <td>34.7337</td>\n",
       "      <td>34.0210</td>\n",
       "      <td>34.6563</td>\n",
       "      <td>34.0728</td>\n",
       "      <td>34.0334</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>0.0664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>33.41389</td>\n",
       "      <td>0.592093</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.085554</td>\n",
       "      <td>33.9495</td>\n",
       "      <td>34.1444</td>\n",
       "      <td>33.5586</td>\n",
       "      <td>33.4852</td>\n",
       "      <td>32.9203</td>\n",
       "      <td>34.1612</td>\n",
       "      <td>33.1922</td>\n",
       "      <td>33.3552</td>\n",
       "      <td>33.1304</td>\n",
       "      <td>32.2419</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>34.01628</td>\n",
       "      <td>0.334351</td>\n",
       "      <td>0.07227</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>34.3612</td>\n",
       "      <td>34.4437</td>\n",
       "      <td>34.0495</td>\n",
       "      <td>33.9382</td>\n",
       "      <td>33.3032</td>\n",
       "      <td>34.1230</td>\n",
       "      <td>33.9392</td>\n",
       "      <td>33.9461</td>\n",
       "      <td>33.7411</td>\n",
       "      <td>34.3176</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.09213</td>\n",
       "      <td>0.271552</td>\n",
       "      <td>0.04879</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>34.5063</td>\n",
       "      <td>34.4195</td>\n",
       "      <td>34.0148</td>\n",
       "      <td>33.8809</td>\n",
       "      <td>33.9621</td>\n",
       "      <td>33.8735</td>\n",
       "      <td>33.8598</td>\n",
       "      <td>33.7669</td>\n",
       "      <td>34.3787</td>\n",
       "      <td>34.2588</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.03229</td>\n",
       "      <td>0.147213</td>\n",
       "      <td>0.04455</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>33.8739</td>\n",
       "      <td>33.8561</td>\n",
       "      <td>34.0091</td>\n",
       "      <td>34.1713</td>\n",
       "      <td>34.1532</td>\n",
       "      <td>33.8691</td>\n",
       "      <td>34.1332</td>\n",
       "      <td>33.9426</td>\n",
       "      <td>34.0381</td>\n",
       "      <td>34.2763</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.20160</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.8877</td>\n",
       "      <td>33.8170</td>\n",
       "      <td>34.3975</td>\n",
       "      <td>34.1070</td>\n",
       "      <td>34.3238</td>\n",
       "      <td>34.2922</td>\n",
       "      <td>34.3360</td>\n",
       "      <td>34.2902</td>\n",
       "      <td>34.3743</td>\n",
       "      <td>34.1906</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>33.77220</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.09480</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>33.7382</td>\n",
       "      <td>33.8192</td>\n",
       "      <td>33.5882</td>\n",
       "      <td>33.7218</td>\n",
       "      <td>33.4791</td>\n",
       "      <td>33.9450</td>\n",
       "      <td>33.9421</td>\n",
       "      <td>33.8468</td>\n",
       "      <td>33.8459</td>\n",
       "      <td>33.7960</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>60</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.16810</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.05370</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.9715</td>\n",
       "      <td>34.0325</td>\n",
       "      <td>34.5261</td>\n",
       "      <td>34.3204</td>\n",
       "      <td>34.4775</td>\n",
       "      <td>34.1714</td>\n",
       "      <td>34.3446</td>\n",
       "      <td>33.7550</td>\n",
       "      <td>33.9080</td>\n",
       "      <td>34.1736</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.02170</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.02940</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>33.8616</td>\n",
       "      <td>33.6905</td>\n",
       "      <td>34.1431</td>\n",
       "      <td>34.0800</td>\n",
       "      <td>34.0269</td>\n",
       "      <td>34.1495</td>\n",
       "      <td>34.0671</td>\n",
       "      <td>34.1982</td>\n",
       "      <td>34.0890</td>\n",
       "      <td>33.9112</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>33.95390</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>33.6247</td>\n",
       "      <td>34.0442</td>\n",
       "      <td>34.1751</td>\n",
       "      <td>34.0652</td>\n",
       "      <td>33.6538</td>\n",
       "      <td>34.0870</td>\n",
       "      <td>33.9074</td>\n",
       "      <td>34.0052</td>\n",
       "      <td>33.8273</td>\n",
       "      <td>34.1494</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.40610</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.06490</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>34.8906</td>\n",
       "      <td>35.8110</td>\n",
       "      <td>34.8277</td>\n",
       "      <td>35.1031</td>\n",
       "      <td>35.3820</td>\n",
       "      <td>35.3879</td>\n",
       "      <td>35.0688</td>\n",
       "      <td>35.8009</td>\n",
       "      <td>35.7868</td>\n",
       "      <td>36.0026</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.0748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.42660</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.06220</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>36.2859</td>\n",
       "      <td>35.4809</td>\n",
       "      <td>34.7119</td>\n",
       "      <td>35.2623</td>\n",
       "      <td>35.0421</td>\n",
       "      <td>35.1546</td>\n",
       "      <td>35.3421</td>\n",
       "      <td>34.9806</td>\n",
       "      <td>36.9482</td>\n",
       "      <td>35.0570</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.42250</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.02160</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>35.6900</td>\n",
       "      <td>35.4300</td>\n",
       "      <td>35.7616</td>\n",
       "      <td>35.6318</td>\n",
       "      <td>35.3681</td>\n",
       "      <td>35.4666</td>\n",
       "      <td>35.5247</td>\n",
       "      <td>35.2215</td>\n",
       "      <td>35.0339</td>\n",
       "      <td>35.0971</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.00700</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.01540</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>35.1352</td>\n",
       "      <td>35.0797</td>\n",
       "      <td>34.9388</td>\n",
       "      <td>35.0725</td>\n",
       "      <td>35.2059</td>\n",
       "      <td>34.7787</td>\n",
       "      <td>34.8483</td>\n",
       "      <td>35.0720</td>\n",
       "      <td>34.9944</td>\n",
       "      <td>34.9447</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.74070</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.02400</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>35.7633</td>\n",
       "      <td>35.4769</td>\n",
       "      <td>35.6850</td>\n",
       "      <td>35.3690</td>\n",
       "      <td>35.8015</td>\n",
       "      <td>36.3752</td>\n",
       "      <td>35.0792</td>\n",
       "      <td>36.0552</td>\n",
       "      <td>35.7728</td>\n",
       "      <td>36.0289</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.10310</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>35.9709</td>\n",
       "      <td>36.0299</td>\n",
       "      <td>35.8589</td>\n",
       "      <td>36.1879</td>\n",
       "      <td>35.8114</td>\n",
       "      <td>36.1512</td>\n",
       "      <td>36.2648</td>\n",
       "      <td>36.2854</td>\n",
       "      <td>36.2138</td>\n",
       "      <td>36.2563</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.94890</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.02310</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>35.8952</td>\n",
       "      <td>36.0750</td>\n",
       "      <td>35.8183</td>\n",
       "      <td>36.1429</td>\n",
       "      <td>36.3012</td>\n",
       "      <td>36.1458</td>\n",
       "      <td>36.0727</td>\n",
       "      <td>35.5236</td>\n",
       "      <td>36.0581</td>\n",
       "      <td>35.4566</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.85040</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.03040</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>35.9462</td>\n",
       "      <td>36.2186</td>\n",
       "      <td>36.3577</td>\n",
       "      <td>36.4732</td>\n",
       "      <td>35.6464</td>\n",
       "      <td>35.5069</td>\n",
       "      <td>35.6691</td>\n",
       "      <td>35.3255</td>\n",
       "      <td>34.9703</td>\n",
       "      <td>36.3903</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.06580</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>37.4658</td>\n",
       "      <td>35.7302</td>\n",
       "      <td>35.6559</td>\n",
       "      <td>35.9850</td>\n",
       "      <td>36.3132</td>\n",
       "      <td>35.6608</td>\n",
       "      <td>36.2174</td>\n",
       "      <td>36.0467</td>\n",
       "      <td>36.2851</td>\n",
       "      <td>35.2974</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.1231</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.1557</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.83560</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>36.5582</td>\n",
       "      <td>36.5673</td>\n",
       "      <td>36.6765</td>\n",
       "      <td>36.7727</td>\n",
       "      <td>36.6858</td>\n",
       "      <td>36.9320</td>\n",
       "      <td>36.8448</td>\n",
       "      <td>37.0639</td>\n",
       "      <td>37.2044</td>\n",
       "      <td>37.0501</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.50060</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.01570</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>36.5280</td>\n",
       "      <td>36.2985</td>\n",
       "      <td>36.4604</td>\n",
       "      <td>36.2603</td>\n",
       "      <td>36.5788</td>\n",
       "      <td>36.4379</td>\n",
       "      <td>36.7048</td>\n",
       "      <td>36.6045</td>\n",
       "      <td>36.2259</td>\n",
       "      <td>36.9071</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.64240</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.03690</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>36.4319</td>\n",
       "      <td>36.3303</td>\n",
       "      <td>36.3830</td>\n",
       "      <td>36.8548</td>\n",
       "      <td>36.7580</td>\n",
       "      <td>36.6907</td>\n",
       "      <td>36.6623</td>\n",
       "      <td>37.0027</td>\n",
       "      <td>36.9593</td>\n",
       "      <td>36.3512</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.0510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  time_shift  window_size lr_lambda  epochs  dropout  \\\n",
       "0   2025-11-09           3           92       cos      44      0.3   \n",
       "1   2025-11-09           2           20       cos      52      0.3   \n",
       "2   2025-11-09           4           61       cos      36      0.3   \n",
       "3   2025-11-09           5           92       cos      36      0.3   \n",
       "4   2025-11-09           2           61      _cos      40      0.3   \n",
       "5   2025-11-09           1           61      _cos      40      0.4   \n",
       "6   2025-11-07           1           20       cos      48      0.4   \n",
       "7   2025-11-07           2           34       cos      48      0.4   \n",
       "8   2025-11-07           3           34       cos      48      0.4   \n",
       "9   2025-11-07           2           34       cos      60      0.3   \n",
       "10  2025-11-07           1           34       cos      64      0.3   \n",
       "11  2025-11-07           3           34       cos      64      0.4   \n",
       "12  2025-11-10           1           50      _cos      64      0.4   \n",
       "13  2025-11-10           1           50      _cos      64      0.4   \n",
       "14  2025-11-10           1          100       cos      64      0.4   \n",
       "15  2025-11-10           2          100       cos      64      0.4   \n",
       "16  2025-11-11           1           50       cos      64      0.4   \n",
       "17  2025-11-11           1           21       cos      64      0.4   \n",
       "18  2025-11-11           1          100       cos      64      0.4   \n",
       "19  2025-11-11           3           50       cos      64      0.4   \n",
       "20  2025-11-11           3           50      _cos      64      0.4   \n",
       "21  2025-11-12           1           21       cos      64      0.4   \n",
       "22  2025-11-12           1          100       cos      64      0.4   \n",
       "23  2025-11-12           2           21       cos      64      0.4   \n",
       "\n",
       "    n_res_blocks  PL10_mean_mean  PL10_mean_std  PL10_std_mean  PL10_std_std  \\\n",
       "0              3        34.04296       0.296592        0.03677      0.032337   \n",
       "1              3        34.08798       0.101572        0.02693      0.010317   \n",
       "2              1        34.52822       0.428319        0.07696      0.025820   \n",
       "3              3        33.41389       0.592093        0.13273      0.085554   \n",
       "4              2        34.01628       0.334351        0.07227      0.023782   \n",
       "5              2        34.09213       0.271552        0.04879      0.013215   \n",
       "6              2        34.03229       0.147213        0.04455      0.014940   \n",
       "7              2        34.20160       0.203500        0.03720      0.024600   \n",
       "8              2        33.77220       0.147600        0.09480      0.032400   \n",
       "9              3        34.16810       0.252500        0.05370      0.024600   \n",
       "10             3        34.02170       0.156300        0.02940      0.015400   \n",
       "11             3        33.95390       0.195700        0.03610      0.019900   \n",
       "12             3        35.40610       0.425200        0.06490      0.028400   \n",
       "13             3        35.42660       0.679800        0.06220      0.028500   \n",
       "14             2        35.42250       0.245600        0.02160      0.009300   \n",
       "15             2        35.00700       0.131600        0.01540      0.007600   \n",
       "16             2        35.74070       0.370900        0.02400      0.012900   \n",
       "17             2        36.10310       0.173900        0.03080      0.007500   \n",
       "18             2        35.94890       0.276300        0.02310      0.008400   \n",
       "19             2        35.85040       0.508200        0.03040      0.014800   \n",
       "20             2        36.06580       0.590100        0.10110      0.035500   \n",
       "21             2        36.83560       0.222200        0.01770      0.011500   \n",
       "22             2        36.50060       0.211900        0.01570      0.008800   \n",
       "23             2        36.64240       0.255000        0.03690      0.011500   \n",
       "\n",
       "    0.0030_mean  0.0020_mean  0.0010_mean  0.0009_mean  0.0008_mean  \\\n",
       "0       33.8967      33.7666      34.1048      34.4722      34.3281   \n",
       "1       33.9794      34.2996      34.1232      34.1260      34.0834   \n",
       "2       34.8397      35.0390      35.1257      34.1310      34.6296   \n",
       "3       33.9495      34.1444      33.5586      33.4852      32.9203   \n",
       "4       34.3612      34.4437      34.0495      33.9382      33.3032   \n",
       "5       34.5063      34.4195      34.0148      33.8809      33.9621   \n",
       "6       33.8739      33.8561      34.0091      34.1713      34.1532   \n",
       "7       33.8877      33.8170      34.3975      34.1070      34.3238   \n",
       "8       33.7382      33.8192      33.5882      33.7218      33.4791   \n",
       "9       33.9715      34.0325      34.5261      34.3204      34.4775   \n",
       "10      33.8616      33.6905      34.1431      34.0800      34.0269   \n",
       "11      33.6247      34.0442      34.1751      34.0652      33.6538   \n",
       "12      34.8906      35.8110      34.8277      35.1031      35.3820   \n",
       "13      36.2859      35.4809      34.7119      35.2623      35.0421   \n",
       "14      35.6900      35.4300      35.7616      35.6318      35.3681   \n",
       "15      35.1352      35.0797      34.9388      35.0725      35.2059   \n",
       "16      35.7633      35.4769      35.6850      35.3690      35.8015   \n",
       "17      35.9709      36.0299      35.8589      36.1879      35.8114   \n",
       "18      35.8952      36.0750      35.8183      36.1429      36.3012   \n",
       "19      35.9462      36.2186      36.3577      36.4732      35.6464   \n",
       "20      37.4658      35.7302      35.6559      35.9850      36.3132   \n",
       "21      36.5582      36.5673      36.6765      36.7727      36.6858   \n",
       "22      36.5280      36.2985      36.4604      36.2603      36.5788   \n",
       "23      36.4319      36.3303      36.3830      36.8548      36.7580   \n",
       "\n",
       "    0.0007_mean  0.0006_mean  0.0005_mean  0.0004_mean  0.0003_mean  \\\n",
       "0       34.1199      34.1343      33.9614      34.2144      33.4312   \n",
       "1       34.0315      34.0361      34.1098      34.1540      33.9368   \n",
       "2       34.7337      34.0210      34.6563      34.0728      34.0334   \n",
       "3       34.1612      33.1922      33.3552      33.1304      32.2419   \n",
       "4       34.1230      33.9392      33.9461      33.7411      34.3176   \n",
       "5       33.8735      33.8598      33.7669      34.3787      34.2588   \n",
       "6       33.8691      34.1332      33.9426      34.0381      34.2763   \n",
       "7       34.2922      34.3360      34.2902      34.3743      34.1906   \n",
       "8       33.9450      33.9421      33.8468      33.8459      33.7960   \n",
       "9       34.1714      34.3446      33.7550      33.9080      34.1736   \n",
       "10      34.1495      34.0671      34.1982      34.0890      33.9112   \n",
       "11      34.0870      33.9074      34.0052      33.8273      34.1494   \n",
       "12      35.3879      35.0688      35.8009      35.7868      36.0026   \n",
       "13      35.1546      35.3421      34.9806      36.9482      35.0570   \n",
       "14      35.4666      35.5247      35.2215      35.0339      35.0971   \n",
       "15      34.7787      34.8483      35.0720      34.9944      34.9447   \n",
       "16      36.3752      35.0792      36.0552      35.7728      36.0289   \n",
       "17      36.1512      36.2648      36.2854      36.2138      36.2563   \n",
       "18      36.1458      36.0727      35.5236      36.0581      35.4566   \n",
       "19      35.5069      35.6691      35.3255      34.9703      36.3903   \n",
       "20      35.6608      36.2174      36.0467      36.2851      35.2974   \n",
       "21      36.9320      36.8448      37.0639      37.2044      37.0501   \n",
       "22      36.4379      36.7048      36.6045      36.2259      36.9071   \n",
       "23      36.6907      36.6623      37.0027      36.9593      36.3512   \n",
       "\n",
       "    0.0030_std  0.0020_std  0.0010_std  0.0009_std  0.0008_std  0.0007_std  \\\n",
       "0       0.0230      0.0407      0.0196      0.0111      0.0304      0.0233   \n",
       "1       0.0222      0.0314      0.0139      0.0264      0.0359      0.0211   \n",
       "2       0.1019      0.0576      0.1342      0.0958      0.0539      0.0594   \n",
       "3       0.0545      0.0252      0.0810      0.0973      0.1441      0.1443   \n",
       "4       0.0815      0.0663      0.0519      0.0770      0.1233      0.0834   \n",
       "5       0.0596      0.0394      0.0345      0.0579      0.0617      0.0406   \n",
       "6       0.0277      0.0379      0.0543      0.0614      0.0696      0.0501   \n",
       "7       0.0247      0.0238      0.0367      0.0367      0.0163      0.1037   \n",
       "8       0.1201      0.0930      0.0927      0.0939      0.0571      0.1116   \n",
       "9       0.0252      0.0654      0.0168      0.0602      0.0510      0.0906   \n",
       "10      0.0285      0.0686      0.0262      0.0238      0.0237      0.0255   \n",
       "11      0.0803      0.0378      0.0197      0.0215      0.0566      0.0425   \n",
       "12      0.1141      0.0480      0.0449      0.0483      0.0288      0.0532   \n",
       "13      0.0740      0.1258      0.0430      0.0395      0.0343      0.0541   \n",
       "14      0.0226      0.0113      0.0224      0.0411      0.0326      0.0214   \n",
       "15      0.0196      0.0094      0.0108      0.0224      0.0152      0.0295   \n",
       "16      0.0557      0.0277      0.0070      0.0238      0.0135      0.0244   \n",
       "17      0.0293      0.0425      0.0360      0.0318      0.0261      0.0377   \n",
       "18      0.0132      0.0238      0.0274      0.0188      0.0418      0.0263   \n",
       "19      0.0168      0.0146      0.0139      0.0141      0.0369      0.0426   \n",
       "20      0.1523      0.1100      0.1231      0.0650      0.1557      0.0533   \n",
       "21      0.0242      0.0138      0.0453      0.0081      0.0230      0.0085   \n",
       "22      0.0220      0.0328      0.0128      0.0114      0.0093      0.0247   \n",
       "23      0.0305      0.0271      0.0450      0.0486      0.0504      0.0256   \n",
       "\n",
       "    0.0006_std  0.0005_std  0.0004_std  0.0003_std  \n",
       "0       0.0211      0.0476      0.0271      0.1238  \n",
       "1       0.0506      0.0239      0.0195      0.0244  \n",
       "2       0.0603      0.0749      0.0652      0.0664  \n",
       "3       0.1279      0.3019      0.2520      0.0991  \n",
       "4       0.0848      0.0614      0.0567      0.0364  \n",
       "5       0.0459      0.0678      0.0274      0.0531  \n",
       "6       0.0495      0.0227      0.0357      0.0366  \n",
       "7       0.0428      0.0314      0.0265      0.0291  \n",
       "8       0.1641      0.0793      0.0508      0.0850  \n",
       "9       0.0771      0.0743      0.0499      0.0266  \n",
       "10      0.0406      0.0228      0.0130      0.0208  \n",
       "11      0.0172      0.0261      0.0376      0.0219  \n",
       "12      0.0634      0.0602      0.1130      0.0748  \n",
       "13      0.0580      0.0909      0.0645      0.0382  \n",
       "14      0.0177      0.0183      0.0104      0.0185  \n",
       "15      0.0069      0.0128      0.0066      0.0213  \n",
       "16      0.0185      0.0214      0.0284      0.0194  \n",
       "17      0.0252      0.0345      0.0285      0.0162  \n",
       "18      0.0274      0.0184      0.0202      0.0138  \n",
       "19      0.0397      0.0289      0.0412      0.0551  \n",
       "20      0.0653      0.0926      0.0876      0.1060  \n",
       "21      0.0208      0.0079      0.0152      0.0101  \n",
       "22      0.0030      0.0176      0.0089      0.0148  \n",
       "23      0.0393      0.0193      0.0325      0.0510  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lr_list predict: START! \n",
    "\n",
    "start_preds_date = pd.to_datetime(config['data']['start_preds_date'], errors='coerce')\n",
    "stop_preds_date = pd.to_datetime(config['data']['stop_preds_date'], errors='coerce')\n",
    "interval_days = int((stop_preds_date - start_preds_date).days)\n",
    "start_index = df.index[df['date']==config['data']['start_preds_date']].tolist()[0]\n",
    "result_list = []\n",
    "\n",
    "cum_mae_pred = 0.0\n",
    "cum_mae_dap  = 0.0\n",
    "steps = 0\n",
    "\n",
    "start_lr_list = [0.003, 0.002, 0.001, 0.0009, 0.0008, 0.0007, 0.0006, 0.0005, 0.0004, 0.0003]\n",
    "results_acc = pd.read_csv('../data_archiv/DTG/res/DTG_res_fresh.csv')\n",
    "total_iters = len(start_lr_list)\n",
    "\n",
    "with tqdm(total=total_iters, desc=\"Rolling forecast\") as pbar:\n",
    "    for start_lr in start_lr_list:\n",
    "        df_temp = df.copy()\n",
    "\n",
    "        config['train']['lr'] = start_lr\n",
    "\n",
    "        results = train_tscv_model_7(config, df_temp, verbose=True)\n",
    "\n",
    "        DAP_ = round(float(df['c_month_real'].iloc[start_index + 1]), 4)\n",
    "        mu_DAP = round(float(results[\"mu\"]['c_month_real']), 4)\n",
    "        sigma_DAP = round(float(results[\"sigma\"]['c_month_real']), 4)\n",
    "        # DAP_ = DAP * sigma_DAP + mu_DAP\n",
    "\n",
    "        Target_ = round(float(df_temp['Target'].iloc[start_index + 1]), 4)\n",
    "        Pred = round(float(results[\"pred_series_per_fold\"][0]), 4)\n",
    "        preds_last_10_mean = round(pd.Series(results[\"preds_last_10\"]).mean(), 4)\n",
    "        preds_last_10_std = round(pd.Series(results[\"preds_last_10\"]).std(), 4)\n",
    "        print(results[\"preds_last_10\"], f\" | mean={preds_last_10_mean:.4f} | std={preds_last_10_std:.4f}\\n\")\n",
    "\n",
    "        mae_target_pred = results[\"fold_metrics\"][-1][\"mae_target_pred\"]\n",
    "        mae_target_dap = abs(DAP_ - Target_)\n",
    "        mae_target_pred_avg = abs(preds_last_10_mean - Target_)\n",
    "        # mae_target_dap = results[\"fold_metrics\"][-1][\"mae_target_dap\"]\n",
    "\n",
    "        result_list.append({\n",
    "            'date': config['data']['stop_preds_date'],\n",
    "            'lr': config['train']['lr'],\n",
    "            'last_DAP': DAP_,\n",
    "            'mu_DAP': round(float(results[\"mu\"]['c_month_real']), 4),\n",
    "            'Target': Target_,\n",
    "            'Predict': Pred,\n",
    "            'Pred_last_10_ep_mean': preds_last_10_mean,\n",
    "            'Pred_last_10_ep_std': preds_last_10_std,\n",
    "            'mae_target_pred': round(mae_target_pred, 4),\n",
    "            'mae_target_pred_avg': round(mae_target_pred_avg, 4),\n",
    "            'mae_target_dap': round(mae_target_dap, 4)\n",
    "        })\n",
    "\n",
    "        # обновляем кумулятивные средние\n",
    "        steps += 1\n",
    "        cum_mae_pred = ((cum_mae_pred * (steps - 1)) + mae_target_pred) / steps\n",
    "        cum_mae_dap  = ((cum_mae_dap  * (steps - 1)) + mae_target_dap)  / steps\n",
    "\n",
    "        # обновляем прогресс-бар: последние и средние MAE\n",
    "        pbar.set_postfix({\n",
    "            \"last_mae_pred\": f\"{mae_target_pred:.4f}\",\n",
    "            \"last_mae_dap\":  f\"{mae_target_dap:.4f}\",\n",
    "            \"avg_mae_pred\":  f\"{cum_mae_pred:.4f}\",\n",
    "            \"avg_mae_dap\":   f\"{cum_mae_dap:.4f}\"\n",
    "        })\n",
    "        pbar.update(1)\n",
    "\n",
    "# собираем итоговый датафрейм один раз\n",
    "result_tab = pd.DataFrame(result_list)\n",
    "make_results_acc(results_acc, result_tab, config)\n",
    "results_acc.to_csv('../data_archiv/DTG/res/DTG_res_fresh.csv', index=False)\n",
    "display(results_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lr</th>\n",
       "      <th>last_DAP</th>\n",
       "      <th>mu_DAP</th>\n",
       "      <th>Target</th>\n",
       "      <th>Predict</th>\n",
       "      <th>Pred_last_10_ep_mean</th>\n",
       "      <th>Pred_last_10_ep_std</th>\n",
       "      <th>mae_target_pred</th>\n",
       "      <th>mae_target_pred_avg</th>\n",
       "      <th>mae_target_dap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.4423</td>\n",
       "      <td>36.4319</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>36.4423</td>\n",
       "      <td>36.4319</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3372</td>\n",
       "      <td>36.3303</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>36.3372</td>\n",
       "      <td>36.3303</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3900</td>\n",
       "      <td>36.3830</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>36.3900</td>\n",
       "      <td>36.3830</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8480</td>\n",
       "      <td>36.8548</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>36.8480</td>\n",
       "      <td>36.8548</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.7379</td>\n",
       "      <td>36.7580</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>36.7379</td>\n",
       "      <td>36.7580</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.6947</td>\n",
       "      <td>36.6907</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>36.6947</td>\n",
       "      <td>36.6907</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.6517</td>\n",
       "      <td>36.6623</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>36.6517</td>\n",
       "      <td>36.6623</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.9879</td>\n",
       "      <td>37.0027</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>36.9879</td>\n",
       "      <td>37.0027</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.9453</td>\n",
       "      <td>36.9593</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>36.9453</td>\n",
       "      <td>36.9593</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>36.2994</td>\n",
       "      <td>33.1234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3090</td>\n",
       "      <td>36.3512</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>36.3090</td>\n",
       "      <td>36.3512</td>\n",
       "      <td>36.2994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      lr  last_DAP   mu_DAP  Target  Predict  \\\n",
       "0  2025-11-12  0.0030   36.2994  33.1234     0.0  36.4423   \n",
       "1  2025-11-12  0.0020   36.2994  33.1234     0.0  36.3372   \n",
       "2  2025-11-12  0.0010   36.2994  33.1234     0.0  36.3900   \n",
       "3  2025-11-12  0.0009   36.2994  33.1234     0.0  36.8480   \n",
       "4  2025-11-12  0.0008   36.2994  33.1234     0.0  36.7379   \n",
       "5  2025-11-12  0.0007   36.2994  33.1234     0.0  36.6947   \n",
       "6  2025-11-12  0.0006   36.2994  33.1234     0.0  36.6517   \n",
       "7  2025-11-12  0.0005   36.2994  33.1234     0.0  36.9879   \n",
       "8  2025-11-12  0.0004   36.2994  33.1234     0.0  36.9453   \n",
       "9  2025-11-12  0.0003   36.2994  33.1234     0.0  36.3090   \n",
       "\n",
       "   Pred_last_10_ep_mean  Pred_last_10_ep_std  mae_target_pred  \\\n",
       "0               36.4319               0.0305          36.4423   \n",
       "1               36.3303               0.0271          36.3372   \n",
       "2               36.3830               0.0450          36.3900   \n",
       "3               36.8548               0.0486          36.8480   \n",
       "4               36.7580               0.0504          36.7379   \n",
       "5               36.6907               0.0256          36.6947   \n",
       "6               36.6623               0.0393          36.6517   \n",
       "7               37.0027               0.0193          36.9879   \n",
       "8               36.9593               0.0325          36.9453   \n",
       "9               36.3512               0.0510          36.3090   \n",
       "\n",
       "   mae_target_pred_avg  mae_target_dap  \n",
       "0              36.4319         36.2994  \n",
       "1              36.3303         36.2994  \n",
       "2              36.3830         36.2994  \n",
       "3              36.8548         36.2994  \n",
       "4              36.7580         36.2994  \n",
       "5              36.6907         36.2994  \n",
       "6              36.6623         36.2994  \n",
       "7              37.0027         36.2994  \n",
       "8              36.9593         36.2994  \n",
       "9              36.3512         36.2994  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.6424 0.255 0.0369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.636399999999995"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(round(result_tab['Pred_last_10_ep_mean'].mean(), 4), round(result_tab['Pred_last_10_ep_mean'].std(), 4), \\\n",
    "      round(result_tab['Pred_last_10_ep_std'].mean(), 4))\n",
    "my_list = result_tab['Pred_last_10_ep_mean'].tolist()\n",
    "my_list.remove(max(my_list))\n",
    "my_list.remove(min(my_list))\n",
    "sum(my_list) / len(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_tab['Predict_mean_10_last_epochs'] = result_tab['Pred_last_10_ep_mean'].copy()\n",
    "# result_tab['Predict_std_10_last_epochs'] = result_tab['Pred_last_10_ep_std'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_tab[['lr', 'Predict_mean_10_last_epochs', 'Predict_std_10_last_epochs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time_shift</th>\n",
       "      <th>window_size</th>\n",
       "      <th>lr_lambda</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dropout</th>\n",
       "      <th>n_res_blocks</th>\n",
       "      <th>PL10_mean_mean</th>\n",
       "      <th>PL10_mean_std</th>\n",
       "      <th>PL10_std_mean</th>\n",
       "      <th>PL10_std_std</th>\n",
       "      <th>0.0030_mean</th>\n",
       "      <th>0.0020_mean</th>\n",
       "      <th>0.0010_mean</th>\n",
       "      <th>0.0009_mean</th>\n",
       "      <th>0.0008_mean</th>\n",
       "      <th>0.0007_mean</th>\n",
       "      <th>0.0006_mean</th>\n",
       "      <th>0.0005_mean</th>\n",
       "      <th>0.0004_mean</th>\n",
       "      <th>0.0003_mean</th>\n",
       "      <th>0.0030_std</th>\n",
       "      <th>0.0020_std</th>\n",
       "      <th>0.0010_std</th>\n",
       "      <th>0.0009_std</th>\n",
       "      <th>0.0008_std</th>\n",
       "      <th>0.0007_std</th>\n",
       "      <th>0.0006_std</th>\n",
       "      <th>0.0005_std</th>\n",
       "      <th>0.0004_std</th>\n",
       "      <th>0.0003_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>44</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.04296</td>\n",
       "      <td>0.296592</td>\n",
       "      <td>0.03677</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>33.8967</td>\n",
       "      <td>33.7666</td>\n",
       "      <td>34.1048</td>\n",
       "      <td>34.4722</td>\n",
       "      <td>34.3281</td>\n",
       "      <td>34.1199</td>\n",
       "      <td>34.1343</td>\n",
       "      <td>33.9614</td>\n",
       "      <td>34.2144</td>\n",
       "      <td>33.4312</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>52</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.08798</td>\n",
       "      <td>0.101572</td>\n",
       "      <td>0.02693</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>33.9794</td>\n",
       "      <td>34.2996</td>\n",
       "      <td>34.1232</td>\n",
       "      <td>34.1260</td>\n",
       "      <td>34.0834</td>\n",
       "      <td>34.0315</td>\n",
       "      <td>34.0361</td>\n",
       "      <td>34.1098</td>\n",
       "      <td>34.1540</td>\n",
       "      <td>33.9368</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.52822</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>34.8397</td>\n",
       "      <td>35.0390</td>\n",
       "      <td>35.1257</td>\n",
       "      <td>34.1310</td>\n",
       "      <td>34.6296</td>\n",
       "      <td>34.7337</td>\n",
       "      <td>34.0210</td>\n",
       "      <td>34.6563</td>\n",
       "      <td>34.0728</td>\n",
       "      <td>34.0334</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>0.0664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>cos</td>\n",
       "      <td>36</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>33.41389</td>\n",
       "      <td>0.592093</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.085554</td>\n",
       "      <td>33.9495</td>\n",
       "      <td>34.1444</td>\n",
       "      <td>33.5586</td>\n",
       "      <td>33.4852</td>\n",
       "      <td>32.9203</td>\n",
       "      <td>34.1612</td>\n",
       "      <td>33.1922</td>\n",
       "      <td>33.3552</td>\n",
       "      <td>33.1304</td>\n",
       "      <td>32.2419</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>34.01628</td>\n",
       "      <td>0.334351</td>\n",
       "      <td>0.07227</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>34.3612</td>\n",
       "      <td>34.4437</td>\n",
       "      <td>34.0495</td>\n",
       "      <td>33.9382</td>\n",
       "      <td>33.3032</td>\n",
       "      <td>34.1230</td>\n",
       "      <td>33.9392</td>\n",
       "      <td>33.9461</td>\n",
       "      <td>33.7411</td>\n",
       "      <td>34.3176</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>_cos</td>\n",
       "      <td>40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.09213</td>\n",
       "      <td>0.271552</td>\n",
       "      <td>0.04879</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>34.5063</td>\n",
       "      <td>34.4195</td>\n",
       "      <td>34.0148</td>\n",
       "      <td>33.8809</td>\n",
       "      <td>33.9621</td>\n",
       "      <td>33.8735</td>\n",
       "      <td>33.8598</td>\n",
       "      <td>33.7669</td>\n",
       "      <td>34.3787</td>\n",
       "      <td>34.2588</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.03229</td>\n",
       "      <td>0.147213</td>\n",
       "      <td>0.04455</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>33.8739</td>\n",
       "      <td>33.8561</td>\n",
       "      <td>34.0091</td>\n",
       "      <td>34.1713</td>\n",
       "      <td>34.1532</td>\n",
       "      <td>33.8691</td>\n",
       "      <td>34.1332</td>\n",
       "      <td>33.9426</td>\n",
       "      <td>34.0381</td>\n",
       "      <td>34.2763</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>34.20160</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.8877</td>\n",
       "      <td>33.8170</td>\n",
       "      <td>34.3975</td>\n",
       "      <td>34.1070</td>\n",
       "      <td>34.3238</td>\n",
       "      <td>34.2922</td>\n",
       "      <td>34.3360</td>\n",
       "      <td>34.2902</td>\n",
       "      <td>34.3743</td>\n",
       "      <td>34.1906</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>48</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>33.77220</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.09480</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>33.7382</td>\n",
       "      <td>33.8192</td>\n",
       "      <td>33.5882</td>\n",
       "      <td>33.7218</td>\n",
       "      <td>33.4791</td>\n",
       "      <td>33.9450</td>\n",
       "      <td>33.9421</td>\n",
       "      <td>33.8468</td>\n",
       "      <td>33.8459</td>\n",
       "      <td>33.7960</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>60</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.16810</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.05370</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>33.9715</td>\n",
       "      <td>34.0325</td>\n",
       "      <td>34.5261</td>\n",
       "      <td>34.3204</td>\n",
       "      <td>34.4775</td>\n",
       "      <td>34.1714</td>\n",
       "      <td>34.3446</td>\n",
       "      <td>33.7550</td>\n",
       "      <td>33.9080</td>\n",
       "      <td>34.1736</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0499</td>\n",
       "      <td>0.0266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.02170</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.02940</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>33.8616</td>\n",
       "      <td>33.6905</td>\n",
       "      <td>34.1431</td>\n",
       "      <td>34.0800</td>\n",
       "      <td>34.0269</td>\n",
       "      <td>34.1495</td>\n",
       "      <td>34.0671</td>\n",
       "      <td>34.1982</td>\n",
       "      <td>34.0890</td>\n",
       "      <td>33.9112</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-07</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>33.95390</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>33.6247</td>\n",
       "      <td>34.0442</td>\n",
       "      <td>34.1751</td>\n",
       "      <td>34.0652</td>\n",
       "      <td>33.6538</td>\n",
       "      <td>34.0870</td>\n",
       "      <td>33.9074</td>\n",
       "      <td>34.0052</td>\n",
       "      <td>33.8273</td>\n",
       "      <td>34.1494</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.40610</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.06490</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>34.8906</td>\n",
       "      <td>35.8110</td>\n",
       "      <td>34.8277</td>\n",
       "      <td>35.1031</td>\n",
       "      <td>35.3820</td>\n",
       "      <td>35.3879</td>\n",
       "      <td>35.0688</td>\n",
       "      <td>35.8009</td>\n",
       "      <td>35.7868</td>\n",
       "      <td>36.0026</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.0748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>35.42660</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.06220</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>36.2859</td>\n",
       "      <td>35.4809</td>\n",
       "      <td>34.7119</td>\n",
       "      <td>35.2623</td>\n",
       "      <td>35.0421</td>\n",
       "      <td>35.1546</td>\n",
       "      <td>35.3421</td>\n",
       "      <td>34.9806</td>\n",
       "      <td>36.9482</td>\n",
       "      <td>35.0570</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.42250</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.02160</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>35.6900</td>\n",
       "      <td>35.4300</td>\n",
       "      <td>35.7616</td>\n",
       "      <td>35.6318</td>\n",
       "      <td>35.3681</td>\n",
       "      <td>35.4666</td>\n",
       "      <td>35.5247</td>\n",
       "      <td>35.2215</td>\n",
       "      <td>35.0339</td>\n",
       "      <td>35.0971</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-10</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.00700</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.01540</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>35.1352</td>\n",
       "      <td>35.0797</td>\n",
       "      <td>34.9388</td>\n",
       "      <td>35.0725</td>\n",
       "      <td>35.2059</td>\n",
       "      <td>34.7787</td>\n",
       "      <td>34.8483</td>\n",
       "      <td>35.0720</td>\n",
       "      <td>34.9944</td>\n",
       "      <td>34.9447</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.74070</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.02400</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>35.7633</td>\n",
       "      <td>35.4769</td>\n",
       "      <td>35.6850</td>\n",
       "      <td>35.3690</td>\n",
       "      <td>35.8015</td>\n",
       "      <td>36.3752</td>\n",
       "      <td>35.0792</td>\n",
       "      <td>36.0552</td>\n",
       "      <td>35.7728</td>\n",
       "      <td>36.0289</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.10310</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>35.9709</td>\n",
       "      <td>36.0299</td>\n",
       "      <td>35.8589</td>\n",
       "      <td>36.1879</td>\n",
       "      <td>35.8114</td>\n",
       "      <td>36.1512</td>\n",
       "      <td>36.2648</td>\n",
       "      <td>36.2854</td>\n",
       "      <td>36.2138</td>\n",
       "      <td>36.2563</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.94890</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.02310</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>35.8952</td>\n",
       "      <td>36.0750</td>\n",
       "      <td>35.8183</td>\n",
       "      <td>36.1429</td>\n",
       "      <td>36.3012</td>\n",
       "      <td>36.1458</td>\n",
       "      <td>36.0727</td>\n",
       "      <td>35.5236</td>\n",
       "      <td>36.0581</td>\n",
       "      <td>35.4566</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>35.85040</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.03040</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>35.9462</td>\n",
       "      <td>36.2186</td>\n",
       "      <td>36.3577</td>\n",
       "      <td>36.4732</td>\n",
       "      <td>35.6464</td>\n",
       "      <td>35.5069</td>\n",
       "      <td>35.6691</td>\n",
       "      <td>35.3255</td>\n",
       "      <td>34.9703</td>\n",
       "      <td>36.3903</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-11-11</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>_cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.06580</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>37.4658</td>\n",
       "      <td>35.7302</td>\n",
       "      <td>35.6559</td>\n",
       "      <td>35.9850</td>\n",
       "      <td>36.3132</td>\n",
       "      <td>35.6608</td>\n",
       "      <td>36.2174</td>\n",
       "      <td>36.0467</td>\n",
       "      <td>36.2851</td>\n",
       "      <td>35.2974</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.1231</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.1557</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.83560</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>36.5582</td>\n",
       "      <td>36.5673</td>\n",
       "      <td>36.6765</td>\n",
       "      <td>36.7727</td>\n",
       "      <td>36.6858</td>\n",
       "      <td>36.9320</td>\n",
       "      <td>36.8448</td>\n",
       "      <td>37.0639</td>\n",
       "      <td>37.2044</td>\n",
       "      <td>37.0501</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>cos</td>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>36.50060</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.01570</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>36.5280</td>\n",
       "      <td>36.2985</td>\n",
       "      <td>36.4604</td>\n",
       "      <td>36.2603</td>\n",
       "      <td>36.5788</td>\n",
       "      <td>36.4379</td>\n",
       "      <td>36.7048</td>\n",
       "      <td>36.6045</td>\n",
       "      <td>36.2259</td>\n",
       "      <td>36.9071</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  time_shift  window_size lr_lambda  epochs  dropout  \\\n",
       "0   2025-11-09           3           92       cos      44      0.3   \n",
       "1   2025-11-09           2           20       cos      52      0.3   \n",
       "2   2025-11-09           4           61       cos      36      0.3   \n",
       "3   2025-11-09           5           92       cos      36      0.3   \n",
       "4   2025-11-09           2           61      _cos      40      0.3   \n",
       "5   2025-11-09           1           61      _cos      40      0.4   \n",
       "6   2025-11-07           1           20       cos      48      0.4   \n",
       "7   2025-11-07           2           34       cos      48      0.4   \n",
       "8   2025-11-07           3           34       cos      48      0.4   \n",
       "9   2025-11-07           2           34       cos      60      0.3   \n",
       "10  2025-11-07           1           34       cos      64      0.3   \n",
       "11  2025-11-07           3           34       cos      64      0.4   \n",
       "12  2025-11-10           1           50      _cos      64      0.4   \n",
       "13  2025-11-10           1           50      _cos      64      0.4   \n",
       "14  2025-11-10           1          100       cos      64      0.4   \n",
       "15  2025-11-10           2          100       cos      64      0.4   \n",
       "16  2025-11-11           1           50       cos      64      0.4   \n",
       "17  2025-11-11           1           21       cos      64      0.4   \n",
       "18  2025-11-11           1          100       cos      64      0.4   \n",
       "19  2025-11-11           3           50       cos      64      0.4   \n",
       "20  2025-11-11           3           50      _cos      64      0.4   \n",
       "21  2025-11-12           1           21       cos      64      0.4   \n",
       "22  2025-11-12           1          100       cos      64      0.4   \n",
       "\n",
       "    n_res_blocks  PL10_mean_mean  PL10_mean_std  PL10_std_mean  PL10_std_std  \\\n",
       "0              3        34.04296       0.296592        0.03677      0.032337   \n",
       "1              3        34.08798       0.101572        0.02693      0.010317   \n",
       "2              1        34.52822       0.428319        0.07696      0.025820   \n",
       "3              3        33.41389       0.592093        0.13273      0.085554   \n",
       "4              2        34.01628       0.334351        0.07227      0.023782   \n",
       "5              2        34.09213       0.271552        0.04879      0.013215   \n",
       "6              2        34.03229       0.147213        0.04455      0.014940   \n",
       "7              2        34.20160       0.203500        0.03720      0.024600   \n",
       "8              2        33.77220       0.147600        0.09480      0.032400   \n",
       "9              3        34.16810       0.252500        0.05370      0.024600   \n",
       "10             3        34.02170       0.156300        0.02940      0.015400   \n",
       "11             3        33.95390       0.195700        0.03610      0.019900   \n",
       "12             3        35.40610       0.425200        0.06490      0.028400   \n",
       "13             3        35.42660       0.679800        0.06220      0.028500   \n",
       "14             2        35.42250       0.245600        0.02160      0.009300   \n",
       "15             2        35.00700       0.131600        0.01540      0.007600   \n",
       "16             2        35.74070       0.370900        0.02400      0.012900   \n",
       "17             2        36.10310       0.173900        0.03080      0.007500   \n",
       "18             2        35.94890       0.276300        0.02310      0.008400   \n",
       "19             2        35.85040       0.508200        0.03040      0.014800   \n",
       "20             2        36.06580       0.590100        0.10110      0.035500   \n",
       "21             2        36.83560       0.222200        0.01770      0.011500   \n",
       "22             2        36.50060       0.211900        0.01570      0.008800   \n",
       "\n",
       "    0.0030_mean  0.0020_mean  0.0010_mean  0.0009_mean  0.0008_mean  \\\n",
       "0       33.8967      33.7666      34.1048      34.4722      34.3281   \n",
       "1       33.9794      34.2996      34.1232      34.1260      34.0834   \n",
       "2       34.8397      35.0390      35.1257      34.1310      34.6296   \n",
       "3       33.9495      34.1444      33.5586      33.4852      32.9203   \n",
       "4       34.3612      34.4437      34.0495      33.9382      33.3032   \n",
       "5       34.5063      34.4195      34.0148      33.8809      33.9621   \n",
       "6       33.8739      33.8561      34.0091      34.1713      34.1532   \n",
       "7       33.8877      33.8170      34.3975      34.1070      34.3238   \n",
       "8       33.7382      33.8192      33.5882      33.7218      33.4791   \n",
       "9       33.9715      34.0325      34.5261      34.3204      34.4775   \n",
       "10      33.8616      33.6905      34.1431      34.0800      34.0269   \n",
       "11      33.6247      34.0442      34.1751      34.0652      33.6538   \n",
       "12      34.8906      35.8110      34.8277      35.1031      35.3820   \n",
       "13      36.2859      35.4809      34.7119      35.2623      35.0421   \n",
       "14      35.6900      35.4300      35.7616      35.6318      35.3681   \n",
       "15      35.1352      35.0797      34.9388      35.0725      35.2059   \n",
       "16      35.7633      35.4769      35.6850      35.3690      35.8015   \n",
       "17      35.9709      36.0299      35.8589      36.1879      35.8114   \n",
       "18      35.8952      36.0750      35.8183      36.1429      36.3012   \n",
       "19      35.9462      36.2186      36.3577      36.4732      35.6464   \n",
       "20      37.4658      35.7302      35.6559      35.9850      36.3132   \n",
       "21      36.5582      36.5673      36.6765      36.7727      36.6858   \n",
       "22      36.5280      36.2985      36.4604      36.2603      36.5788   \n",
       "\n",
       "    0.0007_mean  0.0006_mean  0.0005_mean  0.0004_mean  0.0003_mean  \\\n",
       "0       34.1199      34.1343      33.9614      34.2144      33.4312   \n",
       "1       34.0315      34.0361      34.1098      34.1540      33.9368   \n",
       "2       34.7337      34.0210      34.6563      34.0728      34.0334   \n",
       "3       34.1612      33.1922      33.3552      33.1304      32.2419   \n",
       "4       34.1230      33.9392      33.9461      33.7411      34.3176   \n",
       "5       33.8735      33.8598      33.7669      34.3787      34.2588   \n",
       "6       33.8691      34.1332      33.9426      34.0381      34.2763   \n",
       "7       34.2922      34.3360      34.2902      34.3743      34.1906   \n",
       "8       33.9450      33.9421      33.8468      33.8459      33.7960   \n",
       "9       34.1714      34.3446      33.7550      33.9080      34.1736   \n",
       "10      34.1495      34.0671      34.1982      34.0890      33.9112   \n",
       "11      34.0870      33.9074      34.0052      33.8273      34.1494   \n",
       "12      35.3879      35.0688      35.8009      35.7868      36.0026   \n",
       "13      35.1546      35.3421      34.9806      36.9482      35.0570   \n",
       "14      35.4666      35.5247      35.2215      35.0339      35.0971   \n",
       "15      34.7787      34.8483      35.0720      34.9944      34.9447   \n",
       "16      36.3752      35.0792      36.0552      35.7728      36.0289   \n",
       "17      36.1512      36.2648      36.2854      36.2138      36.2563   \n",
       "18      36.1458      36.0727      35.5236      36.0581      35.4566   \n",
       "19      35.5069      35.6691      35.3255      34.9703      36.3903   \n",
       "20      35.6608      36.2174      36.0467      36.2851      35.2974   \n",
       "21      36.9320      36.8448      37.0639      37.2044      37.0501   \n",
       "22      36.4379      36.7048      36.6045      36.2259      36.9071   \n",
       "\n",
       "    0.0030_std  0.0020_std  0.0010_std  0.0009_std  0.0008_std  0.0007_std  \\\n",
       "0       0.0230      0.0407      0.0196      0.0111      0.0304      0.0233   \n",
       "1       0.0222      0.0314      0.0139      0.0264      0.0359      0.0211   \n",
       "2       0.1019      0.0576      0.1342      0.0958      0.0539      0.0594   \n",
       "3       0.0545      0.0252      0.0810      0.0973      0.1441      0.1443   \n",
       "4       0.0815      0.0663      0.0519      0.0770      0.1233      0.0834   \n",
       "5       0.0596      0.0394      0.0345      0.0579      0.0617      0.0406   \n",
       "6       0.0277      0.0379      0.0543      0.0614      0.0696      0.0501   \n",
       "7       0.0247      0.0238      0.0367      0.0367      0.0163      0.1037   \n",
       "8       0.1201      0.0930      0.0927      0.0939      0.0571      0.1116   \n",
       "9       0.0252      0.0654      0.0168      0.0602      0.0510      0.0906   \n",
       "10      0.0285      0.0686      0.0262      0.0238      0.0237      0.0255   \n",
       "11      0.0803      0.0378      0.0197      0.0215      0.0566      0.0425   \n",
       "12      0.1141      0.0480      0.0449      0.0483      0.0288      0.0532   \n",
       "13      0.0740      0.1258      0.0430      0.0395      0.0343      0.0541   \n",
       "14      0.0226      0.0113      0.0224      0.0411      0.0326      0.0214   \n",
       "15      0.0196      0.0094      0.0108      0.0224      0.0152      0.0295   \n",
       "16      0.0557      0.0277      0.0070      0.0238      0.0135      0.0244   \n",
       "17      0.0293      0.0425      0.0360      0.0318      0.0261      0.0377   \n",
       "18      0.0132      0.0238      0.0274      0.0188      0.0418      0.0263   \n",
       "19      0.0168      0.0146      0.0139      0.0141      0.0369      0.0426   \n",
       "20      0.1523      0.1100      0.1231      0.0650      0.1557      0.0533   \n",
       "21      0.0242      0.0138      0.0453      0.0081      0.0230      0.0085   \n",
       "22      0.0220      0.0328      0.0128      0.0114      0.0093      0.0247   \n",
       "\n",
       "    0.0006_std  0.0005_std  0.0004_std  0.0003_std  \n",
       "0       0.0211      0.0476      0.0271      0.1238  \n",
       "1       0.0506      0.0239      0.0195      0.0244  \n",
       "2       0.0603      0.0749      0.0652      0.0664  \n",
       "3       0.1279      0.3019      0.2520      0.0991  \n",
       "4       0.0848      0.0614      0.0567      0.0364  \n",
       "5       0.0459      0.0678      0.0274      0.0531  \n",
       "6       0.0495      0.0227      0.0357      0.0366  \n",
       "7       0.0428      0.0314      0.0265      0.0291  \n",
       "8       0.1641      0.0793      0.0508      0.0850  \n",
       "9       0.0771      0.0743      0.0499      0.0266  \n",
       "10      0.0406      0.0228      0.0130      0.0208  \n",
       "11      0.0172      0.0261      0.0376      0.0219  \n",
       "12      0.0634      0.0602      0.1130      0.0748  \n",
       "13      0.0580      0.0909      0.0645      0.0382  \n",
       "14      0.0177      0.0183      0.0104      0.0185  \n",
       "15      0.0069      0.0128      0.0066      0.0213  \n",
       "16      0.0185      0.0214      0.0284      0.0194  \n",
       "17      0.0252      0.0345      0.0285      0.0162  \n",
       "18      0.0274      0.0184      0.0202      0.0138  \n",
       "19      0.0397      0.0289      0.0412      0.0551  \n",
       "20      0.0653      0.0926      0.0876      0.1060  \n",
       "21      0.0208      0.0079      0.0152      0.0101  \n",
       "22      0.0030      0.0176      0.0089      0.0148  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
